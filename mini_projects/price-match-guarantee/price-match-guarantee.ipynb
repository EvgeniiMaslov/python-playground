{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Overview\n\n\nDo you scan online retailers in search of the best deals? You're joined by the many savvy shoppers who don't like paying extra for the same product depending on where they shop. Retail companies use a variety of methods to assure customers that their products are the cheapest. Among them is product matching, which allows a company to offer products at rates that are competitive to the same product sold by another retailer. To perform these matches automatically requires a thorough machine learning approach, which is where your data science skills could help.\n\nTwo different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.\n\nShopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.\n\nIn this competition, youâ€™ll apply your machine learning skills to build a model that predicts which items are the same products.\n\nThe applications go far beyond Shopee or other retailers. Your contributions to product matching could support more accurate product categorization and uncover marketplace spam. Customers will benefit from more accurate listings of the same or similar products as they shop. Perhaps most importantly, this will aid you and your fellow shoppers in your hunt for the very best deals."},{"metadata":{},"cell_type":"markdown","source":"### Data description\n\n\nFinding near-duplicates in large datasets is an important problem for many online businesses. In Shopee's case, everyday users can upload their own images and write their own product descriptions, adding an extra layer of challenge. Your task is to identify which products have been posted repeatedly. The differences between related products may be subtle while photos of identical products may be wildly different!\n\nAs this is a code competition, only the first few rows/images of the test set are published; the remainder are only available to your notebook when it is submitted. Expect to find roughly 70,000 images in the hidden test set. The few test rows and images that are provided are intended to illustrate the hidden test set format and folder structure.\n\nFiles\n[train/test].csv - the training set metadata. Each row contains the data for a single posting. Multiple postings might have the exact same image ID, but with different titles or vice versa.\n\nposting_id - the ID code for the posting.\n\nimage - the image id/md5sum.\n\nimage_phash - a perceptual hash of the image.\n\ntitle - the product description for the posting.\n\nlabel_group - ID code for all postings that map to the same product. Not provided for the test set.\n\n[train/test]images - the images associated with the postings.\n\nsample_submission.csv - a sample submission file in the correct format.\n\nposting_id - the ID code for the posting.\n\nmatches - Space delimited list of all posting IDs that match this posting. Posts always self-match. Group sizes were capped at 50, so there's no need to predict more than 50 matches."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cupy as cp\nimport cudf, cuml\n\nimport os\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n#from sklearn.feature_extraction.text import TfidfVectorizer\n#from sklearn.neighbors import NearestNeighbors\n\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0\nimport gc\n\n\nimport cv2\n\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN = False\n\n\nPATH = 'train_images' if TRAIN else 'test_images'\nCSV_FN = 'train.csv' if TRAIN else 'test.csv'\nDATA_PATH = '../input/shopee-product-matching/'\nIMG_PATH = os.path.join(DATA_PATH, PATH)\n\n\nN_WORKERS = 4\nBATCH_SIZE = 1024*4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(os.path.join(DATA_PATH, CSV_FN))\ndataset_cudf = cudf.read_csv(os.path.join(DATA_PATH, CSV_FN))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TRAIN:\n    tmp = dataset.groupby('label_group').posting_id.agg('unique').to_dict()\n    dataset['target'] = dataset.label_group.map(tmp)\n    print(f'Dataset shape {dataset.shape}')\n    \ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Computing text embeddings...')\nmodel = TfidfVectorizer(stop_words='english', binary=True, max_features=25_000)\ntext_embeddings = model.fit_transform(dataset_cudf.title).toarray()\nprint('text embeddings shape',text_embeddings.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\n\nprint('Finding similar titles...')\nCTS = len(dataset)//BATCH_SIZE\nif len(dataset)%BATCH_SIZE!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*BATCH_SIZE\n    b = (j+1)*BATCH_SIZE\n    b = min(b,len(dataset))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = cp.matmul(text_embeddings,text_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        IDX = cp.where(cts[k,]>0.6)[0]\n        o = dataset.iloc[cp.asnumpy(IDX)].posting_id.values\n        preds.append(o)\n    \ndel model, text_embeddings\n_ = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['preds'] = preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path=''): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) // self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            filename = os.path.join(self.path, row.image)\n            img = cv2.imread(filename)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #/128.0 - 1.0\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WGT = '../input/effnetb0/efficientnetb0_notop.h5'\nmodel = EfficientNetB0(weights=WGT,include_top=False, pooling='avg', input_shape=None)\n\nembeds = []\n\nprint('Computing image embeddings...')\n\nCTS = len(dataset)//BATCH_SIZE\nif len(dataset)%BATCH_SIZE!=0: CTS += 1\n    \nfor i,j in enumerate(range(CTS)):\n    \n    a = j*BATCH_SIZE\n    b = (j+1)*BATCH_SIZE\n    b = min(b,len(dataset))\n    print('chunk',a,'to',b)\n    \n    data_gen = DataGenerator(dataset.iloc[a:b], batch_size=32, path=IMG_PATH)\n    image_embeddings = model.predict(data_gen,verbose=1,use_multiprocessing=True, workers=N_WORKERS)\n    embeds.append(image_embeddings)\n\n    #if i>=1: break\n\nimage_embeddings = np.concatenate(embeds)\nprint('image embeddings shape',image_embeddings.shape)\n\ndel model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors = 50 if len(dataset) > 3 else 2\nmodel = NearestNeighbors(n_neighbors=n_neighbors)\nmodel.fit(image_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\n\n\nprint('Finding similar images...')\nCTS = len(image_embeddings)//BATCH_SIZE\nif len(image_embeddings)%BATCH_SIZE!=0: CTS += 1\n    \nfor j in range( CTS ):\n    \n    a = j*BATCH_SIZE\n    b = (j+1)*BATCH_SIZE\n    b = min(b,len(image_embeddings))\n    print('chunk',a,'to',b)\n    distances, indices = model.kneighbors(image_embeddings[a:b,])\n    \n    for k in range(b-a):\n        IDX = np.where(distances[k,]<7.0)[0]\n        IDS = indices[k,IDX]\n        o = dataset.iloc[IDS].posting_id.values\n        preds.append(o)\n        \n#del model, image_embeddings\n_ = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['preds2'] = preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['f1'] = dataset.apply(getMetric('preds2'),axis=1)\nprint('CV Score =', dataset.f1.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = dataset.groupby('image_phash').posting_id.agg('unique').to_dict()\ndataset['preds3'] = dataset.image_phash.map(tmp)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = dataset.groupby('image').posting_id.agg('unique').to_dict()\ndataset['preds4'] = dataset.image.map(tmp)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_for_sub(row):\n    x = np.concatenate([row.preds,row.preds2, row.preds3, row.preds4])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds,row.preds2, row.preds3, row.preds4])\n    return np.unique(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TRAIN:\n    tmp = dataset.groupby('label_group').posting_id.agg('unique').to_dict()\n    dataset['target'] = dataset.label_group.map(tmp)\n    dataset['oof'] = dataset.apply(combine_for_cv,axis=1)\n    dataset['f1'] = dataset.apply(getMetric('oof'),axis=1)\n    print('CV Score =', dataset.f1.mean())\n\ndataset['matches'] = dataset.apply(combine_for_sub,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CV Score = 0.7248077230326005 - base\n\nCV Score = 0.7322238396656401 - tune"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[['posting_id','matches']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub = pd.read_csv('submission.csv')\n#sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}