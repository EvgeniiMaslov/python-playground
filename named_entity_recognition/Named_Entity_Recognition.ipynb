{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense, Dropout\n",
    "\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syyGj3Gbggiu"
   },
   "source": [
    "#### Getting the data\n",
    "\n",
    "For\n",
    "this task, the Groningen Meaning Bank (GMB) data set will be used. This dataset is\n",
    "not considered a gold standard. This means that this data set is built using automatic\n",
    "tagging software, followed by human raters updating subsets of the data. \n",
    "\n",
    "The following named entities are tagged in\n",
    "this corpus:\n",
    "* geo = Geographical entity\n",
    "* org = Organization\n",
    "* per = Person\n",
    "* gpe = Geopolitical entity\n",
    "* tim = Time indicator\n",
    "* art = Artifact\n",
    "* eve = Event\n",
    "* nat = Natural phenomenon\n",
    "\n",
    "\n",
    "To download dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vltKv7LmgcLf",
    "outputId": "7c7669fc-52ed-491e-da03-9b793f577339",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://gmb.let.rug.nl/releases/gmb-2.2.0.zip\n",
    "!unzip gmb-2.2.0.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at data\n",
    "\n",
    "We will be using only\n",
    "files named en.tags in various subdirectories. These files are tab-separated files with\n",
    "each word of a sentence in a row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RurZtvHqhBUi"
   },
   "outputs": [],
   "source": [
    "data_path = 'gmb-2.2.0'\n",
    "output_fn = 'gmb-2.2.0/cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tJM-U3CRggJK"
   },
   "outputs": [],
   "source": [
    "def get_filenames_by_extension(data_path, extension):\n",
    "    fnames = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith(extension):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                fnames.append(file_path)\n",
    "                \n",
    "    return fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tags:  10000\n"
     ]
    }
   ],
   "source": [
    "tags = get_filenames_by_extension(data_path, '.tags')\n",
    "\n",
    "print('Length of tags: ', len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few processing steps need to happen. Each file has a number of sentences, with\n",
    "each words in a row. The entire sentence as a sequence and the corresponding\n",
    "sequence of NER tags need to be fed in as inputs while training the model. As\n",
    "mentioned above, the NER tags also need to be simplified to the top-level entities\n",
    "only. Secondly, the NER tags need to be converted to the IOB format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_ner_subcat(tag):\n",
    "    # NER tags are of form {cat}-{subcat}\n",
    "    # eg tim-dow. We only want first part\n",
    "    return tag.split(\"-\")[0]\n",
    "\n",
    "def iob_format(ners):\n",
    "    # converts IO tags into IOB format\n",
    "    # input is a sequence of IO NER tokens\n",
    "    # convert this: O, PERSON, PERSON, O, O, LOCATION, O\n",
    "    # into: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n",
    "    iob_tokens = []\n",
    "    for idx, token in enumerate(ners):\n",
    "        if token != 'O': # !other\n",
    "            if idx == 0:\n",
    "                token = \"B-\" + token #start of sentence\n",
    "            elif ners[idx-1] == token:\n",
    "                token = \"I-\" + token # continues\n",
    "            else:\n",
    "                token = \"B-\" + token\n",
    "        iob_tokens.append(token)\n",
    "        iob_tags[token] += 1\n",
    "    return iob_tokens\n",
    "\n",
    "def process_data(tags):\n",
    "    total_sentences = 0\n",
    "    outfiles = []\n",
    "    rows = []\n",
    "    for idx, file in enumerate(tags):\n",
    "        with open(file, 'rb') as content:\n",
    "            data = content.read().decode('utf-8').strip()\n",
    "            sentences = data.split(\"\\n\\n\")\n",
    "\n",
    "            total_sentences += len(sentences)\n",
    "\n",
    "            for sentence in sentences:\n",
    "                toks = sentence.split('\\n')\n",
    "                words, pos, ner = [], [], []\n",
    "\n",
    "                for tok in toks:\n",
    "                    t = tok.split(\"\\t\")\n",
    "                    words.append(t[0])\n",
    "                    pos.append(t[1])\n",
    "                    ner_tags[t[3]] += 1\n",
    "                    ner.append(strip_ner_subcat(t[3]))\n",
    "                rows.append([\" \".join(words), \" \".join(iob_format(ner)), \" \".join(pos)])\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags = Counter()\n",
    "iob_tags = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data(tags)\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['text', 'label', 'pos']\n",
    "df.to_csv(os.path.join(data_path, 'dataset.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_path, 'dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tok = Tokenizer(filters='[\\\\]^\\t\\n', lower=False,\n",
    "                    split=' ', oov_token='<OOV>')\n",
    "\n",
    "pos_tok = Tokenizer(filters='[\\\\]^\\t\\n', lower=False,\n",
    "                    split=' ', oov_token='<OOV>')\n",
    "\n",
    "ner_tok = Tokenizer(filters='[\\\\]^\\t\\n', lower=False,\n",
    "                    split=' ', oov_token='<OOV>')\n",
    "\n",
    "text_tok.fit_on_texts(df['text'])\n",
    "pos_tok.fit_on_texts(df['pos'])\n",
    "ner_tok.fit_on_texts(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_config = ner_tok.get_config()\n",
    "text_config = text_tok.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocab = eval(text_config['index_word'])\n",
    "ner_vocab = eval(ner_config['index_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': '<OOV>',\n",
       " '2': 'O',\n",
       " '3': 'B-geo',\n",
       " '4': 'B-tim',\n",
       " '5': 'B-org',\n",
       " '6': 'I-per',\n",
       " '7': 'B-per',\n",
       " '8': 'I-org',\n",
       " '9': 'B-gpe',\n",
       " '10': 'I-geo',\n",
       " '11': 'I-tim',\n",
       " '12': 'B-art',\n",
       " '13': 'B-eve',\n",
       " '14': 'I-art',\n",
       " '15': 'I-eve',\n",
       " '16': 'I-gpe',\n",
       " '17': 'B-nat',\n",
       " '18': 'I-nat'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tok = text_tok.texts_to_sequences(df['text'])\n",
    "y_tok = ner_tok.texts_to_sequences(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62010, 50)\n"
     ]
    }
   ],
   "source": [
    "max_len = 50\n",
    "\n",
    "x_pad = sequence.pad_sequences(x_tok, padding='post', maxlen=max_len)\n",
    "y_pad = sequence.pad_sequences(y_tok, padding='post', maxlen=max_len)\n",
    "\n",
    "print(x_pad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are\n",
    "multiple labels, each label token needs to be one-hot encoded like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62010, 50, 19)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(ner_vocab) + 1\n",
    "Y = tf.keras.utils.to_categorical(y_pad, num_classes=num_classes)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  2,  2, ...,  0,  0,  0],\n",
       "       [ 2,  2,  2, ...,  0,  0,  0],\n",
       "       [ 2,  2,  2, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 2,  2,  2, ...,  0,  0,  0],\n",
       "       [ 2,  4, 11, ...,  0,  0,  0],\n",
       "       [ 2,  2,  2, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(text_vocab) + 1\n",
    "# The embedding dimension\n",
    "embedding_dim = 64\n",
    "# Number of RNN units\n",
    "rnn_units = 100\n",
    "#batch size\n",
    "BATCH_SIZE=90\n",
    "# num of NER classes\n",
    "num_classes = len(ner_vocab)+1\n",
    "dropout=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size, classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        \n",
    "        Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                  batch_input_shape=[batch_size,None]),\n",
    "        \n",
    "        Bidirectional(LSTM(units=rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           dropout=dropout,\n",
    "                           kernel_initializer=tf.keras.initializers.he_normal())),\n",
    "        \n",
    "        TimeDistributed(Dense(rnn_units, activation='relu')),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the embedding layer,\n",
    "there is a BiLSTM layer, followed by a TimeDistributed dense layer. This last\n",
    "layer is different from the sentiment analysis model, where there was only a single\n",
    "unit for binary output. In this problem, for each word in the input sequence, an\n",
    "NER token needs to be predicted. So, the output has as many tokens as the input\n",
    "sequence. Consequently, output tokens correspond 1-to-1 with input tokens and\n",
    "are classified as one of the NER classes. The TimeDistributed layer provides this\n",
    "capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (90, None, 64)            2523072   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (90, None, 200)           132000    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 100)         20100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 19)          1919      \n",
      "=================================================================\n",
      "Total params: 2,677,091\n",
      "Trainable params: 2,677,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model_bilstm(vocab_size = vocab_size,\n",
    "                           embedding_dim=embedding_dim,\n",
    "                           rnn_units=rnn_units,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           classes=num_classes)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_pad\n",
    "# create training and testing splits\n",
    "total_sentences = 62010\n",
    "test_size = round(total_sentences / BATCH_SIZE * 0.2)\n",
    "\n",
    "X_train = X[BATCH_SIZE*test_size:]\n",
    "Y_train = Y[BATCH_SIZE*test_size:]\n",
    "X_test = X[0:BATCH_SIZE*test_size]\n",
    "Y_test = Y[0:BATCH_SIZE*test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "551/551 [==============================] - 51s 92ms/step - loss: 0.1736 - accuracy: 0.9106\n",
      "Epoch 2/15\n",
      "551/551 [==============================] - 51s 93ms/step - loss: 0.0443 - accuracy: 0.9693\n",
      "Epoch 3/15\n",
      "551/551 [==============================] - 51s 92ms/step - loss: 0.0327 - accuracy: 0.9763\n",
      "Epoch 4/15\n",
      "551/551 [==============================] - 51s 93ms/step - loss: 0.0271 - accuracy: 0.9799\n",
      "Epoch 5/15\n",
      "551/551 [==============================] - 51s 92ms/step - loss: 0.0232 - accuracy: 0.9825\n",
      "Epoch 6/15\n",
      "551/551 [==============================] - 51s 92ms/step - loss: 0.0199 - accuracy: 0.9849\n",
      "Epoch 7/15\n",
      "551/551 [==============================] - 51s 93ms/step - loss: 0.0168 - accuracy: 0.9872\n",
      "Epoch 8/15\n",
      "551/551 [==============================] - 52s 94ms/step - loss: 0.0144 - accuracy: 0.9889\n",
      "Epoch 9/15\n",
      "551/551 [==============================] - 52s 94ms/step - loss: 0.0126 - accuracy: 0.9903\n",
      "Epoch 10/15\n",
      "551/551 [==============================] - 52s 94ms/step - loss: 0.0109 - accuracy: 0.9917\n",
      "Epoch 11/15\n",
      "551/551 [==============================] - 54s 97ms/step - loss: 0.0096 - accuracy: 0.9927\n",
      "Epoch 12/15\n",
      "551/551 [==============================] - 51s 92ms/step - loss: 0.0085 - accuracy: 0.9935\n",
      "Epoch 13/15\n",
      "551/551 [==============================] - 50s 90ms/step - loss: 0.0076 - accuracy: 0.9942\n",
      "Epoch 14/15\n",
      "551/551 [==============================] - 52s 94ms/step - loss: 0.0067 - accuracy: 0.9949\n",
      "Epoch 15/15\n",
      "551/551 [==============================] - 53s 96ms/step - loss: 0.0061 - accuracy: 0.9953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18b7af349a0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 4s 27ms/step - loss: 0.1013 - accuracy: 0.9614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10129605978727341, 0.9614012837409973]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM + Conditional Random Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons==0.11.2\n",
      "  Downloading tensorflow_addons-0.11.2-cp38-cp38-win_amd64.whl (911 kB)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.12.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.11.2 typeguard-2.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons==0.11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRFLayer(Layer):\n",
    "    def __init__(self, label_size, mask_id=0, trans_params=None, name='crf', **kwargs):\n",
    "        \n",
    "        super(CRFLayer, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.label_size = label_size\n",
    "        self.mask_id = mask_id\n",
    "        self.transition_params = None\n",
    "        \n",
    "        if trans_params is None:\n",
    "            self.transition_params = tf.Variable(tf.random.uniform(shape=(label_size, label_size)),\n",
    "                                                 trainable=False)\n",
    "        else:\n",
    "            self.transition_params = trans_params\n",
    "            \n",
    "    def call(self, inputs, seq_length, training=None):\n",
    "        \n",
    "        if training is None:\n",
    "            training = L.learning_phase()\n",
    "            \n",
    "        if training:\n",
    "            return inputs\n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    def loss(self, y_true, y_pred):\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_true = tf.cast(self.get_proper_labels(y_true), y_pred.dtype)\n",
    "\n",
    "        seq_lengths = self.get_seq_lengths(y_true)\n",
    "        log_likelihoods, self.transition_params = tfa.text.crf_log_likelihood(y_pred, y_true, seq_lengths)\n",
    "        \n",
    "        self.transition_params = tf.Variable(self.transition_params, trainable=False)\n",
    "        loss = - tf.reduce_mean(log_likelihoods)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def get_proper_labels(self, y_true):\n",
    "        shape = y_true.shape\n",
    "        if len(shape) > 2:\n",
    "            return tf.argmax(y_true, -1, output_type=tf.int32)\n",
    "        return y_true\n",
    "    \n",
    "    def get_seq_lengths(self, matrix):\n",
    "        mask = tf.not_equal(matrix, self.mask_id)\n",
    "        seq_lengths = tf.math.reduce_sum(tf.cast(mask, dtype=tf.int32), axis=-1)\n",
    "        return seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, hidden_num, vocab_size, label_size, embedding_size, name='BilstmCrfModel', **kwargs):\n",
    "        \n",
    "        super(NerModel, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.num_hidden = hidden_num\n",
    "        self.vocab_size = vocab_size\n",
    "        self.label_size = label_size\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size, embedding_size, mask_zero=True, name='embedding')\n",
    "        self.biLSTM = Bidirectional(LSTM(hidden_num, return_sequences=True, name='bilstm'))\n",
    "        self.dense = TimeDistributed(tf.keras.layers.Dense(label_size), name='dense')\n",
    "        self.crf = CRFLayer(self.label_size, name='crf')\n",
    "        \n",
    "    def call(self, text, labels=None, training=None):\n",
    "        seq_length = tf.math.reduce_sum(tf.cast(tf.math.not_equal(text, 0), dtype=tf.int32), axis=-1)\n",
    "        \n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "            \n",
    "        inputs = self.embedding(text)\n",
    "        bilstm = self.biLSTM(inputs)\n",
    "        logits = self.dense(bilstm)\n",
    "        outputs = self.crf(logits, seq_length, training)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(text_vocab) + 1\n",
    "\n",
    "embedding_dim = 64\n",
    "# Number of RNN units\n",
    "rnn_units = 100\n",
    "#batch size\n",
    "BATCH_SIZE=90\n",
    "# num of NER classes\n",
    "num_classes = len(ner_vocab) + 1\n",
    "blc_model = NerModel(rnn_units, vocab_size, num_classes, embedding_dim, dynamic=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sentences = 62010\n",
    "test_size = round(total_sentences / BATCH_SIZE * 0.2)\n",
    "\n",
    "X_train = x_pad[BATCH_SIZE*test_size:]\n",
    "Y_train = Y[BATCH_SIZE*test_size:]\n",
    "X_test = x_pad[0:BATCH_SIZE*test_size]\n",
    "Y_test = Y[0:BATCH_SIZE*test_size]\n",
    "\n",
    "Y_train_int = tf.cast(Y_train, dtype=tf.int32)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,\n",
    "Y_train_int))\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE,\n",
    "drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = tf.Tensor(62.06899, shape=(), dtype=float32)\n",
      "step 50: mean loss = tf.Tensor(30.476456, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(23.678583, shape=(), dtype=float32)\n",
      "step 150: mean loss = tf.Tensor(20.412226, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(17.945261, shape=(), dtype=float32)\n",
      "step 250: mean loss = tf.Tensor(16.17172, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(14.79893, shape=(), dtype=float32)\n",
      "step 350: mean loss = tf.Tensor(13.732441, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(12.8564625, shape=(), dtype=float32)\n",
      "step 450: mean loss = tf.Tensor(12.078372, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(11.394629, shape=(), dtype=float32)\n",
      "step 550: mean loss = tf.Tensor(10.793428, shape=(), dtype=float32)\n",
      "Start of epoch 1\n",
      "step 0: mean loss = tf.Tensor(10.781557, shape=(), dtype=float32)\n",
      "step 50: mean loss = tf.Tensor(10.2322645, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(9.731948, shape=(), dtype=float32)\n",
      "step 150: mean loss = tf.Tensor(9.283842, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(8.883194, shape=(), dtype=float32)\n",
      "step 250: mean loss = tf.Tensor(8.522689, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(8.187927, shape=(), dtype=float32)\n",
      "step 350: mean loss = tf.Tensor(7.8806276, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(7.608766, shape=(), dtype=float32)\n",
      "step 450: mean loss = tf.Tensor(7.3609514, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(7.1341066, shape=(), dtype=float32)\n",
      "step 550: mean loss = tf.Tensor(6.9266562, shape=(), dtype=float32)\n",
      "Start of epoch 2\n",
      "step 0: mean loss = tf.Tensor(6.922041, shape=(), dtype=float32)\n",
      "step 50: mean loss = tf.Tensor(6.721489, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(6.5359397, shape=(), dtype=float32)\n",
      "step 150: mean loss = tf.Tensor(6.3615494, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(6.2017684, shape=(), dtype=float32)\n",
      "step 250: mean loss = tf.Tensor(6.0516243, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(5.907881, shape=(), dtype=float32)\n",
      "step 350: mean loss = tf.Tensor(5.7701945, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(5.6445384, shape=(), dtype=float32)\n",
      "step 450: mean loss = tf.Tensor(5.5269237, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(5.416147, shape=(), dtype=float32)\n",
      "step 550: mean loss = tf.Tensor(5.3118234, shape=(), dtype=float32)\n",
      "Start of epoch 3\n",
      "step 0: mean loss = tf.Tensor(5.309526, shape=(), dtype=float32)\n",
      "step 50: mean loss = tf.Tensor(5.205674, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(5.1077185, shape=(), dtype=float32)\n",
      "step 150: mean loss = tf.Tensor(5.01384, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(4.926581, shape=(), dtype=float32)\n",
      "step 250: mean loss = tf.Tensor(4.8426385, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(4.760779, shape=(), dtype=float32)\n",
      "step 350: mean loss = tf.Tensor(4.6808863, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(4.6066194, shape=(), dtype=float32)\n",
      "step 450: mean loss = tf.Tensor(4.5366254, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(4.4697323, shape=(), dtype=float32)\n",
      "step 550: mean loss = tf.Tensor(4.4061484, shape=(), dtype=float32)\n",
      "Start of epoch 4\n",
      "step 0: mean loss = tf.Tensor(4.404704, shape=(), dtype=float32)\n",
      "step 50: mean loss = tf.Tensor(4.339694, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(4.27787, shape=(), dtype=float32)\n",
      "step 150: mean loss = tf.Tensor(4.2175403, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(4.1611314, shape=(), dtype=float32)\n",
      "step 250: mean loss = tf.Tensor(4.1065803, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(4.0529375, shape=(), dtype=float32)\n",
      "step 350: mean loss = tf.Tensor(3.9997883, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(3.9496682, shape=(), dtype=float32)\n",
      "step 450: mean loss = tf.Tensor(3.902263, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(3.8571293, shape=(), dtype=float32)\n",
      "step 550: mean loss = tf.Tensor(3.813334, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss_metric = tf.keras.metrics.Mean()\n",
    "epochs = 5\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (text_batch, labels_batch) in enumerate(train_dataset):\n",
    "        \n",
    "        labels_max = tf.argmax(labels_batch, -1, output_type=tf.int32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = blc_model(text_batch, training=True)\n",
    "            loss = blc_model.crf.loss(labels_max, logits)\n",
    "            \n",
    "            grads = tape.gradient(loss, blc_model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, blc_model.trainable_weights))\n",
    "\n",
    "            loss_metric(loss)\n",
    "        if step % 50 == 0:\n",
    "            print('step %s: mean loss = %s' % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Named Entity Recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
