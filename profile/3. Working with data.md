

[**Working with data**](#Working-with-data)



1. Tabular data

* [Data imputation](#Data imputation)
* [Standardization and Normalization](#Standardization and Normalization)
* [Feature Selection](#Feature-selection)
* [Feature Extraction](#feature-extraction)
* [Feature importance](#Feature importance)
* [Strategies for Imbalance Data](#strategies-for-imbalance-data)
* [Dealing with categorical variables](#Dealing-with-categorical-variables)



2. NLP

* [Best sentence tokenizers](#Best sentence tokenizers)
* [Normalizing your vocabulary](#Normalizing your vocabulary)
* [Stop words](#Stop words)
* [Bag of words](#Bag of words)
* [n-Gram Language Models](#n-gram-language-models)
* [TFIDF](#tfidf)
* [Word Vectors](#Word Vectors)



3. Computer Vision

* [Contrast Normalization](#Contrast Normalization)







# Working with data



### Data imputation

1. One data imputation technique consists in replacing the missing value of a feature by an **average** value of this feature in the dataset. 

   **ĞŸĞ»ÑÑÑ‹:**

   - ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾.
   - Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….

   **ĞœĞ¸Ğ½ÑƒÑÑ‹:**

   - Ğ—Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑÑ‚ÑÑ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ°, Ñ‚Ğ°Ğº Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ.
   - ĞĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸.
   - ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹.
   - ĞĞ¸ĞºĞ°Ğº Ğ½Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ³Ñ€ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ¼Ğ¿ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¸.

2. Another technique is to replace the missing value by a **value outside the normal range of values**. For example, if the normal range is [0, 1], then you can set the missing value to 2 or -1. The idea is that the learning algorithm will learn what is best to do when the feature has a value significantly different from regular values. 

3. Alternatively, you can replace the missing value by a **value in the middle of the range**. For example, if the range for a feature is [-1, 1], you can set the missing value to be equal to 0. Here, the idea is that the value in the middle of the range will not significantly affect the prediction. 

4. A more advanced technique is to use the missing value as the target variable for a **regression** problem. You can use all remaining features [x(1) i , x(2) i ,...,x(jâ‰ 1) i , x(j+1) i ,...,x(D) i ] to form a feature vector xË†i, set yË†i Î© x(j) , where j is the feature with a missing value. Then you build a regression model to predict yË† from xË†. Of course, to build training examples (xË†, yË†), you only use those examples from the original dataset, in which the value of feature j is present. 

5. Ğ¡Ğ¿Ğ¾ÑĞ¾Ğ± Ñ‡ĞµÑ‚Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ¹: Ğ¸Ğ¼Ğ¿ÑƒÑ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ **k-NN**

   ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾Ñ‡ĞµĞº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ *k* Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ñƒ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑÑ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ ĞµÑÑ‚ÑŒ. Ğ˜Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ *k* Ñ‚Ğ¾Ñ‡ĞµĞº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸ Ğ½Ğ° Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµĞ¼ÑƒÑ, Ğ¸ ÑƒĞ¶Ğµ Ğ½Ğ° Ğ¸Ñ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ ÑÑ‡ĞµĞ¹ĞºĞ¸.

   
   
   ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¿ĞµÑ€Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ğ¼Ğ¿ÑƒÑ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ²ÑˆĞµĞ³Ğ¾ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹. Ğ’Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ¸Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰ĞµĞ³Ğ¾.
   
   **ĞŸĞ»ÑÑÑ‹:**
   
   - ĞĞ° Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾/Ğ¼ĞµĞ´Ğ¸Ğ°Ğ½Ñ‹ Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹.
   - Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸.

   **ĞœĞ¸Ğ½ÑƒÑÑ‹:**

   - Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğµ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ÑŒ Ğ²ĞµÑÑŒ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.
- Ğ’Ğ°Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ, ĞºĞ°ĞºĞ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞ¾ÑĞµĞ´ĞµĞ¹. Ğ˜Ğ¼Ğ¿Ğ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ² impyute [Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°Ğ½Ñ…ÑÑ‚Ñ‚ĞµĞ½ÑĞºÑƒÑ Ğ¸ ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ²Ñƒ Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ñ](https://impyute.readthedocs.io/en/master/api/cross_sectional_imputation.html), Ñ‚Ğ°Ğº Ñ‡Ñ‚Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ (ÑĞºĞ°Ğ¶ĞµĞ¼, ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° ÑĞ°Ğ¹Ñ‚Ñ‹ Ğ»ÑĞ´ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ¾Ğ²) Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.
   - Ğ§ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ĞµĞ½ Ğº Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ°Ğ¼ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ SVM).
   
6. Finally, if you have a significantly large dataset and just a few features with missing values, you can increase the dimensionality of your feature vectors by adding a **binary indicator** feature for each feature with missing values. Letâ€™s say feature j = 12 in your D-dimensional dataset has missing values. For each feature vector x, you then add the feature j = D + 1 which is equal to 1 if the value of feature 12 is present in x and 0 otherwise. The missing feature value then can be replaced by 0 or any number of your choice.



### Standardization and Normalization

Normalization is the process of converting an actual range of values which a numerical feature can take, into a standard range of values, typically in the interval [-1, 1] or [0, 1].
$$
\bar x^{(j)} = \frac{x^{(j)} - min^{(j)}}{max^{(j)} - min^{(j)}}
$$
Standardization (or z-score normalization) is the procedure during which the feature values are rescaled so that they have the properties of a standard normal distribution with Âµ = 0 and â€¡ = 1, where Âµ is the mean (the average value of the feature, averaged over all examples in the dataset) and â€¡ is the standard deviation from the mean.
$$
\hat x^{(j)} = \frac{x^{(j)} - \mu^{(j)}}{\sigma^{(j)}}
$$
Usually, if your dataset is not too big and you have time, you can try both and see which one performs better for your task. If you donâ€™t have time to run multiple experiments, as a rule of thumb:

* unsupervised learning algorithms, in practice, more often benefit from standardization than from normalization; 
* standardization is also preferred for a feature if the values this feature takes are distributed close to a normal distribution (so-called bell curve); 
* again, standardization is preferred for a feature if it can sometimes have extremely high or low values (outliers); this is because normalization will â€œsqueezeâ€ the normal values into a very small range; 
* in all other cases, normalization is preferable.



### Feature Selection

A classic sequential feature selection algorithm is sequential backward selection (SBS), which aims to reduce the dimensionality of the initial feature subspace with a minimum decay in the performance of the classifier to improve upon computational efficiency.

The idea behind the SBS algorithm is quite simple: SBS sequentially removes features from the full feature subset until the new feature subspace contains the desired number of features. In order to determine which feature is to be removed at each stage, we need to define the criterion function, J, that we want to minimize. 

The criterion calculated by the criterion function can simply be the difference in performance of the classifier before and after the removal of a particular feature. Then, the feature to be removed at each stage can simply be defined as the feature that maximizes this criterion; or in more simple terms, at each stage we eliminate the feature that causes the least performance loss after removal. Based on the preceding definition of SBS, we can outline the algorithm in four simple steps: 

1. Initialize the algorithm with k = d, where d is the dimensionality of the full feature space, ğ‘¿ğ‘‘. 
2. Determine the feature, ğ’™âˆ’, that maximizes the criterion: ğ’™âˆ’ = argmax ğ½(ğ‘¿ğ‘˜ âˆ’ ğ’™), where ğ’™ âˆˆ ğ‘¿ğ‘˜. 
3. Remove the feature, ğ’™âˆ’, from the feature set: ğ‘¿ğ‘˜âˆ’1 = ğ‘¿ğ‘˜ âˆ’ ğ’™âˆ’ ; ğ‘˜ = ğ‘˜ âˆ’ 1. 
4. Terminate if k equals the number of desired features; otherwise, go to step 2

Also we can select features use random forest importance, l1 regularization.

### Feature Extraction

For the feature extraction you can use:

* Principal component analysis (PCA) for unsupervised data compression 
* Linear discriminant analysis (LDA) as a supervised dimensionality reduction technique for maximizing class separability 
* Nonlinear dimensionality reduction via kernel principal component analysis (KPCA)

### Feature importance

There are two ways to measure variable importance: 

1. By the decrease in accuracy of the model if the values of a variable are randomly permuted (type=1). Randomly permuting the values has the effect of removing all predictive power for that variable. The accuracy is computed from the out-of-bag data (so this measure is effectively a cross-validated estimate). 
2. By the mean decrease in the Gini impurity score (see â€œMeasuring Homogeneity or Impurityâ€) for all of the nodes that were split on a variable (type=2). This measures how much improvement to the purity of the nodes that variable contributes. This measure is based on the training set, and therefore less reliable than a measure calculated on out-of-bag data.

Ensemble models improve model accuracy by combining the results from many models. Bagging is a particular type of ensemble model based on fitting many models to bootstrapped samples of the data and averaging the models. Random forest is a special type of bagging applied to decision trees. In addition to resampling the data, the random forest algorithm samples the predictor variables when splitting the trees. A useful output from the random forest is a measure of variable importance that ranks the predictors in terms of their contribution to model accuracy. The random forest has a set of hyperparameters that should be tuned using cross-validation to avoid overfitting.



### Strategies for Imbalance Data

* Undersample - Use fewer of the prevalent class records in the classification model.

* Oversample - Use more of the rare class records in the classification model, bootstrapping if necessary.

* Up weight or down weight - Attach more (or less) weight to the rare (or prevalent) class in the model.

* Data generation - Like bootstrapping, except each new bootstrapped record is slightly different from its source.

* Z-score - The value that results after standardization.

  

A variation of upsampling via bootstrapping (see â€œUndersamplingâ€) is data generation by perturbing existing records to create new records. The intuition behind this idea is that since we only observe a limited set of instances, the algorithm doesnâ€™t have a rich set of information to build classification â€œrules.â€ By creating new records that are similar but not identical to existing records, the algorithm has a chance to learn a more robust set of rules.

Highly imbalanced data (i.e., where the interesting outcomes, the 1s, are rare) are problematic for classification algorithms. One strategy is to balance the training data via undersampling the abundant case (or oversampling the rare case). If using all the 1s still leaves you with too few 1s, you can bootstrap the rare cases, or use SMOTE to create synthetic data similar to existing rare cases. Imbalanced data usually indicates that correctly classifying one class (the 1s) has higher value, and that value ratio should be built into the assessment metric.



One way to deal with imbalanced class proportions during model fitting is to assign a larger penalty to wrong predictions on the minority class. Via scikit-learn, adjusting such a penalty is as convenient as setting the class_weight parameter to class_weight='balanced', which is implemented for most classifiers.

Other popular strategies for dealing with class imbalance include upsampling the minority class, downsampling the majority class, and the generation of synthetic training examples.

```python
from sklearn.utils import resample
X_upsampled, y_upsampled = resample(
									X_imb[y_imb == 1],
									y_imb[y_imb == 1],
									replace=True,
									n_samples=X_imb[y_imb == 0].shape[0],
									random_state=123)
```

Similarly, we could downsample the majority class by removing training examples from the dataset. To perform downsampling using the resample function, we could simply swap the class 1 label with class 0 in the previous code example and vice versa.

There are two popular algorithms that oversample the minority class by creating synthetic examples: the **synthetic minority oversampling technique** (SMOTE) and the **adaptive synthetic sampling method** (ADASYN).

SMOTE and ADASYN work similarly in many ways. For a given example xi of the minority class, they pick k nearest neighbors of this example (letâ€™s denote this set of k examples Sk) and then create a synthetic example xnew as xi + (xzi - xi), where xzi is an example of the minority class chosen randomly from Sk. The interpolation hyperparameter is a random number in the range [0, 1]. Both SMOTE and ADASYN randomly pick all possible xi in the dataset. In ADASYN, the number of synthetic examples generated for each xi is proportional to the number of examples in Sk which are not from the minority class. Therefore, more synthetic examples are generated in the area where the examples of the minority class are rare.



### Dealing with categorical variables

* Dummy variables - Binary 0-1 variables derived by recording factor data for use in regression and other models.
* Reference coding - The most common type of coding used by statisticians, in which one level of a factor is used as a reference and other factors are compared to that level
* One hot encoder - A common type of coding used in machine learning community in which all factors levels are retained. While useful for a certain machine learning algorithms, this approach is not appropriate for multiple linear regression.
* Deviation coding - A type of coding that compares each level against the overall mean as opposed to the reference level.

Factor variables need to be converted into numeric variables for use in a regression. The most common method to encode a factor variable with P distinct values is to represent them using P-1 dummy variables. A factor variable with many levels, even in very big data sets, may need to be consolidated into a variable with fewer levels. Some factors have levels that are ordered and can be represented as a single numeric variable.

Because of correlation between predictors, care must be taken in the interpretation of the coefficients in multiple linear regression. Multicollinearity can cause numerical instability in fitting the regression equation. A confounding variable is an important predictor that is omitted from a model and can lead to a regression equation with spurious relationships. An interaction term between two variables is needed if the relationship between the variables and the response is interdependent.





### Best sentence tokenizers

You can use the NLTK function RegexpTokenizer to replicate your simple tokenizer example like this:

```python
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+|$[0-9.]+|\S+')
tokenizer.tokenize(sentence)
```

An even better tokenizer is the Treebank Word Tokenizer from the NLTK package. It incorporates a variety of common rules for English word tokenization. For example, it separates phrase-terminating punctuation (?!.;,) from adjacent tokens and retains decimal numbers containing a period as a single token. In addition it contains rules for English contractions. For example â€œdonâ€™tâ€ is tokenized as ["do", "nâ€™t"] (itâ€™s important to separate the words to allow the syntax tree parser to have a consistent, predictable set of tokens with known grammar rules as its input).

```python
from nltk.tokenize import TreebankWordTokenizer
sentence = """Monticello wasn't designated as UNESCO World Heritage Site until 1987."""
tokenizer = TreebankWordTokenizer()
tokenizer.tokenize(sentence)
```

The NLTK library includes a tokenizerâ€”casual_tokenizeâ€”that was built to deal with short, informal, emoticon-laced texts from social networks where grammar and spelling conventions vary widely.

```python
from nltk.tokenize.casual import casual_tokenize
message = """RT @TJMonticello Best day everrrrrrr at Monticello. Awesommmmmmeeeeeeee day :*)"""
casual_tokenize(message)
```



### Normalizing your vocabulary



**Case folding.** Words can become case â€œdenormalizedâ€ when they are capitalized because of their presence at the beginning of a sentence, or when theyâ€™re written in ALL CAPS for emphasis. Undoing this denormalization is called case normalization, or more commonly, case folding. Normalizing word and character capitalization is one way to reduce your vocabulary size and generalize your NLP pipeline. However, some information is often communicated by capitalization of a wordâ€” for example, 'doctor' and 'Doctor' often have different meanings.

The simplest and most common way to normalize the case of a text string is to lowercase all the characters with a function like Pythonâ€™s built-in str.lower(). Unfortunately this approach will also â€œnormalizeâ€ away a lot of meaningful capitalization in addition to the less meaningful first-word-in-sentence capitalization you intended to normalize away. A better approach for case normalization is to lowercase only the first word of a sentence and allow all other words to retain their capitalization.

Even with this careful approach to case normalization, where you lowercase words only at the start of a sentence, you will still introduce capitalization errors for the rare proper nouns that start a sentence. â€œJoe Smith, the word smith, with a cup of joe.â€ will produce a different set of tokens than â€œSmith the word with a cup of joe, Joe Smith.â€

To avoid this potential loss of information, many NLP pipelines donâ€™t normalize for case at all. For many applications, the efficiency gain (in storage and processing) for reducing oneâ€™s vocabulary size by about half is outweighed by the loss of information for proper nouns. But some information may be â€œlostâ€ even without case normalization. If you donâ€™t identify the word â€œTheâ€ at the start of a sentence as a stop word, that can be a problem for some applications. Really sophisticated pipelines will detect proper nouns before selectively normalizing the case for words at the beginning of sentences that are clearly not proper nouns. You should implement whatever case normalization approach makes sense for your application.



**Stemming.** Another common vocabulary normalization technique is to eliminate the small meaning differences of pluralization or possessive endings of words, or even various verb forms. Stemming lead to big improvement in the â€œrecallâ€ score, but stemming could greatly reduce the â€œprecisionâ€ score.

Two of the most popular stemming algorithms are the Porter and Snowball stemmers.

```python
from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()
' '.join([stemmer.stem(w).strip("'") for w in "dish washer's washed dishes".split()])
```



**Lemmatization.** If you have access to information about connections between the meanings of various words, you might be able to associate several words together even if their spelling is quite different. This more extensive normalization down to the semantic root of a wordâ€”its lemmaâ€”is called lemmatization.

Lemmatization is a potentially more accurate way to normalize a word than stemming or case normalization because it takes into account a wordâ€™s meaning. A lemmatizer uses a knowledge base of word synonyms and word endings to ensure that only words that mean similar things are consolidated into a single token.

Some lemmatizers use the wordâ€™s part of speech (POS) tag in addition to its spelling to help improve accuracy. The POS tag for a word indicates its role in the grammar of a phrase or sentence.

So lemmatizers are better than stemmers for most applications. Stemmers are only really used in large-scale information retrieval applications (keyword search). And if you really want the dimension reduction and recall improvement of a stemmer in your information retrieval pipeline, you should probably also use a lemmatizer right before the stemmer. Because the lemma of a word is a valid English word, stemmers work well on the output of a lemmatizer. This trick will reduce your dimensionality and increase your information retrieval recall even more than a stemmer alone.

```python
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize("better")
```

**When should you use a lemmatizer or a stemmer? ** Stemmers are generally faster to compute and require less-complex code and datasets. But stemmers will make more errors and stem a far greater number of words, reducing the information content or meaning of your text much more than a lemmatizer would.

If your application involves search, stemming and lemmatization will improve the recall of your searches by associating more documents with the same query words. However, stemming, lemmatization, and even case folding will significantly reduce the precision and accuracy of your search results. These vocabulary compression approaches will cause an information retrieval system (search engine) to return many documents not relevant to the wordsâ€™ original meanings. Because search results can be ranked according to relevance, search engines and document indexes often use stemming or lemmatization to increase the likelihood that the search results include the documents a user is looking for. But they combine search results for stemmed and unstemmed versions of words to rank the search results that they present to you.

For a search-based chatbot, however, accuracy is more important. As a result, a chatbot should first search for the closest match using unstemmed, unnormalized words before falling back to stemmed or filtered token matches to find matches. It should rank such matches of normalized tokens lower than the unnormalized token matches.





### Stop words

Stop words are common words in any language that occur with a high frequency but carry much less substantive information about the meaning of a phrase (for example: a, an, the, this, etc.).

Historically, stop words have been excluded from NLP pipelines in order to reduce the computational effort to extract information from a text. Even though the words themselves carry little information, the stop words can provide important relational information as part of an n-gram. Consider these two examples: 

* Mark reported to the CEO 
* Suzanne reported as the CEO to the board 

In your NLP pipeline, you might create 4-grams such as reported to the CEO and reported as the CEO. If you remove the stop words from the 4-grams, both examples would be reduced to "reported CEO", and you would lack the information about the professional hierarchy. Unfortunately, retaining the stop words within your pipeline creates another problem: it increases the length of the n-grams required to make use of these connections formed by the otherwise meaningless stop words. 

So if you have sufficient memory and processing bandwidth to run all the NLP steps in your pipeline on the larger vocabulary, you probably donâ€™t want to worry about ignoring a few unimportant words here and there. And if youâ€™re worried about overfitting a small training set with a large vocabulary, there are better ways to select your vocabulary or reduce your dimensionality than ignoring stop words.









### Bag of words

Bag of words is also called a word frequency vector, because it only counts the frequency of words, not their order. You could use this single vector to represent the whole document or sentence in a single, reasonable length vector.

**Binary.** Alternatively, if youâ€™re doing basic keyword search, you could OR the one-hot word vectors into a binary bag-of-words vector. Search indexes only need to know the presence or absence of each word in each document to help you find those documents later.

```python
# build vocabulary
wordfreq = {}
for sentence in corpus:
    tokens = nltk.word_tokenize(sentence)
    for token in tokens:
        if token not in wordfreq.keys():
            wordfreq[token] = 1
        else:
            wordfreq[token] += 1
            
# select most frequently occurring words
import heapq
most_freq = heapq.nlargest(2000, wordfreq, key=wordfreq.get)


# convert the sentences in our corpus into their corresponding vector representation
sentence_vectors = []
for sentence in corpus:
    sentence_tokens = nltk.word_tokenize(sentence)
    sent_vec = []
    for token in most_freq:
        if token in sentence_tokens:
            sent_vec.append(1)
        else:
            sent_vec.append(0)
    sentence_vectors.append(sent_vec)
```



**Measuring bag-of-words overlap**. If we can measure the bag of words overlap for two vectors, we can get a good estimate of how similar they are in the words they use. And this is a good estimate of how similar they are in meaning. You can use dot product to estimate the bag-of-words vector overlap between some new sentences and the original sentence.

```python
>>> np.dot(sentence0, sentence1)
1 # one word was used in both sentences
```





### n-Gram Language Models

An n-gram is a sequence containing up to n elements that have been extracted from a sequence of those elements, usually a string. In general the â€œelementsâ€ of an n-gram can be characters, syllables, words, or even symbols like â€œA,â€ â€œT,â€ â€œG,â€ and â€œCâ€ used to represent a DNA sequence. When a sequence of tokens is vectorized into a bag-of-words vector, it loses a lot of the meaning inherent in the order of those words. By extending your concept of a token to include multiword tokens, n-grams, your NLP pipeline can retain much of the meaning inherent in the order of words in your statements.

If tokens or n-grams are extremely rare, they donâ€™t carry any correlation with other words that you can use to help identify topics or themes that connect documents or classes of documents. So rare n-grams wonâ€™t be helpful for classification problems.

```python
from nltk.util import ngrams
list(ngrams(tokens, 2))
```



```python
from collections import defaultdict
transitions = defaultdict(list)
for prev, current in zip(document, document[1:]):
 transitions[prev].append(current)

def generate_using_bigrams() -> str:
 current = "." # this means the next word will start a sentence
 result = []
 while True:
 	next_word_candidates = transitions[current] # bigrams (current, _)
 	current = random.choice(next_word_candidates) # choose one at random
 	result.append(current) # append it to results
 	if current == ".": return " ".join(result) # if "." we're done
```

```python
trigram_transitions = defaultdict(list)
starts = []
for prev, current, next in zip(document, document[1:], document[2:]):
 if prev == ".": # if the previous "word" was a period
 	starts.append(current) # then this is a start word
 trigram_transitions[(prev, current)].append(next)

def generate_using_trigrams() -> str:
 current = random.choice(starts) # choose a random starting word
 prev = "." # and precede it with a '.'
 result = [current]
 while True:
 	next_word_candidates = trigram_transitions[(prev, current)]
 	next_word = random.choice(next_word_candidates)
 	prev, current = current, next_word
 	result.append(current)
 	if current == ".":
 		return " ".join(result)
```

### TFIDF

**Zipfâ€™s law** states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table.

A good way to think of a termâ€™s inverse document frequency is this: How strange is it that this token is in this document? If a term appears in one document a lot of times, but occurs rarely in the rest of the corpus, one could assume itâ€™s important to that document specifically.

When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. These frequently occurring words typically don't contain useful or discriminatory information. In this subsection, you will learn about a useful technique called the term frequency-inverse document frequency (tf-idf), which can be used to downweight these frequently occurring words in the feature vectors. The tf-idf can be defined as the product of the term frequency and the inverse document frequency (for a given term, t, in a given document, d, in a corpus): 
$$
ğ‘¡fğ‘–df(ğ‘¡, ğ‘‘) = ğ‘¡f(ğ‘¡, ğ‘‘) Ã— ğ‘–df(ğ‘¡, ğ‘‘) 
\\
or
\\
ğ‘¡fğ‘–df(ğ‘¡, ğ‘‘) = ğ‘¡f(ğ‘¡, ğ‘‘) Ã— (1 + ğ‘–df(ğ‘¡, ğ‘‘))
$$

tf(t, d)â€”the number of times a term, t, occurs in a document, d. It should be noted that, in the bag-of-words model, the word or term order in a sentence or document does not matter. The order in which the term frequencies appear in the feature vector is derived from the vocabulary indices, which are usually assigned alphabetically.
$$
tf(t, d) = \frac{count(t)}{count(d)}
\\
idf(t, d) = log(\frac{n_d}{1 + df(d, t)}) = log\frac{number_.of_.documents}{number_.of_.documents_.containing_.t}
\\
or
\\
idf(t, d) = log(\frac{1 + n_d}{1 + df(d, t)})
$$
Here, nd is the total number of documents, and df(d, t) is the number of documents, d, that contain the term t. 



To make sure that we understand how TfidfTransformer works, let's walk through an example and calculate the tf-idf of the word 'is' in the third document. The word 'is' has a term frequency of 3 (tf = 3) in the third document, and the document frequency of this term is 3 since the term 'is' occurs in all three documents (df = 3). Thus, we can calculate the inverse document frequency as follows:
$$
idf('is', d_3) = log\frac{1 + 3}{1 + 3} = 0
\\
tfidf('is', 3) = 3 Ã— (0 + 1) = 3
$$
If we repeated this calculation for all terms in the third document, we'd obtain the following tf-idf vectors: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0, 1.69, 1.29]. However, notice that the values in this feature vector are different from the values that we obtained from TfidfTransformer that we used previously. The final step that we are missing in this tf-idf calculation is the L2-normalization, which can be applied as follows:
$$
tfidf(d_3)_{norm} = \frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0, 1.69, 1.29]}{\sqrt{3.39^2 + 3^2 + 3.39^2 + 1.29^2 + 1.29^2 + 1.29^2 + 2^2 + 1.69^2 + 1.29^2}}=
\\
= [0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]
$$



### Word Vectors

One important innovation involves representing words as low-dimensional vectors. These vectors can be compared, added together, fed into machine learning models, or anything else you want to do with them. 

Coming up with such vectors for a large vocabulary of words is a difficult undertaking, so typically weâ€™ll learn them from a corpus of text. There are a couple of different schemes, but at a high level the task typically looks something like this:

1. Get a bunch of text. 
2. Create a dataset where the goal is to predict a word given nearby words (or alternatively, to predict nearby words given a word). 
3. Train a neural net to do well on this task. 
4. Take the internal states of the trained neural net as the word vectors.







### Contrast Normalization



One of the most obvious sources of variation that can be safely removed for many tasks is the amount of contrast in the image. Contrast simply refers to the magnitude of the difference between the bright and the dark pixels in an image.

Global contrast normalization (GCN) aims to prevent images from having varying amounts of contrast by subtracting the mean from each image, then rescaling it so that the standard deviation across its pixels is equal to some constant s. This approach is complicated by the fact that no scaling factor can change the contrast of a zero-contrast image (one whose pixels all have equal intensity). Images with very low but non-zero contrast often have little information content. Dividing by the true standard deviation usually accomplishes nothing more than amplifying sensor noise or compression artifacts in such cases.

