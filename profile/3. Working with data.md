

[**Working with data**](#Working-with-data)



1. Tabular data

* [Data imputation](#Data imputation)
* [Standardization and Normalization](#Standardization and Normalization)
* [Feature Selection](#Feature-selection)
* [Feature Extraction](#feature-extraction)
* [Feature importance](#Feature importance)
* [Strategies for Imbalance Data](#strategies-for-imbalance-data)
* [Dealing with categorical variables](#Dealing-with-categorical-variables)



2. NLP

* [n-Gram Language Models](#n-gram-language-models)
* [TFIDF](#tfidf)
* [Word Vectors](#Word Vectors)



3. Computer Vision

* [Contrast Normalization](#Contrast Normalization)







# Working with data



### Data imputation

1. One data imputation technique consists in replacing the missing value of a feature by an **average** value of this feature in the dataset. 

   **–ü–ª—é—Å—ã:**

   - –ü—Ä–æ—Å—Ç–æ –∏ –±—ã—Å—Ç—Ä–æ.
   - –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

   **–ú–∏–Ω—É—Å—ã:**

   - –ó–Ω–∞—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞, —Ç–∞–∫ —á—Ç–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –Ω–µ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è.
   - –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏.
   - –ú–µ—Ç–æ–¥ –Ω–µ –æ—Å–æ–±–µ–Ω–Ω–æ —Ç–æ—á–Ω—ã–π.
   - –ù–∏–∫–∞–∫ –Ω–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å –∏–º–ø—É—Ç–∞—Ü–∏–∏.

2. Another technique is to replace the missing value by a **value outside the normal range of values**. For example, if the normal range is [0, 1], then you can set the missing value to 2 or -1. The idea is that the learning algorithm will learn what is best to do when the feature has a value significantly different from regular values. 

3. Alternatively, you can replace the missing value by a **value in the middle of the range**. For example, if the range for a feature is [-1, 1], you can set the missing value to be equal to 0. Here, the idea is that the value in the middle of the range will not significantly affect the prediction. 

4. A more advanced technique is to use the missing value as the target variable for a **regression** problem. You can use all remaining features [x(1) i , x(2) i ,...,x(j‚â†1) i , x(j+1) i ,...,x(D) i ] to form a feature vector xÀÜi, set yÀÜi Œ© x(j) , where j is the feature with a missing value. Then you build a regression model to predict yÀÜ from xÀÜ. Of course, to build training examples (xÀÜ, yÀÜ), you only use those examples from the original dataset, in which the value of feature j is present. 

5. –°–ø–æ—Å–æ–± —á–µ—Ç–≤—ë—Ä—Ç—ã–π: –∏–º–ø—É—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é **k-NN**

   –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–æ —Ç–æ—á–µ–∫, —á—Ç–æ–±—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ *k* –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ—á–µ–∫, —É –∫–æ—Ç–æ—Ä—ã—Ö —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –µ—Å—Ç—å. –ò–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è *k* —Ç–æ—á–µ–∫, –∫–æ—Ç–æ—Ä—ã–µ –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –ø–æ—Ö–æ–∂–∏ –Ω–∞ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º—É—é, –∏ —É–∂–µ –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø—É—Å—Ç–æ–π —è—á–µ–π–∫–∏.

   ```python
   import sys
   from impyute.imputation.cs import fast_knn
   # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≥–ª—É–±–∏–Ω—É —Ä–µ–∫—É—Ä—Å–∏–∏
   sys.setrecursionlimit(100000)
   # –Ω–∞—á–∏–Ω–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É KNN
   imputed_training=fast_knn(train.values, k=30)
   ```

   –ê–ª–≥–æ—Ä–∏—Ç–º —Å–ø–µ—Ä–≤–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç –∏–º–ø—É—Ç–∞—Ü–∏—é –ø—Ä–æ—Å—Ç—ã–º —Å—Ä–µ–¥–Ω–∏–º, –ø–æ—Ç–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –ø–æ–ª—É—á–∏–≤—à–µ–≥–æ—Å—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∏—Ç –¥–µ—Ä–µ–≤–æ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–≥–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π. –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –≤—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤ –∏—Å—Ö–æ–¥–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–≥–æ.

   **–ü–ª—é—Å—ã:**

   - –ù–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ—á–Ω–µ–µ —Å—Ä–µ–¥–Ω–µ–≥–æ/–º–µ–¥–∏–∞–Ω—ã –∏–ª–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã.
   - –£—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.

   **–ú–∏–Ω—É—Å—ã:**

   - –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –¥–æ—Ä–æ–∂–µ, —Ç–∞–∫ –∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç –¥–µ—Ä–∂–∞—Ç—å –≤–µ—Å—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç–∏.
   - –í–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å, –∫–∞–∫–∞—è –º–µ—Ç—Ä–∏–∫–∞ –¥–∏—Å—Ç–∞–Ω—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ—Å–µ–¥–µ–π. –ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è –≤ impyute [–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –º–∞–Ω—Ö—ç—Ç—Ç–µ–Ω—Å–∫—É—é –∏ –µ–≤–∫–ª–∏–¥–æ–≤—É –¥–∏—Å—Ç–∞–Ω—Ü–∏—é](https://impyute.readthedocs.io/en/master/api/cross_sectional_imputation.html), —Ç–∞–∫ —á—Ç–æ –∞–Ω–∞–ª–∏–∑ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–π (—Å–∫–∞–∂–µ–º, –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤—Ö–æ–¥–æ–≤ –Ω–∞ —Å–∞–π—Ç—ã –ª—é–¥–µ–π —Ä–∞–∑–Ω—ã—Ö –≤–æ–∑—Ä–∞—Å—Ç–æ–≤) –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.
   - –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±—Ä–æ—Å–∞–º –≤ –¥–∞–Ω–Ω—ã—Ö (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç SVM).

6. Finally, if you have a significantly large dataset and just a few features with missing values, you can increase the dimensionality of your feature vectors by adding a **binary indicator** feature for each feature with missing values. Let‚Äôs say feature j = 12 in your D-dimensional dataset has missing values. For each feature vector x, you then add the feature j = D + 1 which is equal to 1 if the value of feature 12 is present in x and 0 otherwise. The missing feature value then can be replaced by 0 or any number of your choice.



### Standardization and Normalization

Normalization is the process of converting an actual range of values which a numerical feature can take, into a standard range of values, typically in the interval [-1, 1] or [0, 1].
$$
\bar x^{(j)} = \frac{x^{(j)} - min^{(j)}}{max^{(j)} - min^{(j)}}
$$
Standardization (or z-score normalization) is the procedure during which the feature values are rescaled so that they have the properties of a standard normal distribution with ¬µ = 0 and ‚Ä° = 1, where ¬µ is the mean (the average value of the feature, averaged over all examples in the dataset) and ‚Ä° is the standard deviation from the mean.
$$
\hat x^{(j)} = \frac{x^{(j)} - \mu^{(j)}}{\sigma^{(j)}}
$$
Usually, if your dataset is not too big and you have time, you can try both and see which one performs better for your task. If you don‚Äôt have time to run multiple experiments, as a rule of thumb:

* unsupervised learning algorithms, in practice, more often benefit from standardization than from normalization; 
* standardization is also preferred for a feature if the values this feature takes are distributed close to a normal distribution (so-called bell curve); 
* again, standardization is preferred for a feature if it can sometimes have extremely high or low values (outliers); this is because normalization will ‚Äúsqueeze‚Äù the normal values into a very small range; 
* in all other cases, normalization is preferable.



### Feature Selection

A classic sequential feature selection algorithm is sequential backward selection (SBS), which aims to reduce the dimensionality of the initial feature subspace with a minimum decay in the performance of the classifier to improve upon computational efficiency.

The idea behind the SBS algorithm is quite simple: SBS sequentially removes features from the full feature subset until the new feature subspace contains the desired number of features. In order to determine which feature is to be removed at each stage, we need to define the criterion function, J, that we want to minimize. 

The criterion calculated by the criterion function can simply be the difference in performance of the classifier before and after the removal of a particular feature. Then, the feature to be removed at each stage can simply be defined as the feature that maximizes this criterion; or in more simple terms, at each stage we eliminate the feature that causes the least performance loss after removal. Based on the preceding definition of SBS, we can outline the algorithm in four simple steps: 

1. Initialize the algorithm with k = d, where d is the dimensionality of the full feature space, ùëøùëë. 
2. Determine the feature, ùíô‚àí, that maximizes the criterion: ùíô‚àí = argmax ùêΩ(ùëøùëò ‚àí ùíô), where ùíô ‚àà ùëøùëò. 
3. Remove the feature, ùíô‚àí, from the feature set: ùëøùëò‚àí1 = ùëøùëò ‚àí ùíô‚àí ; ùëò = ùëò ‚àí 1. 
4. Terminate if k equals the number of desired features; otherwise, go to step 2

Also we can select features use random forest importance, l1 regularization.

### Feature Extraction

For the feature extraction you can use:

* Principal component analysis (PCA) for unsupervised data compression 
* Linear discriminant analysis (LDA) as a supervised dimensionality reduction technique for maximizing class separability 
* Nonlinear dimensionality reduction via kernel principal component analysis (KPCA)

### Feature importance

There are two ways to measure variable importance: 

1. By the decrease in accuracy of the model if the values of a variable are randomly permuted (type=1). Randomly permuting the values has the effect of removing all predictive power for that variable. The accuracy is computed from the out-of-bag data (so this measure is effectively a cross-validated estimate). 
2. By the mean decrease in the Gini impurity score (see ‚ÄúMeasuring Homogeneity or Impurity‚Äù) for all of the nodes that were split on a variable (type=2). This measures how much improvement to the purity of the nodes that variable contributes. This measure is based on the training set, and therefore less reliable than a measure calculated on out-of-bag data.

Ensemble models improve model accuracy by combining the results from many models. Bagging is a particular type of ensemble model based on fitting many models to bootstrapped samples of the data and averaging the models. Random forest is a special type of bagging applied to decision trees. In addition to resampling the data, the random forest algorithm samples the predictor variables when splitting the trees. A useful output from the random forest is a measure of variable importance that ranks the predictors in terms of their contribution to model accuracy. The random forest has a set of hyperparameters that should be tuned using cross-validation to avoid overfitting.



### Strategies for Imbalance Data

* Undersample - Use fewer of the prevalent class records in the classification model.

* Oversample - Use more of the rare class records in the classification model, bootstrapping if necessary.

* Up weight or down weight - Attach more (or less) weight to the rare (or prevalent) class in the model.

* Data generation - Like bootstrapping, except each new bootstrapped record is slightly different from its source.

* Z-score - The value that results after standardization.

  

A variation of upsampling via bootstrapping (see ‚ÄúUndersampling‚Äù) is data generation by perturbing existing records to create new records. The intuition behind this idea is that since we only observe a limited set of instances, the algorithm doesn‚Äôt have a rich set of information to build classification ‚Äúrules.‚Äù By creating new records that are similar but not identical to existing records, the algorithm has a chance to learn a more robust set of rules.

Highly imbalanced data (i.e., where the interesting outcomes, the 1s, are rare) are problematic for classification algorithms. One strategy is to balance the training data via undersampling the abundant case (or oversampling the rare case). If using all the 1s still leaves you with too few 1s, you can bootstrap the rare cases, or use SMOTE to create synthetic data similar to existing rare cases. Imbalanced data usually indicates that correctly classifying one class (the 1s) has higher value, and that value ratio should be built into the assessment metric.



One way to deal with imbalanced class proportions during model fitting is to assign a larger penalty to wrong predictions on the minority class. Via scikit-learn, adjusting such a penalty is as convenient as setting the class_weight parameter to class_weight='balanced', which is implemented for most classifiers.

Other popular strategies for dealing with class imbalance include upsampling the minority class, downsampling the majority class, and the generation of synthetic training examples.

```python
from sklearn.utils import resample
X_upsampled, y_upsampled = resample(
									X_imb[y_imb == 1],
									y_imb[y_imb == 1],
									replace=True,
									n_samples=X_imb[y_imb == 0].shape[0],
									random_state=123)
```

Similarly, we could downsample the majority class by removing training examples from the dataset. To perform downsampling using the resample function, we could simply swap the class 1 label with class 0 in the previous code example and vice versa.

There are two popular algorithms that oversample the minority class by creating synthetic examples: the **synthetic minority oversampling technique** (SMOTE) and the **adaptive synthetic sampling method** (ADASYN).

SMOTE and ADASYN work similarly in many ways. For a given example xi of the minority class, they pick k nearest neighbors of this example (let‚Äôs denote this set of k examples Sk) and then create a synthetic example xnew as xi + (xzi - xi), where xzi is an example of the minority class chosen randomly from Sk. The interpolation hyperparameter is a random number in the range [0, 1]. Both SMOTE and ADASYN randomly pick all possible xi in the dataset. In ADASYN, the number of synthetic examples generated for each xi is proportional to the number of examples in Sk which are not from the minority class. Therefore, more synthetic examples are generated in the area where the examples of the minority class are rare.



### Dealing with categorical variables

* Dummy variables - Binary 0-1 variables derived by recording factor data for use in regression and other models.
* Reference coding - The most common type of coding used by statisticians, in which one level of a factor is used as a reference and other factors are compared to that level
* One hot encoder - A common type of coding used in machine learning community in which all factors levels are retained. While useful for a certain machine learning algorithms, this approach is not appropriate for multiple linear regression.
* Deviation coding - A type of coding that compares each level against the overall mean as opposed to the reference level.

Factor variables need to be converted into numeric variables for use in a regression. The most common method to encode a factor variable with P distinct values is to represent them using P-1 dummy variables. A factor variable with many levels, even in very big data sets, may need to be consolidated into a variable with fewer levels. Some factors have levels that are ordered and can be represented as a single numeric variable.

Because of correlation between predictors, care must be taken in the interpretation of the coefficients in multiple linear regression. Multicollinearity can cause numerical instability in fitting the regression equation. A confounding variable is an important predictor that is omitted from a model and can lead to a regression equation with spurious relationships. An interaction term between two variables is needed if the relationship between the variables and the response is interdependent.





### n-Gram Language Models

Given some starting word (say, book) we look at all the words that follow it in the source document. We randomly choose one of these to be the next word, and we repeat the process until we get to a period, which signifies the end of the sentence. What about a starting word? We can just pick randomly from words that follow a period.

```python
from collections import defaultdict
transitions = defaultdict(list)
for prev, current in zip(document, document[1:]):
 transitions[prev].append(current)

def generate_using_bigrams() -> str:
 current = "." # this means the next word will start a sentence
 result = []
 while True:
 	next_word_candidates = transitions[current] # bigrams (current, _)
 	current = random.choice(next_word_candidates) # choose one at random
 	result.append(current) # append it to results
 	if current == ".": return " ".join(result) # if "." we're done
```

```python
trigram_transitions = defaultdict(list)
starts = []
for prev, current, next in zip(document, document[1:], document[2:]):
 if prev == ".": # if the previous "word" was a period
 	starts.append(current) # then this is a start word
 trigram_transitions[(prev, current)].append(next)

def generate_using_trigrams() -> str:
 current = random.choice(starts) # choose a random starting word
 prev = "." # and precede it with a '.'
 result = [current]
 while True:
 	next_word_candidates = trigram_transitions[(prev, current)]
 	next_word = random.choice(next_word_candidates)
 	prev, current = current, next_word
 	result.append(current)
 	if current == ".":
 		return " ".join(result)
```

### TFIDF

When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. These frequently occurring words typically don't contain useful or discriminatory information. In this subsection, you will learn about a useful technique called the term frequency-inverse document frequency (tf-idf), which can be used to downweight these frequently occurring words in the feature vectors. The tf-idf can be defined as the product of the term frequency and the inverse document frequency: 
$$
ùë°f-ùëñdf(ùë°, ùëë) = ùë°f(ùë°, ùëë) √ó ùëñdf(ùë°, ùëë) 
\\
or
\\
ùë°f-ùëñdf(ùë°, ùëë) = ùë°f(ùë°, ùëë) √ó (1 + ùëñdf(ùë°, ùëë))
$$


tf(t, d)‚Äîthe number of times a term, t, occurs in a document, d. It should be noted that, in the bag-of-words model, the word or term order in a sentence or document does not matter. The order in which the term frequencies appear in the feature vector is derived from the vocabulary indices, which are usually assigned alphabetically.
$$
idf(t, d) = log(\frac{n_d}{1 + df(d, t)})
\\
or
\\
idf(t, d) = log(\frac{1 + n_d}{1 + df(d, t)})
$$
Here, nd is the total number of documents, and df(d, t) is the number of documents, d, that contain the term t. 



To make sure that we understand how TfidfTransformer works, let's walk through an example and calculate the tf-idf of the word 'is' in the third document. The word 'is' has a term frequency of 3 (tf = 3) in the third document, and the document frequency of this term is 3 since the term 'is' occurs in all three documents (df = 3). Thus, we can calculate the inverse document frequency as follows:
$$
idf('is', d_3) = log\frac{1 + 3}{1 + 3} = 0
\\
tfidf('is', 3) = 3 √ó (0 + 1) = 3
$$
If we repeated this calculation for all terms in the third document, we'd obtain the following tf-idf vectors: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0, 1.69, 1.29]. However, notice that the values in this feature vector are different from the values that we obtained from TfidfTransformer that we used previously. The final step that we are missing in this tf-idf calculation is the L2-normalization, which can be applied as follows:
$$
tfidf(d_3)_{norm} = \frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0, 1.69, 1.29]}{\sqrt{3.39^2 + 3^2 + 3.39^2 + 1.29^2 + 1.29^2 + 1.29^2 + 2^2 + 1.69^2 + 1.29^2}}=
\\
= [0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]
$$



### Word Vectors

One important innovation involves representing words as low-dimensional vectors. These vectors can be compared, added together, fed into machine learning models, or anything else you want to do with them. 

Coming up with such vectors for a large vocabulary of words is a difficult undertaking, so typically we‚Äôll learn them from a corpus of text. There are a couple of different schemes, but at a high level the task typically looks something like this:

1. Get a bunch of text. 
2. Create a dataset where the goal is to predict a word given nearby words (or alternatively, to predict nearby words given a word). 
3. Train a neural net to do well on this task. 
4. Take the internal states of the trained neural net as the word vectors.







### Contrast Normalization



One of the most obvious sources of variation that can be safely removed for many tasks is the amount of contrast in the image. Contrast simply refers to the magnitude of the difference between the bright and the dark pixels in an image.

Global contrast normalization (GCN) aims to prevent images from having varying amounts of contrast by subtracting the mean from each image, then rescaling it so that the standard deviation across its pixels is equal to some constant s. This approach is complicated by the fact that no scaling factor can change the contrast of a zero-contrast image (one whose pixels all have equal intensity). Images with very low but non-zero contrast often have little information content. Dividing by the true standard deviation usually accomplishes nothing more than amplifying sensor noise or compression artifacts in such cases.

