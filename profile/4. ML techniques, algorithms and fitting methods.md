[**Machine Learning techniques and tasks**](#Machine-Learning-techniques-and-tasks)

* [Learning to label sequence](#Learning-to-label-sequence)
* [Active learning](#Active learning)
* [Semi-supervised learning](#Semi-supervised learning)
* [One-shot learning](#One-shot learning)
* [Zero-shot learning](#Zero-shot learning)
* [Handling Multiple Inputs](#Handling-Multiple-Inputs)
* [Handling Multiple Outputs](#Handling-Multiple-Outputs)
* [Transfer Learning](#Transfer Learning)
* [Learning to Rank](#Learning to Rank)
* [Learning to Recommend](#Learning to Recommend)
* [Прогнозирование временных рядов](#Прогнозирование временных рядов)



[**Machine learning models**](#Machine-learning-models)



1. Regression models:

* [Linear Regression](#linear-regression)
* [Polynomial Regression](#Polynomial Regression)
* [RANSAC](#RANSAC)
* [SVM](#support-vector-machine)
* [Generalized Additive Models](#Generalized Additive Models)
* [K-Nearest Neighboors](#k-nearest-neighboors)
* [Decision Tree](#Decision-tree)
* [Random Forest](#Random-Forest)
* 



2. Classification models:

* [Logistic regression](#logistic-regression)
* [Generalized linear model](#Generalized linear model)
* [SVM](#support-vector-machine)
* [Naive Bayes](#naive-bayes)
* [Not so Naive Bayes](#Not so Naive Bayes)
* [K-Nearest Neighboors](#k-nearest-neighboors)
* [Decision Tree](#Decision-tree)
* [Random Forest](#Random-Forest)



3. Clustering

* [K-means](#k-means)
* [K-means++](#k-means++)
* [Hierarchical Clustering](#hierarchical-clustering)
* [Locating regions of high density via DBSCAN](#Locating regions of high density via DBSCAN)
* [Gaussian Mixture Models](#Gaussian-Mixture-Models)
* [Fuzzy K-means](#fuzzy-k-means)
* [Кластеризация с помощью минимального остовного дерева](#Кластеризация-с-помощью-минимального-остовного-дерева)



4. Dimensionality reduction

* [Principal Components Analysis](#principal-components-analysis)
* [Linear Discriminant Analysis](#Linear Discriminant Analysis)
* [Kernel PCA](#Kernel PCA)
* [UMAP](#UMAP)





[**Models fitting and estimating**](#Models-fitting-and-estimating)

* [Train/validation split](#Train/validation-split)
* [Maximum Likelihood](#Maximum Likelihood)
* [Maximum a Posteriori Estimation](#Maximum-a-Posteriori-Estimation)
* [Gradient Descent](#Gradient-Descent)
* [Newton's Method](#Newton's-Method)
* [Iteratively Reweighted Least Squares](#Iteratively-Reweighted-Least-Squares)
* [Regression metrics](#Regression metrics)
* [Regression residuals plot](#Regression-residuals-plot)
* [Classification metrics](#Classification metrics)
* [Quantifying the quality of clustering via silhouette plots](#Quantifying-the-quality-of-clustering-via-silhouette-plots)



[**Models improvement**](#Models-improvement)

* [Regularization](#regularization)
* [Hyperparameter Tuning](#Hyperparameter-Tuning)
* [Bagging](#bagging)
* [Boosting](#boosting)
* [Combining models](#Combining-models)









## Machine Learning techniques and tasks



### Learning to label sequence

Sequence labeling is the problem of automatically assigning a label to each element of a sequence. A labeled sequential training example in sequence labeling is a pair of lists (X, Y), where X is a list of feature vectors, one per time step, Y is a list of the same length of labels. For example, X could represent words in a sentence such as [“big”, “beautiful”, “car”], and Y would be the list of the corresponding parts of speech, such as [“adjective”, “adjective”, “noun”]).

Models for label sequence:

1. RNN

2. Conditional Random Fields (CRF). 

   For example, imagine we have the task of named entity extraction and we want to build a model that would label each word in the sentence such as “I go to San Francisco” with one of the following classes: {location, name, company_name, other}. If our feature vectors (which represent words) contain such binary features as “whether or not the word starts with a capital letter” and “whether or not the word can be found in the list of locations,” such features would be very informative and help to classify the words San and Francisco as location



### Sequence-to-sequence learning 

Sequence-to-sequence learning (often abbreviated as seq2seq learning) is a generalization of the sequence labeling problem. In seq2seq, Xi and Yi can have different lengths. seq2seq models have found application in machine translation (where, for example, the input is an English sentence, and the output is the corresponding French sentence), conversational interfaces (where the input is a question typed by the user, and the output is the answer from the machine), text summarization, spelling correction, and many others.

Many but not all seq2seq learning problems are currently best solved by neural networks. The network architectures used in seq2seq all have two parts: an encoder and a decoder. 

In seq2seq neural network learning, the encoder is a neural network that accepts sequential input. It can be an RNN, but also a CNN or some other architecture. The role of the encoder is to read the input and generate some sort of state (similar to the state in RNN) that can be seen as a numerical representation of the meaning of the input the machine can work with. The meaning of some entity, whether it be an image, a text or a video, is usually a vector or a matrix that contains real numbers. This vector (or matrix) is called in the machine learning jargon the embedding of the input.

The decoder is another neural network that takes an embedding as input and is capable of generating a sequence of outputs. As you could have already guessed, that embedding comes from the encoder. To produce a sequence of outputs, the decoder takes a start of sequence input feature vector x(0) (typically all zeroes), produces the first output y(1), updates its state by combining the embedding and the input x(0), and then uses the output y(1) as its next input x(1). 

More accurate predictions can be obtained using an architecture with **attention**. Attention mechanism is implemented by an additional set of parameters that combine some information from the encoder (in RNNs, this information is the list of state vectors of the last recurrent layer from all encoder time steps) and the current state of the decoder to generate the label. That allows for even better retention of long-term dependencies than provided by gated units and bidirectional RNN.

### Active learning

Active learning is an interesting supervised learning paradigm. It is usually applied when obtaining labeled examples is costly. That is often the case in the medical or financial domains, where the opinion of an expert may be required to annotate patients’ or customers’ data. The idea is to start learning with relatively few labeled examples, and a large number of unlabeled ones, and then label only those examples that contribute the most to the model quality.

There are multiple strategies of active learning. Here, we discuss only the following two: 

1. data density and uncertainty based
2. support vector-based.

The former strategy applies the current model f, trained using the existing labeled examples, to each of the remaining unlabelled examples (or, to save the computing time, to some random sample of them). For each unlabeled example x, the following importance score is computed: density(x) · uncertaintyf (x). Density reflects how many examples surround x in its close neighborhood, while uncertaintyf (x) reflects how uncertain the prediction of the model f is for x. In binary classification with sigmoid, the closer the prediction score is to 0.5, the more uncertain is the prediction. In SVM, the closer the example is to the decision boundary, the most uncertain is the prediction

In multiclass classification, entropy can be used as a typical measure of uncertainty:
$$
H_f(x) = -\sum_{c=1}^C Pr(y^{(c)}; f(x))ln[Pr(y^{(c)}; f(x))]
$$
Density for the example x can be obtained by taking the average of the distance from x to each of its k nearest neighbors (with k being a hyperparameter).

Once we know the importance score of each unlabeled example, we pick the one with the highest importance score and ask the expert to annotate it. Then we add the new annotated example to the training set, rebuild the model and continue the process until some stopping criterion is satisfied. A stopping criterion can be chosen in advance (the maximum number of requests to the expert based on the available budget) or depend on how well our model performs according to some metric.

The support vector-based active learning strategy consists in building an SVM model using the labeled data. We then ask our expert to annotate the unlabeled example that lies the closest to the hyperplane that separates the two classes. The idea is that if the example lies closest to the hyperplane, then it is the least certain and would contribute the most to the reduction of possible places where the true (the one we look for) hyperplane could lie.

### Semi-supervised learning

In semi-supervised learning (SSL) we also have labeled a small fraction of the dataset; most of the remaining examples are unlabeled. Our goal is to leverage a large number of unlabeled examples to improve the model performance without asking for additional labeled examples.

The neural network architecture that attained a remarkable performance is called a **ladder network**. To understand ladder networks you have to understand what an **autoencoder** is.

An autoencoder is a feed-forward neural network with an encoder-decoder architecture. It is trained to reconstruct its input. So the training example is a pair (x, x). We want the output xˆ of the model f(x) to be as similar to the input x as possible.

An important detail here is that an autoencoder’s network looks like an hourglass with a bottleneck layer in the middle that contains the embedding of the D-dimensional input vector; the embedding layer usually has much fewer units than D. The goal of the decoder is to reconstruct the input feature vector from this embedding.

A **denoising autoencoder** corrupts the left-hand side x in the training example (x, x) by adding some random perturbation to the features. If our examples are grayscale images with pixels represented as values between 0 and 1, usually a normal Gaussian noise is added to each feature. 

A **ladder network** is a denoising autoencoder with an upgrade. The encoder and the decoder have the same number of layers. The bottleneck layer is used directly to predict the label (using the softmax activation function). The network has several cost functions. For each layer l of the encoder and the corresponding layer l of the decoder, one cost Cl d penalizes the difference between the outputs of the two layers (using the squared Euclidean distance). When a labeled example is used during training, another cost function, Cc, penalizes the error in prediction of the label (the negative log-likelihood cost function is used).



Other semi-supervised learning techniques, not related to training neural networks, exist. One of them implies building the model using the labeled data and then cluster the unlabeled and labeled examples together using any clustering technique. For each new example, we then output as a prediction the majority label in the cluster it belongs to. Another technique, called S3VM, is based on using SVM. We build one SVM model for each possible labeling of unlabeled examples and then we pick the model with the largest margin. The paper on S3VM describes an approach that allows solving this problem without actually enumerating all possible labelings.

### One-shot learning

In one-shot learning, typically applied in face recognition, we want to build a model that can recognize that two photos of the same person represent that same person. If we present to the model two photos of two different people, we expect the model to recognize that the two people are different. 

To solve such a problem, we could go a traditional way and build a binary classifier that takes two images as input and predict either true (when the two pictures represent the same person) or false (when the two pictures belong to different people). However, in practice, this would result in a neural network twice as big as a typical neural network, because each of the two pictures needs its own embedding subnetwork. Training such a network would be challenging not only because of its size but also because the positive examples would be much harder to obtain than negative ones. So the problem is highly imbalanced.

One way to effectively solve the problem is to train a siamese neural network (SNN). An SNN can be implemented as any kind of neural network, a CNN, an RNN, or an MLP. The network only takes one image as input at a time; so the size of the network is not doubled. To obtain a binary classifier “same_person”/“not_same” out of a network that only takes one picture as input, we train the networks in a special way. To train an SNN, we use the triplet loss function. For example, let us have three images of a face: image A (for anchor), image P (for positive) and image N (for negative). A and P are two different pictures of the same person; N is a picture of another person. Each training example i is now a triplet (Ai, Pi, Ni).

Let’s say we have a neural network model f that can take a picture of a face as input and output an embedding of this picture. The triplet loss for example i is defined as,
$$
max(||f(A_i) - f(P_i)||^2 - ||f(A_i) - f(N_i)||^2 + \alpha, 0)
$$
The cost function is defined as the average triplet loss:
$$
\frac{1}{N} \sum_{i=1}^N max(||f(A_i) -f(P_i)||^2 - ||f(A_i) - f(N_i)||^2 + \alpha, 0)
$$
where – is a positive hyperparameter. Intuitively, ||f(A) - f(P)||^2 is low when our neural network outputs similar embedding vectors for A and P; ||f(A) - f(N)||^2 is high when the embedding for pictures of two different people are different.



Rather than randomly choosing an image for N, a better way to create triplets for training is to use the current model after several epochs of learning and find candidates for N that are similar to A and P according to that model. Using random examples as N would significantly slow down the training because the neural network will easily see the difference between pictures of two random people, so the average triplet loss will be low most of the time and the parameters will not be updated fast enough. 

To build an SNN, we first decide on the architecture of our neural network. For example, CNN is a typical choice if our inputs are images. Given an example, to calculate the average triplet loss, we apply, consecutively, the model to A, then to P, then to N, and then we compute the loss for that example. We repeat that for all triplets in the batch and then compute the cost; gradient descent with backpropagation propagates the cost through the network to update its parameters.

### Zero-shot learning

In zero-shot learning (ZSL) we want to train a model to assign labels to objects. The most frequent application is to learn to assign labels to images.

However, contrary to standard classification, we want the model to be able to predict labels that we didn’t have in the training data. How is that possible? The trick is to use embeddings not just to represent the input x but also to represent the output y. Imagine that we have a model that for any word in English can generate an embedding vector with the following property: if a word yi has a similar meaning to the word yk, then the embedding vectors for these two words will be similar.

For example, if yi is Paris and yk is Rome, then they will have embeddings that are similar; on the other hand, if yk is potato, then the embeddings of yi and yk will be dissimilar. Such embedding vectors are called word embeddings, and they are usually compared using cosine similarity metrics.

Now, in our classification problem, we can replace the label yi for each example i in our training set with its word embedding and train a multi-label model that predicts word embeddings. To get the label for a new example x, we apply our model f to x, get the embedding yˆ and then search among all English words those whose embeddings are the most similar to yˆ using cosine similarity.

### Handling Multiple Inputs

With neural networks, you have more flexibility. You can build two subnetworks, one for each type of input. For example, a CNN subnetwork would read the image while an RNN subnetwork would read the text. Both subnetworks have as their last layer an embedding: CNN has an embedding of the image, while RNN has an embedding of the text. You can now concatenate two embeddings and then add a classification layer, such as softmax or sigmoid, on top of the concatenated embeddings. Neural network libraries provide simple to use tools that allow concatenating or averaging layers from several subnetworks.

### Handling Multiple Outputs

In some cases the outputs are multimodal, and their combinations cannot be effectively enumerated. Consider the following example: you want to build a model that detects an object on an image and returns its coordinates. In addition, the model has to return a tag describing the object, such as “person,” “cat,” or “hamster.” Your training examples will a feature vector that represents an image. The label will be represented as a vector of coordinates of the object and another vector with a one-hot encoded tag. To handle a situation like that, you can create one subnetwork that would work as an encoder. It will read the input image using, for example, one or several convolution layers. The encoder’s last layer would be the embedding of the image. Then you add two other subnetworks on top of the embedding layer: one that takes the embedding vector as input and predicts the coordinates of an object. This first subnetwork can have a ReLU as the last layer, which is a good choice for predicting positive real numbers, such as coordinates; this subnetwork could use the mean squared error cost C1. The second subnetwork will take the same embedding vector as input and predict the probabilities for each label. This second subnetwork can have a softmax as the last layer, which is appropriate for the probabilistic output, and use the averaged negative log-likelihood cost C2 (also called cross-entropy cost).

### Transfer Learning

Transfer learning is probably where neural networks have a unique advantage over the shallow models. In transfer learning, you pick an existing model trained on some dataset, and you adapt this model to predict examples from another dataset, different from the one the model was built on. This second dataset is not like holdout sets you use for validation and test. It may represent some other phenomenon, or, as machine learning scientists say, it may come from another statistical distribution.

With neural networks, the situation is much more favorable. Transfer learning in neural networks works like this. 

1. You build a deep model on the original big dataset (wild animals). 
2. You compile a much smaller labeled dataset for your second model (domestic animals). 
3. You remove the last one or several layers from the first model. Usually, these are layers responsible for the classification or regression; they usually follow the embedding layer. 
4. You replace the removed layers with new layers adapted for your new problem. 
5. You “freeze” the parameters of the layers remaining from the first model. 
6. You use your smaller labeled dataset and gradient descent to train the parameters of only the new layers.

### Learning to Rank

Learning to rank is a supervised learning problem. Among others, one frequent problem solved using learning to rank is the optimization of search results returned by a search engine for a query. In search result ranking optimization, a labeled example Xi in the training set of size N is a ranked collection of documents of size ri (labels are ranks of documents). A feature vector represents each document in the collection. The goal of the learning is to find a ranking function f which outputs values that can be used to rank documents. For each training example, an ideal function f would output values that induce the same ranking of documents as given by the labels.

### Learning to Recommend

Learning to recommend is an approach to build recommender systems. Usually, we have a user who consumes content. We have the history of consumption and want to suggest this user new content that they would like. It could be a movie on Netflix or a book on Amazon. Traditionally, two approaches were used to give recommendations: content-based filtering and collaborative filtering.

Content-based filtering consists of learning what users like based on the description of the content they consume. For example, if the user of a news site often reads news articles on science and technology, then we would suggest to this user more documents on science and technology. More generally, we could create one training set per user and add news articles to this dataset as a feature vector x and whether the user recently read this news article as a label y. Then we build the model of each user and can regularly examine each new piece of content to determine whether a specific user would read it or not. The content-based approach has many limitations. For example, the user can be trapped in the so-called filter bubble: the system will always suggest to that user the information that looks very similar to what user already consumed. That could result in complete isolation of the user from information that disagrees with their viewpoints or expands them. On a more practical side, the users might just stop following recommendations, which is undesirable. Collaborative filtering has a significant advantage over content-based filtering: the recommendations to one user are computed based on what other users consume or rate. For instance, if two users gave high ratings to the same ten movies, then it’s more likely that user 1 will appreciate new movies recommended based on the tastes of the user 2 and vice versa. The drawback of this approach is that the content of the recommended items is ignored. In collaborative filtering, the information on user preferences is organized in a matrix. Each row corresponds to a user, and each column corresponds to a piece of content that user rated or consumed. Usually, this matrix is huge and extremely sparse, which means that most of its cells aren’t filled (or filled with a zero). The reason for such a sparsity is that most users consume or rate just a tiny fraction of available content items. It’s is very hard to make meaningful recommendations based on such sparse data.

Two effective recommender system learning algorithms are 

1. Denoising autoencoders (DAE).

2. Factorization machines (FM) . The factorization machine model is defined as follows:
   $$
   f(x) = b + \sum_{i=1}^D w_ix_i + \sum_{i=1}^D\sum_{j=i+1}^D(v_iv_j)x_ix_j
   $$
   Depending on the problem, the loss function could be squared error loss (for regression) or hinge loss. For classification with y œ {≠1, +1}, with hinge loss or logistic loss the prediction is made as y = sign(f(x)). The logistic loss is defined as,
   $$
   loss(f(x), y) = \frac{1}{ln2}ln(1 + e^{-yf(x)})
   $$
   





Вы должны взять матрицу X, заполнить все пустые ячейки средними значениями рейтинга для данного элемента (не нужно заполнять его нулями, поскольку это может означать нечто в рейтинговой системе, а SVD не может обрабатывать отсутствующие значения), а затем вычислить SVD. Теперь, когда вы произвели такое разложение, это значит, что вы захватили скрытые характеристики, которые при желании можете применять для сравнения пользователей. Но вам нужно не это — вы хотите предсказать. Перемножив U, S и Vτ , вы получите приближение A к X или предсказание , таким образом, можете прогнозировать оценку, просто просматривая запись для соответствующей пары «пользователь/ элемент» в матрице X. 





### Прогнозирование временных рядов

**Ресурсы:** Hyndman R.J., Athanasoupouls G. - Forecasting: principles and practice

Эконометрика - основной источник задач прогнозирования.

Основные явления в эконометрических временных рядах:

1. Тренд - главное долгосрочное изменение уровня ряда
2. Сезонности - циклическое изменение уровня ряда с постоянным периодом
3. Разладки (смены модели ряда)
4. Цикл - изменение уровня ряда с переменным периодом (экономические циклы, периоды солнечной активности)
5. Ошибка - непрогнозируемая случайная компонента ряда

В роли признаков - n предыдущих наблюдений ряда:
$$
\hat y _{t+1}(w) = \sum_{j=1}^n w_jy_{t-j+1}
$$
В роли объектов - l = t - n + 1 моментов в истории ряда.

**Автокорреляция** - с ее помощью можно квантифицировать степень сходства между значениями ряда в соседних точках (например в соседних месяцах):
$$
r_{\tau} = \frac{E((y_t - Ey)(y_{t+\tau} - Ey))}{Dy}
\\
по выборке:
\\
r_{\tau} = \frac{\sum_{t=1}^{T- \tau} (y_t - \bar y)(y_{t+\tau} - \bar y)}{\sum_{t=1}^T (y_t -\bar y)^2}
$$
\tau - лаг автокорреляции.

По сути автокорреляция - обычная корреляция Пирсона между исходным рядом и его версией, сдвинутой на несколько отсчетов (например месяцев), количество отсчетов называется лаг автокорреляции.

Отдельные временные ряды не имеют тенденции или циклической компоненты, каждый их следующий уровень равен сумме среднего уровня ряда и случайной компоненты. Такие ряды носят название **стационарные ряды.**

Проверить стационарен ли ряд можно с помощью критерия Дики-Фуллера и KPSS.

Если ряд не стационарен можно привести его к такому:

1. Стабилизация дисперсии. - Для монотонно меняющейся дисперсии можно использовать стабилизирующее преобразование (часто используют логарифмирование). 
2. Дифференцирование - переход к попарным разностям соседних значений. Это позволяет стабилизировать среднее значение ряда и избавиться от тренда. (Может применяться неоднократно).

**ARMA**

**Авторегрессия** - будем делать регрессию ряда не на какие то внешние признаки, зависящие от времени, а непосредственно на его собственные значения в прошлом:
$$
y_t = \alpha + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \epsilon_t
$$
Такая модель называется моделью авторегрессии порядка р - р(AR(p)): y_t - линейная комбинация р предыдущих значений ряда и шумовой компоненты. 

**Модели скользящего среднего** -  функция, значение который в каждой точке определения равны некоторому среднему значению исходной функции за предыдущий период. Обобщим модель:
$$
y_t = \alpha + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ... + \theta_q \epsilon_{t-q}
$$
Такая модель называется модель скользящего среднего порядка q(MA(q)): y_t - линейная комбинация q последних значений шумовой компоненты.

**ARMA(p, q)**:
$$
y_t = \alpha + \phi_1 y_{t-1} + ... + \phi_p y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + ... + \theta_q \epsilon_{t-q}
$$
**Теорема Вольда**: Любой стационарный временной ряд может быть описан моделью ARMA(p, q).



**ARIMA(p, d, q)** -  модель ARMA(p, q) для d раз продифференцированного ряда.

https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/

**SARMA(p, q) * (P,Q)** - модель ARMA + P авторегрессионных компонент + Q компонент скользящего окна. (Пусть ряд имеет сезонный период длины S)
$$
авторегрессионные: +\phi_Sy_{t-S} + \phi_{2S}y_{t-2S} + ... + \phi_{PS}y_{t-PS}
\\
скользящегосреднего: +\theta_S\epsilon_{t-S} + \theta_{2S}\epsilon_{t-2S} + ... + \theta_{PS}\epsilon_{t-PS}
$$
**SARIMA(p, d, q) * (P, D, Q)** - модель SARMA для ряда, к которому d раз было применено обычное дифференцирование и D раз - сезонное.



**Настройка параметров:**

1. \alpha, \phi, \theta - если все остальные параметры фиксированны, коэффициенты подбираются МНК. Чтобы найти коэффициенты \theta, шумовая компонента предварительно оценивается с помощью остатков авторегрессии.
2. d, D (порядки дифференцирования) - подбираются так, чтобы ряд стал стационарным. Если ряд сезонный, рекомендуется начинать с сезонного дифференцирования (D). Чем меньше раз мы продифференцируем, тем меньше будет дисперсия итогового прогноза.
3. q, Q, p, P - для сравнения моделей с разными гиперпараметрами можно использовать критерий Акаике. Начальное приближение можно выбрать с помощью автокорреляции.

- **p**: Trend autoregression order.
- **d**: Trend difference order.
- **q**: Trend moving average order. - максимальный значимый лаг
- **P**: Seasonal autoregressive order. - последний значимый сезонный лаг
- **D**: Seasonal difference order. 
- **Q**: Seasonal moving average order.
- **m**: The number of time steps for a single seasonal period.

**Подбор ARIMA:**

1. Смотрим на ряд
2. При необходимости стабилизируем дисперсию (преборазование Бокса-Кокса/логарифмирование)
3. Если ряд нестационарен, подбираем порядок дифференцирования
4. Анализируем графики автокорреляционной функции и частичной автокорреляционной функции, определяем примерные значения p, q, P, Q.
5. Обучаем модели-кандидаты, сравниваем их по критерию Акаике, выбираем победителя
6. Смотрим на остатки полученной модели, если они плохие пробуем что то поменять







# Machine learning models



### Linear Regression

* Response - The variable we are trying to predict.
* Independent variable - The variable used to predict to response.
* Regression coefficient - The slope of the regression line.
* Fitted values - The estimates Yi obtained from the regression line.
* Residuals - The difference between the observed values and the fitted values.
* Least squares - The method of fitting a regression by minimizing the sum of squared residuals.

The regression equation models the relationship between a response variable Y and a predictor variable X as a line. A regression model yields fitted values and residuals—predictions of the response and the errors of the predictions. Regression models are typically fit by the method of least squares. Regression is used both for prediction and explanation.

A linear model makes predictions by computing a weighted sum of the input feature, plus constant called the *bias term* (or *intercept term*):

$$
y = w_0 + w_1x_1 + w_2x_2 + ... +w_nx_n + \epsilon
$$


where a - parameter vector (containing bias term), x - feature vector

So the task is to compute vector **a**, we can do this in different ways:

1. The normal equation. It is the *closet form solution* that gives result directly: 
   $$
   w = (x^Tx)^{-1}x^Ty
   $$

   The computational complexity of this approach is between O(n<sup>2.4</sup>) and O(n<sup>3</sup>).

2. Using optimization methods such as Gradient Descent, SGD, Mini-batch GD.

**Implementation of linear regression can be found in python-playground/mini_projects/ml_models_implementations/linear_models.py**



### Polynomial Regression

It is linear regression with powers of each features as a new feature. Polynomial Regression can fit non-linear relationship in data.

```python
from sklearn.preprocessing import PolynomialFeatures 
poly_features = PolynominalFeatures(degree = 2, include_bias = False) 
X_poly = poly_features.fit_transform(X) #add square of each feature

lin_reg = LinearRegression()
lin_reg.fit(X_poly)
```

Also PolynomialFeatures() is capable of finding relationships between features, because it adds all combination of features up to the given degree. PolynomialFeatures() transform array that containing **n** features to array containing **(n+degree)! / (d! n!)** features. 



### RANSAC

Linear regression models can be heavily impacted by the presence of outliers. In certain situations, a very small subset of our data can have a big effect on the estimated model coefficients. There are many statistical tests that can be used to detect outliers. However, removing outliers always requires our own judgment as data scientists as well as our domain knowledge. As an alternative to throwing out outliers, we will look at a robust method of regression using the RANdom SAmple Consensus (RANSAC) algorithm, which fits a regression model to a subset of the data, the so-called inliers. We can summarize the iterative RANSAC algorithm as follows:

1. Select a random number of examples to be inliers and fit the model. 
2. Test all other data points against the fitted model and add those points that fall within a user-given tolerance to the inliers. 
3. Refit the model using all inliers. 
4. Estimate the error of the fitted model versus the inliers. 
5. Terminate the algorithm if the performance meets a certain user-defined threshold or if a fixed number of iterations were reached; go back to step 1 otherwise.



### Generalized Additive Models

Suppose you suspect a nonlinear relationship between the response and a predictor variable, either by a priori knowledge or by examining the regression diagnostics. Polynomial terms may not flexible enough to capture the relationship, and spline terms require specifying the knots. Generalized additive models, or GAM, are a technique to automatically fit a spline regression.





### Logistic regression

**Logistic regression is a special instance of a generalized linear model** (GLM) developed to extend linear regression to other settings.

In linear regression, we minimized the empirical risk which was defined as the average squared error loss, also known as the mean squared error or MSE. 

In logistic regression, on the other hand, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function defines how likely the observation (an example) is according to our model. **Likelihood, L:**
$$
L(\bold w) = P(\bold y | \bold{x;w}) = \prod_{i=1}^{n} P(y^{(i)} | x^{(i)}; \bold w) = \prod_{i=1}^{n} (\phi(z^{i}))^{y^{(i)}} (1 - \phi (z^{(i)}))^{1-y^{(i)}}
$$
We used the product operator in the objective function instead of the sum operator which was used in linear regression. It’s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case).

In practice, it is easier to maximize the (natural) log of this equation, which is called the **log-likelihood function**:
$$
l(\bold w) = logL(\bold w) = \sum_{i=1}^{n} [y^{(i)}log(\phi(z^{(i)})) + (1-y^{(i)})log(1-\phi(z^{(i)}))] \\ 
\\
J(\bold w) = \sum_{i=1}^{n} [-y^{(i)}log(\phi(z^{(i)})) - (1-y^{(i)})log(1-\phi(z^{(i)}))]
$$
Since maximizing the log-likelihood is equal to minimizing the cost function, J, we can write the gradient descent update rule as follows:
$$
\bold w = \bold w + (-\eta \frac{dJ}{dw_j}) = \bold w + \eta \sum_{i=1}^{n} (y^{(i)} - \phi(z^{(i)}))x_j^{(i)}
$$

The logit function takes input values in the range 0 to 1 and transforms them to values over the entire real-number range, which we can use to express a linear relationship between feature values and the log-odds:
$$
logit(p(y=1|\bold x)) = \sum_{i=0}^{m}w_ix_i = \bold w^T \bold x 
$$
Now, we are actually interested in predicting the probability that a certain example belongs to a particular class, which is the inverse form of the logit function.  It is also called the logistic sigmoid function, which is sometimes simply abbreviated to sigmoid function due to its characteristic S-shape:
$$
\phi(z) = \frac{1}{1+e^{-z}}
$$
**Implementation of logistic regression can be found in python-playground/mini_projects/ml_models_implementations/linear_models.py**



### Generalized linear model

Generalized linear models (GLMs) are the second most important class of models besides regression. GLMs are characterized by two main components: 

* A probability distribution or family (binomial in the case of logistic regression) 
* A link function mapping the response to the predictors (logit in the case of logistic regression)

Logistic regression is by far the most common form of GLM. A data scientist will encounter other types of GLMs. Sometimes a log link function is used instead of the logit; in practice, use of a log link is unlikely to lead to very different results for most applications. The poisson distribution is commonly used to model count data (e.g., the number of times a user visits a web page in a certain amount of time). Other families include negative binomial and gamma, often used to model elapsed time (e.g., time to failure). In contrast to logistic regression, application of GLMs with these models is more nuanced and involves greater care. These are best avoided unless you are familiar with and understand the utility and pitfalls of these methods.



### Support Vector Machine

Another powerful and widely used learning algorithm is the support vector machine (SVM), which can be considered an extension of the perceptron. Using the perceptron algorithm, we minimized misclassification errors. However, in SVMs our optimization objective is to maximize the margin. The margin is defined as the distance between the separating hyperplane (decision boundary) and the training examples that are closest to this hyperplane, which are the so-called support vectors. 

The basic idea behind **kernel methods** to deal with such linearly inseparable data is to create nonlinear combinations of the original features to project them onto a higher-dimensional space via a mapping function, 𝜙, where the data becomes linearly separable. 

To solve a nonlinear problem using an SVM, we would transform the training data into a higher-dimensional feature space via a mapping function, 𝜙, and train a linear SVM model to classify the data in this new feature space. Then, we could use the same mapping function, 𝜙, to transform new, unseen data to classify it using the linear SVM model. 

However, one problem with this mapping approach is that the construction of the new features is computationally very expensive, especially if we are dealing with high-dimensional data. This is where the so-called kernel trick comes into play. 

In order to save the expensive step of calculating this dot product between two points explicitly, we define a so-called kernel function:
$$
k(\bold x^{(i)}, \bold x^{(j)}) = \phi(x^{(i)})^T\phi(x^{(j)})
$$
One of the most widely used kernels is the radial basis function (RBF) kernel, which can simply be called the Gaussian kernel:
$$
k(\bold x^{(i)}, \bold x^{(j)}) = exp(-\frac {||x^{(i)}- x^{(j)}||^2}{2\sigma^2} )
\\
or
\\
k(\bold x^{(i)}, \bold x^{(j)}) = exp(-\gamma||x^{(i)}- x^{(j)}||^2)
$$

For example, it can be shown that linear function used by the SVM can be re-written as:
$$
\bold w^T \bold x + b = b + \sum_{i=1}^m \alpha_i \bold x^T \bold x
$$
where \alpha is a vector coefficients. Rewriting the learning algorithm this way allows us to replace x by the output of a given feature function φ(x) and the dot product with a function k(x, x^i) = φ(x)·φ(x^i) called kernel. The · operator represents inner product analogous to φ(x)^T φ(x^i).
$$
f(x) = b + \sum_{i} \alpha_i k(x, x^{(i)})
$$


### Naive Bayes

* Conditional probability - The probability of observing some event (say X = i) given some other event (say Y = i) written as P(X_i | Y_i)

* Posterior probability - The probability of an outcome after the predictor information has been incorporated (in contrast to the prior probability of outcomes, not taking predictor information into account)

  

The Naive Bayes algorithm uses the probability of observing predictor values, given an outcome, to estimate the probability of observing outcome Y = i, given a set of predictor values.

To understand Bayesian classification, we can start out by imagining “non-naive” Bayesian classification. For each record to be classified: 

1. Find all the other records with the same predictor profile (i.e., where the predictor values are the same). 
2. Determine what classes those records belong to and which class is most prevalent (i.e., probable). 
3. Assign that class to the new record.

The preceding approach amounts to finding all the records in the sample that are exactly like the new record to be classified in the sense that all the predictor values are identical.

In the naive Bayes solution, we no longer restrict the probability calculation to those records that match the record to be classified. Instead, we use the entire data set. The naive Bayes modification is as follows: 

1. For a binary response Y = i (i = 0 or 1), estimate the individual conditional probabilities for each predictor  P(X_j | Y = i); these are the probabilities that the predictor value is in the record when we observe Y = i. This probability is estimated by the proportion of X_j values among the Y = i records in the training set. 
2. Multiply these probabilities by each other, and then by the proportion of records belonging to Y = i. 
3. Repeat steps 1 and 2 for all the classes. 
4. Estimate a probability for outcome i by taking the value calculated in step 2 for class i and dividing it by the sum of such values for all classes. 
5. Assign the record to the class with the highest probability for this set of predictor values.

From the definition, we see that the Bayesian classifier works only with categorical predictors (e.g., with spam classification, where presence or absence of words, phrases, characters, and so on, lies at the heart of the predictive task). To apply naive Bayes to numerical predictors, one of two approaches must be taken: 

* Bin and convert the numerical predictors to categorical predictors and apply the algorithm of the previous section. 
* Use a probability model—for example, the normal distribution (see “Normal Distribution”)—to estimate the conditional probability P(X_j | Y = i)

Naive Bayes works with categorical (factor) predictors and outcomes. It asks, “Within each outcome category, which predictor categories are most probable?” That information is then inverted to estimate probabilities of outcome categories, given predictor values.

**When to Use Naive Bayes** 

Because naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model. That said, they have several advantages:

* They are extremely fast for both training and prediction 
* They provide straightforward probabilistic prediction 
* They are often very easily interpretable 
* They have very few (if any) tunable parameters

Naive Bayes classifiers tend to perform especially well in one of the following situations:

* When the naive assumptions actually match the data (very rare in practice) 
* For very well-separated categories, when model complexity is less important 
* For very high-dimensional data, when model complexity is less important

**Пример**

Появление слова viagra увеличит вероятность того, что письмо — спам. Но это еще не окончательно. Нам нужно узнать остальное содержание данного письма. Для начала будем сосредотачиваться только на одном слове за раз, его мы обозначим как word. Затем, применив закон Байеса, мы получим:
$$
p(spam|word) = \frac{p(word|spam)p(spam)}{p(word)}
\\
p(word) = p(word|spam)p(spam) + p(word|ham)p(ham)
$$
Мы моделируем слова независимо от других слов (это также называется «независимые испытания»), так что берем результат в правой части предыдущей формулы и не подсчитываем, сколько раз встречаются эти слова. Вот почему данный метод называется наивным: мы знаем, что на самом деле есть определенные слова, которые, как правило, появляются вместе, но мы игнорируем данный факт.



### Not so Naive Bayes

For naive Bayes, the generative model is a simple axis-aligned Gaussian. With a density estimation algorithm like KDE, we can remove the “naive” element and perform the same classification with a more sophisticated generative model for each class. It’s still Bayesian classification, but it’s no longer naive. 

The general approach for generative classification is this: 

1. Split the training data by label. 
2. For each set, fit a KDE to obtain a generative model of the data. This allows you for any observation x and label y to compute a likelihood P(x|y) . 
3. From the number of examples of each class in the training set, compute the class prior, P(y) . 
4. For an unknown point x, the posterior probability for each class is P(y|x) ֣~P(x|y)P(y). The class that maximizes this posterior is the label assigned to the point.

One benefit of such a generative classifier is interpretability of results: for each unknown sample, we not only get a probabilistic classification, but a full model of the distribution of points we are comparing it to! If desired, this offers an intuitive window into the reasons for a particular classification that algorithms like SVMs and random forests tend to obscure.

**Implementation can be found in python-playground/mini_projects/ml_models_implementations/kde_classifier.py**



### K-Nearest Neighboors

The idea behind K-Nearest Neighbors (KNN) is very simple. For each record to be classified or predicted: 

1. Find K records that have similar features (i.e., similar predictor values). 
2. For classification: Find out what the majority class is among those similar records, and assign that class to the new record. 
3. For prediction (also called KNN regression): Find the average among those similar records, and predict that average for the new record.

KNN as feature engine:

1. KNN is run on the data, and for each record, a classification (or quasiprobability of a class) is derived. 
2. That result is added as a new feature to the record, and another classification method is then run on the data. The original predictor variables are thus used twice.





### Decision Tree



* Recursive partitioning - Repeatedly dividing and subdividing the data with the goal of making the outcome in each final subdivision as homogenous as possible.
* Split value - A predictor value that divides the records into those where that predictor is less than the split value, and those there it is more.
* Node - In the decision tree, a graphical or rule representation of a split value.
* Impurity - The extent to which a mix of classes is found in a subpartition of the data.
* Pruning - The process of taking a fully grown tree and progressively cutting its brunches back, to reduce overfitting.

A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a specific feature j of the feature vector is examined. If the value of the feature is below a specific threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs.

Suppose we have a response variable Y and a set of P predictor variables X for j = 1, 2 ... P. For a partition A of records, recursive partitioning will find the best way to partition A into two subpartitions: 

1. For each predictor variable X , 

   * For each value s of X :

     * Split the records in A with X values < s as one partition, and the remaining records where X ≥ s as another partition. 

     * Measure the homogeneity of classes within each subpartition of A. 

   * Select the value of s that produces maximum within-partition homogeneity of class.

2. Select the variable X and the split value s that produces maximum within-partition homogeneity of class. 

Now comes the recursive part: 

1. Initialize A with the entire data set. 
2. Apply the partitioning algorithm to split A into two subpartitions, A1 and A2 . 
3. Repeat step 2 on subpartitions A1 and A2. 
4. The algorithm terminates when no further partition can be made that sufficiently improves the homogeneity of the partitions.

In order to split the nodes at the most informative features, we need to define an objective function that we want to optimize via the tree learning algorithm. Here, our objective function is to maximize the IG at each split, which we define as follows:
$$
IG(D_p, f) = I(D_p) - \sum_{i=1}^m \frac{N_j}{N_p}I(D_j)
$$
Here, f is the feature to perform the split; 𝐷𝑝 and 𝐷𝑗 are the dataset of the parent and jth child node; I is our impurity measure; 𝑁𝑝 is the total number of training examples at the parent node; and 𝑁𝑗 is the number of examples in the jth child node. As we can see, the information gain is simply the difference between the impurity of the parent node and the sum of the child node impurities—the lower the impurities of the child nodes, the larger the information gain. However, for simplicity and to reduce the combinatorial search space, most libraries (including scikit-learn) implement binary decision trees:
$$
IG(D_p, f) = I(D_p) - \frac{N_{left}}{N_p}I(D_{left}) - \frac{N_{right}}{N_p}I(D_{right})
$$
The three impurity measures or splitting criteria that are commonly used in binary decision trees are Gini impurity (𝐼𝐺), entropy (𝐼𝐻 ), and the classification error (𝐼𝐸). Let's start with the definition of entropy for all non-empty classes (𝑝(𝑖|𝑡) ≠ 0):
$$
I_H(t) = - \sum_{i=1}^c p(i|t)log_2p(i|t)
$$
Here, 𝑝𝑝(𝑖𝑖|𝑡𝑡) is the proportion of the examples that belong to class i for a particular node, t.

When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S-, S+), is simply a weighted sum of two entropies:
$$
H(S_{-}, S_{+}) = \frac{|S_{-}|}{|S|}H(S_{-}) + \frac{|S_{+}|}{|S|}H(S_{+})
$$
So, at each step, at each leaf node, we find a split that minimizes the entropy or we stop at this leaf node.

The Gini impurity can be understood as a criterion to minimize the probability of misclassification:
$$
I_G(t) = \sum_{i=1}^c p(i|t)(1 - p(i|t)) = 1 - \sum_{i=1}^c p(i|t)^2
$$



**CART** - tree for regression.

![](img/decision-tree-cost.png)

When we used decision trees for classification, we defined entropy as a measure of impurity to determine which feature split maximizes the information gain (IG). To use a decision tree for regression, however, we need an impurity metric that is suitable for continuous variables, so we define the impurity measure of a node, t, as the MSE instead:
$$
I(t) = MSE(t) = \frac{1}{N_t} \sum_{i э D_t} (y^{(i)} - \hat y ^{(i)})^2
\\
IG(D_p, x_i) = I(D_p) - \frac{N_{left}}{N_p}I(D_{left}) - \frac{N_{right}}{N_p}I(D_{right})\\
\hat y_t = \frac{1}{N_t}\sum y^{(i)}
$$
Here, 𝑁𝑡 is the number of training examples at node t, 𝐷𝑡 is the training subset at node t, 𝑦(𝑖) is the true target value, and 𝑦̂𝑡 is the predicted target value.

In the context of decision tree regression, the MSE is often referred to as within node variance, which is why the splitting criterion is also better known as variance reduction.



**Pruning**

Pruning consists of going back through the tree once it’s been created and removing branches that don’t contribute significantly enough to the error reduction by replacing them with leaf nodes.




### Random Forest

The random forest is based on applying bagging to decision trees with one important extension: in addition to sampling the records, the algorithm also samples the variables.  The random forest algorithm adds two more steps: the bagging and the bootstrap sampling of variables at each split: 

1. Take a bootstrap (with replacement) subsample from the records. 

2. For the first split, sample p < P variables at random without replacement. 

3. For each of the sampled variables Xj1, Xj2, ..., Xjp, apply the splitting algorithm:

   * For each value s_jk of X_jk:
     * Split the records in partition A with X < s as one partition, and the remaining records where X >= s as another partition. 
     * Measure the homogeneity of classes within each subpartition of A.

   * Select the value of s_jk that produces maximum within-partition homogeneity of class

4. Select the variable X_jk and the split value s_jk that produces maximum within-partition homogeneity of class. 
5. Proceed to the next split and repeat the previous steps, starting with step 2. 
6. Continue with additional splits following the same procedure until the tree is grown. 
7. Go back to step 1, take another bootstrap subsample, and start the process over again.

The random forest algorithm can be summarized in four simple steps: 

1. Draw a random bootstrap sample of size n (randomly choose n examples from the training dataset with replacement). 
2. Grow a decision tree from the bootstrap sample. At each node: 
   * Randomly select d features without replacement. 
   * Split the node using the feature that provides the best split according to the objective function, for instance, maximizing the information gain. 
3. Repeat the steps 1-2 k times. 
4. Aggregate the prediction by each tree to assign the class label by majority vote.











### K-means

The k-means algorithm belongs to the category of prototype-based clustering. 

Prototype-based clustering means that each cluster is represented by a prototype, which is usually either the centroid (average) of similar points with continuous features, or the medoid (the most representative or the point that minimizes the distance to all other points that belong to a particular cluster) in the case of categorical features. 

Thus, our goal is to group the examples based on their feature similarities, which can be achieved using the k-means algorithm, as summarized by the following four steps: 

1. Randomly pick k centroids from the examples as initial cluster centers. 
2. Assign each example to the nearest centroid, 𝜇(𝑗) ,𝑗 ∈ {1, … , 𝑘}. 
3. Move the centroids to the center of the examples that were assigned to it. 
4. Repeat steps 2 and 3 until the cluster assignments do not change or a user defined tolerance or maximum number of iterations is reached.

K-Means also can be described as a E-M algorithm.

Expectation–maximization (E–M) is a powerful algorithm that comes up in a variety of contexts within data science. k-means is a particularly simple and easy-to understand application of the algorithm, and we will walk through it briefly here. In short, the expectation–maximization approach consists of the following procedure: 

1. Guess some cluster centers 

2. Repeat until converged 

   * E-Step: assign points to the nearest cluster center 

   * M-Step: set the cluster centers to the mean

     

### K-means++

So far, we have discussed the classic k-means algorithm, which uses a random seed to place the initial centroids, which can sometimes result in bad clustering or slow convergence if the initial centroids are chosen poorly. One way to address this issue is to run the k-means algorithm multiple times on a dataset and choose the best performing model in terms of the SSE. 

Another strategy is to place the initial centroids far away from each other via the k-means++ algorithm, which leads to better and more consistent results than the classic k-means.

The initialization in k-means++ can be summarized as follows:

1. Initialize an empty set, M, to store the k centroids being selected. 

2. Randomly choose the first centroid, 𝝁(𝑗) , from the input examples and assign it to M. 

3. For each example, 𝒙(𝑖) , that is not in M, find the minimum squared distance, 𝑑(𝒙(𝑖) , 𝚳)^2 , to any of the centroids in M. 

4. To randomly select the next centroid, 𝝁^(𝑝) , use a weighted probability distribution equal to
   $$
   \frac{d(\bold \mu^{(p)}, \bold M)^2}{\sum_i d(\bold x^{(i)}, \bold M)^2}
   $$

5. Repeat steps 2 and 3 until k centroids are chosen. 

6. Proceed with the classic k-means algorithm.





### Hierarchical Clustering

* Dendrogram - A visual representation of the records and the hierarchy of clusters to which they belong.

* Dissimilarity - A measure of how close one cluster is to another.

Hierarchical clustering starts by setting each record as its own cluster and iterates to combine the least dissimilar clusters. The main algorithm for hierarchical clustering is the agglomerative algorithm, which iteratively merges similar clusters. 

The main steps of the agglomerative algorithm are: 

1. Create an initial set of clusters with each cluster consisting of a single record for all records in the data. 
2. Compute the dissimilarity D(C_k, C_l) between all pairs of clusters k, l. 
3. Merge the two clusters C_k and C_l that are least dissimilar as measured by D(C_k, C_l). 
4. If we have more than one cluster remaining, return to step 2. Otherwise, we are done.

There are four common measures of dissimilarity: complete linkage(1), single linkage(2), average linkage, and minimum variance.
$$
D(A, B) = max(d(a_i, b_i)) for:i,j
\\
D(A, B) = min(d(a_i, b_i)) for:i,j
\\
$$
The average linkage method is the average of all distance pairs and represents a compromise between the single and complete linkage methods. Finally, the minimum variance method, also referred to as Ward’s method, is similar to K-means since it minimizes the within-cluster sum of squares.

Start with every record in its own cluster. Progressively, clusters are joined to nearby clusters until all records belong to a single cluster (the agglomerative algorithm). The agglomeration history is retained and plotted, and the user (without specifying the number of clusters beforehand) can visualize the number and structure of clusters at different stages. Inter-cluster distances are computed in different ways, all relying on the set of all inter-record distances.

The two main approaches to hierarchical clustering are agglomerative and divisive hierarchical clustering. In divisive hierarchical clustering, we start with one cluster that encompasses the complete dataset, and we iteratively split the cluster into smaller clusters until each cluster only contains one example. In this section, we will focus on agglomerative clustering, which takes the opposite approach. We start with each example as an individual cluster and merge the closest pairs of clusters until only one cluster remains.

The two standard algorithms for agglomerative hierarchical clustering are single linkage and complete linkage. Using single linkage, we compute the distances between the most similar members for each pair of clusters and merge the two clusters for which the distance between the most similar members is the smallest. The complete linkage approach is similar to single linkage but, instead of comparing the most similar members in each pair of clusters, we compare the most dissimilar members to perform the merge. 



### Locating regions of high density via DBSCAN

Density-based spatial clustering of applications with noise (DBSCAN), which does not make assumptions about spherical clusters like k-means, nor does it partition the dataset into hierarchies that require a manual cut-off point. As its name implies, density-based clustering assigns cluster labels based on dense regions of points. In DBSCAN, the notion of density is defined as the number of points within a specified radius, 𝜀𝜀.

According to the DBSCAN algorithm, a special label is assigned to each example (data point) using the following criteria: 

* A point is considered a core point if at least a specified number (MinPts) of neighboring points fall within the specified radius, 𝜀𝜀. 
* A border point is a point that has fewer neighbors than MinPts within ε, but lies within the 𝜀𝜀 radius of a core point. 
* All other points that are neither core nor border points are considered noise points.

After labeling the points as core, border, or noise, the DBSCAN algorithm can be summarized in two simple steps: 

1. Form a separate cluster for each core point or connected group of core points. (Core points are connected if they are no farther away than 𝜀𝜀.) 
2. Assign each border point to the cluster of its corresponding core point.

With an increasing number of features in our dataset—assuming a fixed number of training examples—the negative effect of the curse of dimensionality increases. This is especially a problem if we are using the Euclidean distance metric. However, the problem of the curse of dimensionality is not unique to DBSCAN: it also affects other clustering algorithms that use the Euclidean distance metric, for example, k-means and hierarchical clustering algorithms. In addition, we have two hyperparameters in DBSCAN (MinPts and 𝜀𝜀) that need to be optimized to yield good clustering results. Finding a good combination of MinPts and 𝜀𝜀 can be problematic if the density differences in the dataset are relatively large.

DBSCAN is a density-based clustering algorithm. Instead of guessing how many clusters you need, by using DBSCAN, you define two hyperparameters: ‘ and n. You start by picking an example x from your dataset at random and assign it to cluster 1. Then you count how many examples have the distance from x less than or equal to ‘. If this quantity is greater than or equal to n, then you put all these ‘-neighbors to the same cluster 1. You then examine each member of cluster 1 and find their respective ‘-neighbors. If some member of cluster 1 has n or more ‘-neighbors, you expand cluster 1 by putting those ‘-neighbors to the cluster. You continue expanding cluster 1 until there are no more examples to put in it. In the latter case, you pick from the dataset another example not belonging to any cluster and put it to cluster 2. You continue like this until all examples either belong to some cluster or are marked as outliers. An outlier is an example whose ‘-neighborhood contains less than n examples.



### Gaussian Mixture Models

A Gaussian mixture model (GMM) attempts to find a mixture of multidimensional Gaussian probability distributions that best model any input dataset.  In the simplest case, GMMs can be used for finding clusters in the same manner as k-means.

Under the hood, a Gaussian mixture model is very similar to k-means: it uses an expectation–maximization approach that qualitatively does the following: 

1. Choose starting guesses for the location and shape 
2. Repeat until converged: 
   * E-step: for each point, find weights encoding the probability of membership in each cluster
   * M-step: for each cluster, update its location, normalization, and shape based on all data points, making use of the weights

The result of this is that each cluster is associated not with a hard-edged sphere, but with a smooth Gaussian model. Just as in the k-means expectation–maximization approach, this algorithm can sometimes miss the globally optimal solution, and thus in practice multiple random initializations are used.

**GMM as Density Estimation Though** 

GMM is often categorized as a clustering algorithm, fundamentally it is an algorithm for density estimation. That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic model describing the distribution of the data.

**How many components?** 

The fact that GMM is a generative model gives us a natural means of determining the optimal number of components for a given dataset. A generative model is inherently a probability distribution for the dataset, and so we can simply evaluate the likelihood of the data under the model, using cross-validation to avoid overfitting. Another means of correcting for overfitting is to adjust the model likelihoods using some analytic criterion such as the Akaike information criterion (AIC) or the Bayesian information criterion (BIC). Scikit-Learn’s GMM estimator actually includes built-in methods that compute both of these, and so it is very easy to operate on this approach.

Notice the important point: this choice of number of components measures how well GMM works as a density estimator, not how well it works as a clustering algorithm. 



### Fuzzy K-means



Hard clustering describes a family of algorithms where each example in a dataset is assigned to exactly one cluster, as in the k-means and k-means++ algorithms that we discussed earlier in this chapter. In contrast, algorithms for soft clustering (sometimes also called fuzzy clustering) assign an example to one or more clusters. A popular example of soft clustering is the fuzzy C-means (FCM) algorithm (also called soft k-means or fuzzy k-means).  Almost a decade later, James C. Bedzek published his work on the improvement of the fuzzy clustering algorithm, which is now known as the FCM algorithm.

The FCM procedure is very similar to k-means. However, we replace the hard cluster assignment with probabilities for each point belonging to each cluster. Here, each value falls in the range [0, 1] and represents a probability of membership of the respective cluster centroid. The sum of the memberships for a given example is equal to 1. As with the k-means algorithm, we can summarize the FCM algorithm in four key steps: 

1. Specify the number of k centroids and randomly assign the cluster memberships for each point. 
2. Compute the cluster centroids, 𝝁(𝑗) ,𝑗 ∈ {1, … , 𝑘}. 
3. Update the cluster memberships for each point. 
4. Repeat steps 2 and 3 until the membership coefficients do not change or a user-defined tolerance or maximum number of iterations is reached.

The objective function of FCM—we abbreviate it as 𝐽𝑚—looks very similar to the within-cluster SSE that we minimize in k-means:
$$
J_m = \sum_{i=1}^n \sum_{j=1}^k w^{(i, j) ^m}||\bold x^{(i)} - \mu^{(j)}||^2_2
$$
However, note that the membership indicator, 𝑤𝑤(𝑖𝑖,𝑗𝑗) , is not a binary value as in k-means (𝑤𝑤(𝑖𝑖,𝑗𝑗) ∈ {0, 1}), but a real value that denotes the cluster membership probability (𝑤𝑤(𝑖𝑖,𝑗𝑗) ∈ [0, 1]). You also may have noticed that we added an additional exponent to 𝑤𝑤(𝑖𝑖,𝑗𝑗) ; the exponent m, any number greater than or equal to one (typically m = 2), is the so-called fuzziness coefficient (or simply fuzzifier), which controls the degree of fuzziness.

The larger the value of m, the smaller the cluster membership, 𝑤𝑤(𝑖𝑖,𝑗𝑗) , becomes, which leads to fuzzier clusters. The cluster membership probability itself is calculated as follows:
$$
w^{(i, j)} = [\sum_{c=1}^{k} (\frac{||\bold x ^ {(i)} - \bold \mu ^ {(j)} ||_2}{||\bold x ^ {(i)} - \bold \mu ^ {(c)} ||_2})^{\frac{2}{m-1}}] ^ {-1}
$$
The center, 𝝁^(𝑗𝑗) , of a cluster itself is calculated as the mean of all examples weighted by the degree to which each example belongs to that cluster (𝑤𝑤(𝑖𝑖,𝑗𝑗)𝑚𝑚 ):
$$
\mu ^ {(j)} = \frac{\sum_{i=1}^n w ^{(i, j)^m}x^{(i)}}{\sum_{i=1}^n w ^{(i, j)^m}}
$$

### 



### Кластеризация с помощью минимального остовного дерева

1. Строим взвешенный граф, где веса ребер - расстояния между объектами.
2. Строим минимальное отсовное дерево (алгоритм Крускала) для этого графа
3. Удаляем К-1 ребро с максимальным весом
4. Получаем К компонент связности, которые интерпретируем как кластеры





### Principal Components Analysis



1. Standardize the d-dimensional dataset. 

2. Construct the covariance matrix. For example, the covariance between two features, 𝑥𝑥𝑗𝑗 and 𝑥𝑥𝑘𝑘, on the population level can be calculated via the following equation:
   $$
   \sigma_{jk} = \frac{1}{n-1}\sum_{i=1}^{n}(x_j^{(i)}-\mu_j)(x_k^{(i)} - \mu_k)
   $$
    Note that the sample means are zero if we standardized the dataset. That's mean:

$$
   \sigma_{jk} = \frac{1}{n-1} \sum_{i=1}^n x_j^{(i)}x_k^{(i)}
$$


3. Decompose the covariance matrix into its eigenvectors and eigenvalues. 

4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors. 

5. Select k eigenvectors, which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace (𝑘𝑘 ≤ 𝑑𝑑). 

6. Construct a projection matrix, W, from the "top" k eigenvectors.

7. Transform the d-dimensional input dataset, X, using the projection matrix, W, to obtain the new k-dimensional feature subspace.

```python
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
cov_mat = np.cov(X_train.T)
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]
eigen_pairs.sort(key=lambda k: k[0], reverse=True)
w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))
X_train_pca = X_train.dot(w)
```

How PCA find the c1, c2 (principal component)? There is standard matrix factorization technique called *Singular Value Decomposition*, that decompose training set matrix X into the matrix multiplication of three matrices  U Σ V<sup>T</sup> where V contains all principal components that we are looking for.

```python
X_centered = X - X.mean(axis=0)
U, s, Vt = np.linalg.svd(X_centered)
c1 = Vt.T[:, 0]
c2 = Vt.T[:, 1]
```



Рассмотрим другой подход к прогнозированию предпочтений. В этом случае вы по-прежнему ищете U и V, но вам больше не нужно S, поэтому следует просто искать U и V, такие, что:
$$
X = UV
$$
Ваша проблема оптимизации заключается в том, что вы хотите свести к минимуму расхождение между фактическим X и приближением к X через оценку значений U и V с помощью квадратичных ошибок:
$$
argmin\sum_{i,j}(x_{i,j} - u_i v_j)^2
$$
Здесь через ui обозначена строка матрицы U, соответствующая i-му пользователю, и аналогично через vj обозначена строка матрицы V, соответствующая j-му элементу. Как обычно, элементы могут включать информацию о метаданных (поэтому вектором возраста всех пользователей будет строка в V). Тогда значение скалярного произведения ui · vj является предсказанным предпочтением i-го пользователя для j-го элемента, и вы хотите, чтобы оно было как можно ближе к действительному предпочтению xi,j

Пока ваша задача выпуклая, решение будет хорошо сходиться (то есть вы не обнаружите себя в локальном, но не глобальном максимуме) и вы можете заставить вашу задачу быть таковой, используя регуляризацию.

Ниже приведен алгоритм. 

* Выберите случайную матрицу V.
* Оптимизируйте U при фиксированной V. 
* Оптимизируйте V при фиксированной U. 
* Продолжайте выполнять два предыдущих шага, пока все полностью не изменится. Точнее, вы выбираете значение ϵ, и если ваши коэффициенты изменяются меньше чем на ϵ, то объявляете свой алгоритм конвергентным.



```python
import math,numpy

pu = [[(0,0,1),(0,1,22),(0,2,1),(0,3,1),(0,5,0)],
      [(1,0,1),(1,1,32),(1,2,0),(1,3,0),(1,4,1),(1,5,0)],
      [(2,0,0),(2,1,18),(2,2,1),(2,3,1),(2,4,0),(2,5,1)],
      [(3,0,1),(3,1,40),(3,2,1),(3,4,0),(3,5,1)],
      [(4,0,0),(4,1,40),(4,2,0),(4,4,1)],
      [(5,0,0),(5,1,25),(5,2,1),(5,3,1),(5,4,1)]]

pv = [[(0,0,1),(0,1,1),(0,2,0),(0,3,1),(0,4,0),(0,5,0)],
      [(1,0,22),(1,1,32),(1,2,18),(1,3,40),(1,4,40),(1,5,25)],
      [(2,0,1),(2,1,0),(2,2,1),(2,3,1),(2,4,0),(2,5,1)],
      [(3,0,1),(3,2,1),(3,3,0),(3,5,1)],
      [(4,1,1),(4,2,0),(4,3,0),(4,5,1)],
      [(5,0,0),(5,1,0),(5,2,1),(5,3,1),(5,4,0)]]

V = numpy.mat([[ 0.15968384, 0.9441198 , 0.83651085],
               [ 0.73573009, 0.24906915, 0.85338239],
               [ 0.25605814, 0.6990532 , 0.50900407],
               [ 0.2405843 , 0.31848888, 0.60233653],
               [ 0.24237479, 0.15293281, 0.22240255],
               [ 0.03943766, 0.19287528, 0.95094265]])

U = numpy.mat(numpy.zeros([6,3]))
L = 0.03

for iter in xrange(5):
	urs = []
	for uset in pu:
		vo = []
		pvo = []
		for i,j,p in uset:
			vor = []
			for k in range(3):
				vor.append(V[j,k])
                
			vo.append(vor)
			pvo.append(p) 
			vo = numpy.mat(vo)
			ur = numpy.linalg.inv(vo.T*vo + L*numpy.mat(numpy.eye(3))) * vo.T * numpy.mat(pvo).T
            
			urs.append(ur.T)
	U = numpy.vstack(urs)

	vrs = []
	for vset in pv:
		uo = []
		puo = []
		for j,i,p in vset:
			uor = []
			for k in xrange(3):
				uor.append(U[i,k])
			uo.append(uor)
			puo.append(p)
		uo = numpy.mat(uo)
		vr = numpy.linalg.inv(uo.T*uo + L*numpy.mat(num py.eye(3))) * uo.T * numpy.mat(puo).T
		vrs.append(vr.T)
	V = numpy.vstack(vrs)

	err = 0.
	n = 0.
	for uset in pu:
		for i,j,p in uset:
			err += (p — (U[i]*V[j].T)[0,0])**2
			n += 1

```



### 





### Linear Discriminant Analysis

It is a method that tries to maximize the distance of points belonging to different classes while minimizing the distance of points of the same class.

The general concept behind LDA is very similar to PCA, but whereas PCA attempts to find the orthogonal component axes of maximum variance in a dataset, the goal in LDA is to find the feature subspace that optimizes class separability.

One assumption in LDA is that the data is normally distributed. Also, we assume that the classes have identical covariance matrices and that the training examples are statistically independent of each other. However, even if one, or more, of those assumptions is (slightly) violated, LDA for dimensionality reduction can still work reasonably well.

Before we dive into the code implementation, let's briefly summarize the main steps that are required to perform LDA: 

1. Standardize the d-dimensional dataset (d is the number of features). 
2. For each class, compute the d-dimensional mean vector. 
3. Construct the between-class scatter matrix, 𝑺𝐵, and the within-class scatter matrix, 𝑺𝑤.
4. Compute the eigenvectors and corresponding eigenvalues of the matrix, 𝑺𝑊^(-1) * 𝑺𝐵. 
5. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors. 
6. Choose the k eigenvectors that correspond to the k largest eigenvalues to construct a 𝑑 × 𝑘-dimensional transformation matrix, W; the eigenvectors are the columns of this matrix. 
7. Project the examples onto the new feature subspace using the transformation matrix, W

 Each mean vector, 𝒎𝑖 , stores the mean feature value, 𝜇𝜇𝑚𝑚, with respect to the examples of class i.

Using the mean vectors, we can now compute the within-class scatter matrix, 𝑺𝑊:
$$
S_W = \sum_{i=1}^c S_i
\\
\\
S_i = \sum_{x э D_i} (x - m_i)(x - m_i)^T
$$
The assumption that we are making when we are computing the scatter matrices is that the class labels in the training dataset are uniformly distributed.  When we divide the scatter matrices by the number of class examples, 𝑛𝑖, we can see that computing the scatter matrix is in fact the same as computing the covariance matrix, Σ𝑖𝑖 —the covariance matrix is a normalized version of the scatter matrix.

After we compute the scaled within-class scatter matrix (or covariance matrix), we can move on to the next step and compute the between-class scatter matrix 𝑺𝑩:
$$
S_B = \sum_{i=1}^c n_i (m_i - m) (m_i - m)^T
$$
Here, m is the overall mean that is computed, including examples from all c classes.

The remaining steps of the LDA are similar to the steps of the PCA. However, instead of performing the eigendecomposition on the covariance matrix, we solve the generalized eigenvalue problem of the matrix, 𝑺𝑊^(-1) * 𝑺𝐵. 

```python
d = 13 # number of features
S_W = np.zeros((d, d))
for label, mv in zip(range(3), means):
    class_scatter = np.zeros((d, d))
    cnt = 0
    for row in features[target == label]:
        row, mv = row.reshape(d, 1), mv.reshape(d, 1)
        class_scatter += (row - mv).dot((row - mv).T)
        cnt += 1
    
    S_W += class_scatter / cnt

m = features.mean(axis=0)
S_B = np.zeros((d, d))
for i, mean in enumerate(means):
    n = features[target == i].shape[0]
    mean = mean.reshape(d, 1)
    m = m.reshape(d, 1)
    S_B += n*(mean - m).dot((mean-m).T)

matrix = np.linalg.inv(S_W).dot(S_B)
eigen_vals, eigen_vecs = np.linalg.eig(matrix)
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) 
               for i in range(len(eigen_vals))] 
eigen_pairs = sorted(eigen_pairs,key=lambda k: k[0], reverse=True)
w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real, eigen_pairs[1][1][:, np.newaxis].real))
X_train_lda = features.dot(w)
```

### Kernel PCA



If we are dealing with nonlinear problems, which we may encounter rather frequently in real-world applications, linear transformation techniques for dimensionality reduction, such as PCA and LDA, may not be the best choice.

We perform a nonlinear mapping via KPCA that transforms the data onto a higher-dimensional space. We then use standard PCA in this higher dimensional space to project the data back onto a lower-dimensional space where the examples can be separated by a linear classifier (under the condition that the examples can be separated by density in the input space). However, one downside of this approach is that it is computationally very expensive, and this is where we use the kernel trick. Using the kernel trick, we can compute the similarity between two high-dimension feature vectors in the original feature space.

Covariance matrix:
$$
\sum = \frac{1}{n} \sum_{i=1}^n \bold x^{(i)} \bold x^{(i)^T}
$$
Bernhard Scholkopf generalized this approach (Kernel principal component analysis, B. Scholkopf, A. Smola, and K.R. Muller, pages 583-588, 1997) so that we can replace the dot products between examples in the original feature space with the nonlinear feature combinations via 𝜙:
$$
\sum = \frac{1}{n}\sum_{i=1}^{n} \phi(\bold x^{(i)}) \phi(\bold x^{(i)^T})
$$
To obtain the eigenvectors—the principal components—from this covariance matrix, we have to solve the following equation:
$$
\sum \bold v = \lambda \bold v
\\
=> \frac{1}{n}\sum_{i=1}^n \phi(\bold x^{(i)}) \phi(\bold x^{(i)^T})\bold v = \lambda \bold v
\\
\bold v = \frac{1}{n\lambda}\sum_{i=1}^n \phi(\bold x^{(i)}) \phi(\bold x^{(i)^T}) \bold v = \frac{1}{n}\sum_{i=1}^{n} \bold a^{(i)} \phi(\bold x ^ {(i)})
\\
\bold K = \phi(\bold X) \phi (\bold X)^ T
$$
Here, 𝜆𝜆 and v are the eigenvalues and eigenvectors of the covariance matrix, Σ, and **a** can be obtained by extracting the eigenvectors of the kernel (similarity) matrix, K, as you will see in the following paragraphs.





To summarize what we have learned so far, we can define the following three steps to implement an RBF  (function of similarity) KPCA: 

 1. We compute the kernel (similarity) matrix, K, where we need to calculate the following:
    $$
    k(x^{(i)}, x^{(j)}) = exp(-\gamma(||x^{(i)} - x^{(j)}||^2))
    $$
    For example, if our dataset contains 100 training examples, the symmetric kernel matrix of the pairwise similarities would be 100 × 100-dimensional.

2. We center the kernel matrix, K, using the following equation:
   $$
   \bold K' = \bold K - 1_n\bold K - \bold K1_n +1_n\bold K1_n
   $$
   Here, 𝟏𝒏 is an 𝑛𝑛 × 𝑛𝑛-dimensional matrix (the same dimensions as the kernel matrix) where all values are equal to 1/𝑛 .

3. We collect the top k eigenvectors of the centered kernel matrix based on their corresponding eigenvalues, which are ranked by decreasing magnitude. In contrast to standard PCA, the eigenvectors are not the principal component axes, but the examples already projected onto these axes.

```python
gamma = 15
n_components = 2
# Calculate pairwise squared Euclidean distances
# in the MxN dimensional dataset
sqdists = pdist(x, 'sqeuclidean')
# Convert pairwise distances into a square matrix
mat_sq_dist = squareform(sqdists)
# Compute the symmetric kernel matrix.
K = exp(-gamma * mat_sq_dist)
# Center the kernel matrix.
N = K.shape[0]
one_n = np.ones((N,N)) / N
K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)
# Obtaining eigenpairs from the centered kernel matrix
# scipy.linalg.eigh returns them in ascending order
eigvals, eigvecs = eigh(K)
eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]
# Collect the top k eigenvectors (projected examples)
alphas = np.column_stack([eigvecs[:, i] for i in range(n_components)])
# Collect the corresponding eigenvalues
lambdas = [eigvals[i] for i in range(n_components)]

```

To project new data we have to calculate the pairwise RBF kernel (similarity) between each ith example in the training dataset and the new example, 𝒙𝒙′:
$$
\phi (x')^T v = \sum_i a^{(i)}\phi(x')^T \phi(x^{(i)}) = \sum_i a^{(i)} k(x', x^{(i)})
$$
After calculating the similarity between the new examples and the examples in the training dataset, we have to normalize the eigenvector, a, by its eigenvalue. 

```python
def project_x(x_new, X, gamma, alphas, lambdas):
	pair_dist = np.array([np.sum((x_new-row)**2) for row in X])
	k = np.exp(-gamma * pair_dist)
	return k.dot(alphas / lambdas)
```

### UMAP

The idea behind many of the modern dimensionality reduction algorithms, especially those designed specifically for visualization purposes, such as t-SNE and UMAP, is basically the same. We first design a similarity metric for two examples. For visualization purposes, besides the Euclidean distance between the two examples, this similarity metric often reflects some local properties of the two examples, such as the density of other examples around them.

In UMAP, this similarity metric w is defined as follows,
$$
w(x_i, x_j) = w_i(x_i, x_j) + w_j(x_j, x_i) - w_i(x_i, x_j)w_j(x_j, x_i)
$$
The function wi(xi, xj ) is defined as,
$$
w_i(x_i, x_j) = exp(-\frac{d(x_i, x_j) - p_i}{\sigma_i})
$$
Let w denote the similarity of two examples in the original high-dimensional space and let w' be the similarity given by the same eq. 5 in the new low-dimensional space. Because the values of w and w' lie in the range between 0 and 1, we can see them as probabilities of two binary random variables. A widely used metric of similarity between two probability distributions is cross-entropy:
$$
C_{w, w'} = \sum_{i=1}^N \sum_{j=1}^N [w(x_i, x_j) ln(\frac{w(x_i,x_j)}{w'(x'_i, x'_j)}) + (1 - w(x_i, x_j))ln(\frac{1 - w(x_i,x_j)}{1 - w'(x'_i, x'_j)}))]
$$





# Models fitting and estimating



### Train/validation split



**Cross-validation (кросс проверка)**

Cross-validation works like follows. First, you fix the values of the hyperparameters you want to evaluate. Then you split your training set into several subsets of the same size. Each subset is called a fold. Typically, five-fold cross-validation is used in practice. With five-fold cross-validation, you randomly split your training data into five folds: {F1, F2,...,F5}. Each Fk, k = 1,..., 5 contains 20% of your training data. Then you train five models as follows. To train the first model, f1, you use all examples from folds F2, F3, F4, and F5 as the training set and the examples from F1 as the validation set. To train the second model, f2, you use the examples from folds F1, F3, F4, and F5 to train and the examples from F2 as the validation set. You continue building models iteratively like this and compute the value of the metric of interest on each validation set, from F1 to F5. Then you average the five values of the metric to get the final value

**Hold-out validation**

**Leave-one-out**

**q-fold Cross-validation (Поблочная кросс-проверка)**

**t * q-fold Cross-validation (Многократная поблочная кросс-проверка)**





### Maximum likelihood

Consider a set of *m* examples X drawn independently from the true but unknow data generating distribution p_{data}(x). The maximum likelihood estimator for θ is then given as:
$$
\theta_{ML} = argmax_{\theta} p_{model}(X; \theta) = argmax_{\theta} \prod_{i=1}^m p_{model} (x^{(i)};\theta)
$$
Maximizing likelihood corresponds exactly to minimizing the cross-entropy between distributions (for example MSE is the cross-entropy between empirical and Gaussian distributions). We can thus see maximum likelihood as an attempt to make the model distribution match the empirical distribution.

The likelihood function indicates how likely the observed sample is as a function of possible parameter values. Therefore, maximizing the likelihood function determines the parameters that are most likely to produce the observed data. From a statistical point of view, MLE is usually recommended for large samples because it is versatile, applicable to most models and different types of data, and produces the most precise estimates.

Least squares estimates are calculated by fitting a regression line to the points from a data set that has the minimal sum of the deviations squared (least square error). In reliability analysis, the line and the data are plotted on a probability plot.

**Maximum likelihood properties**

1. If T is the maximum likelihood estimator of parameter \theta and g(\theta) is an invertible function of \theta then g(T) is the maximum likelihood estimator of g(\theta).
2. The maximum likelihood estimator T may be biased. But asymptotically (as the size n of the dataset goes to infinity) maximum likelihood estimators are unbiased.
3. Maximum likelihood estimators have asymptotically the smallest variance among unbiased estimators.



**Для регрессии** 

Модель данных с некоррелированным гауссовким шумом:
$$
y(x_i) = f(x_i, \alpha) + \epsilon, \epsilon = N(0, \sigma_i^2)
$$
Метод максимума правдоподобия и метод наименьших квадратов:
$$
L(\epsilon_1, ... , \epsilon_n|\alpha) = \prod_{i=1}^l \frac{1}{\sigma_i \sqrt{2\pi}}exp(-\frac{1}{2\sigma^2_i}\epsilon_i^2) \rightarrow max_{\alpha};
\\
-lnL(\epsilon_1, ... , \epsilon_n|\alpha) = const(\alpha) + \frac{1}{2}\sum_{i=1}^l \frac{1}{\sigma^2_i}(f(x_i, \alpha) - y_i)^2 \rightarrow min_{\alpha}
$$
Постановки ММП и МНК совпадают, причем веса объектов обратно пропорциональны дисперсии шума.

**Для классификации**

Можем линейную модель с заданной функцией потерь переинтерпретировать как параметрическую модель вероятности класса у.

Максимизация правдоподобия:
$$
L(w) = \sum_{i=1}^l logP(y_i|x_i, w) \rightarrow max_w
$$
Минимизация имперического риска:
$$
Q(w) = \sum_{i=1}^l L(y_ig(x_i, w)) \rightarrow min_w
$$
Эти два принципа эквивалентны, если положить
$$
-logP(y_i|x_i, w) = L(y_ig(x_i, w))
$$



###  Maximum a Posteriori Estimation



While the most principled approach is to make predictions using the full Bayesian posterior distribution over the parameter θ, it is still often desirable to have a single point estimate. One common reason for desiring a point estimate is that most operations involving the Bayesian posterior for most interesting models are intractable, and a point estimate offers a tractable approximation. Rather than simply returning to the maximum likelihood estimate, we can still gain some of the benefit of the Bayesian approach by allowing the prior to influence the choice of the point estimate.

The MAP estimate chooses the point of maximal posterior probability (or maximal probability density in the more common case of continuous θ):
$$
\theta_{MAP} = argmax_{\theta} p(\theta | x) = argmax_{\theta}p(x|\theta) + log(p(\theta))
$$
As with full Bayesian inference, MAP Bayesian inference has the advantage of leveraging information that is brought by the prior and cannot be found in the training data. This additional information helps to reduce the variance in the MAP point estimate (in comparison to the ML estimate). However, it does so at the price of increased bias. Many regularized estimation strategies, such as maximum likelihood learning regularized with weight decay, can be interpreted as making the MAP approximation to Bayesian inference.













### Gradient Descent





### Newton's Method



```python
from autograd import grad
from autograd import hessian

# import NumPy library
import numpy as np
# Newton ’s method
def newtons_method (g, max_its, w):
    # compute gradient/ Hessian using autograd
    gradient = grad(g)
    hess = hessian (g)
    
    # set numerical stability parameter
    epsilon = 10 **(-7)
    if ’epsilon ’ in kwargs:
        epsilon = kwargs[’epsilon ’]

    # run the Newton ’s method loop
    weight_history = [w] # container for weight history
    cost_history = [g(w)] # container for cost function history

    for k in range( max_its ):
        # evaluate the gradient and hessian
        grad_eval = gradient (w)
        hess_eval = hess(w)

        # reshape hessian to square matrix
        hess_eval .shape = (int((np. size( hess_eval ))**(0 .5)),int((np.size( hess_eval ))**(0 .5)))

        # solve second -order system for weight update
        A = hess_eval + epsilon* np. eye(w. size)
        b = grad_eval
        w = np. linalg. solve(A, np .dot(A ,w)-b)

        # record weight and cost
        weight_history. append(w)
        cost_history.append(g(w))
        
    return weight_history, cost_history
```



Newton’s method is a powerful algorithm that makes enormous progress towards finding a function’s minimum at each step, compared to zero- and first order methods that can require a large number of steps to make equivalent progress. However, Newton’s method suffers from its own unique weaknesses – primarily in dealing with nonconvexity, as well as scaling with input dimension.





### Iteratively Reweighted Least Squares

(МНК с итерационным перевзвешиванием объектов)

Что делать, если модель регрессии не линейная или функция потерь не квадратичная? Общий рецепт такой: применение метода Ньютона-Рафсона приводит к итерационному процессу, на каждом шаге которого решается задача линейной регрессии. Смысл её сводится к тому, чтобы поточнее настроиться на тех объектах, на которых модель в текущем её приближении работает недостаточно хорошо. В этот общий сценарий неплохо вписывается серия важных частных случаев. Нелинейная регрессия с квадратичной функцией потерь. Логистическая регрессия. Обобщённая линейная модель (GLM), в которой прогнозируемая величина описывается экспоненциальным семейством распределений. Логистическая регрессия является частным случаем GLM, и, благодаря этому факту, мы теперь понимаем, почему вероятность классов выражается через сигмоиду от дискриминантной функции

**Вход**: F, y - матрица объекты признаки и вектор ответов

**Выход**: w - вектор коэффициентов линейной коомбинации.

1. w = (F^T F) ^ {-1} F^T y - начальное приближение, обычный МНК
2. для t = 1, 2, 3...
   * \sigma_i = \sigma(y_i w^T x)  для всех i
   * \gamma_i = \sqrt{(1 - \sigma_i) \sigma_i} для всех i
   * ~F = diag(\gamma_1, ... \gamma_n)F
   * ~y_i = y_i \sqrt{(1 - \sigma_i) \sigma_i} для всех i
   * выбрать градиентный шаг h_t
   * w = w + h_t (~F^T ~F)^{-1} ~F^T ~y
   * если \sigma_i мало изменилось, выйти из цикла







### Regression metrics

* Residual standard error - The same as the RMSE, but adjusted for degrees of freedom.

* R-squared - The proportion of variance explained by the model, from 0 to 1. It is useful mainly in explanatory uses of regression where you want to assess how well the model fits the data.

* t-statistic - The coefficient for a predictor, divided by the standard error of the coefficient, giving a metric to compare the importance of variables in the model. The t-statistic—and its mirror image, the p-value—measures the extent to which a coefficient is “statistically significant”—that is, outside the range of what a random chance arrangement of predictor and target variable might produce. The higher the t-statistic (and the lower the p-value), the more significant the predictor.

* Weighted regression - Regression with the records having different weights.

  

$$
MSE = \frac{\sum_{i=1}^{n}(y_i - \hat y_i)^2}{n}
\\
RMSE = \sqrt{\frac{\sum_{i=1}^{n}(y_i - \hat y_i)^2}{n}}
\\
R^2 = 1 - \frac{\sum_{i = 1}^{n}(y_i - \hat y_i)^2}{\sum_{i = 1}^{n}(y_i - \bar y_i)^2}
\\
t_b = \frac{\hat b}{RMSE(\hat b)} 
\\
AIC = 2P + nlog(RSS/n)
$$

AIC - Akaike's information criteria, penalizes adding terms to a model. P - is the number of variables and n is the number of records.

How do we find the model that minimizes AIC? One approach is to search through all possible models, called all subset regression. This is computationally expensive and is not feasible for problems with large data and many variables. An attractive alternative is to use stepwise regression, which successively adds and drops predictors to find a model that lowers AIC.

Simpler yet are forward selection and backward selection. In forward selection, you start with no predictors and add them one-by-one, at each step adding the predictor that has the largest contribution to R^2, stopping when the contribution is no longer statistically significant. In backward selection, or backward elimination, you start with the full model and take away predictors that are not statistically significant until you are left with a model in which all predictors are statistically significant.

Penalized regression is similar in spirit to AIC. Instead of explicitly searching through a discrete set of models, the model-fitting equation incorporates a constraint that penalizes the model for too many variables (parameters). Rather than eliminating predictor variables entirely—as with stepwise, forward, and backward selection—penalized regression applies the penalty by reducing coefficients, in some cases to near zero. Common penalized regression methods are ridge regression and lasso regression.



### Regression residuals plot

Since our model uses multiple explanatory variables, we can't visualize the linear regression line (or hyperplane, to be precise) in a two-dimensional plot, but we can plot the residuals (the differences or vertical distances between the actual and predicted values) versus the predicted values to diagnose our regression model. Residual plots are a commonly used graphical tool for diagnosing regression models. They can help to detect nonlinearity and outliers, and check whether the errors are randomly distributed.

```python
plt.scatter(y_train_pred, y_train_pred - y_train,
			c='steelblue', marker='o', edgecolor='white',
			label='Training data')
plt.scatter(y_test_pred, y_test_pred - y_test,
			c='limegreen', marker='s', edgecolor='white',
			label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()
```

![](img/residual plot.png)

In the case of a perfect prediction, the residuals would be exactly zero, which we will probably never encounter in realistic and practical applications. However, for a good regression model, we would expect the errors to be randomly distributed and the residuals to be randomly scattered around the centerline. If we see patterns in a residual plot, it means that our model is unable to capture some explanatory information, which has leaked into the residuals, as you can slightly see in our previous residual plot. Furthermore, we can also use residual plots to detect outliers, which are represented by the points with a large deviation from the centerline.







## 





### Classification metrics

For classification, things are a little bit more complicated. The most widely used metrics and tools to assess the classification model are: 

* confusion matrix
* accuracy - не учитывается ни дисбаланс классов, ни цена ошибки на объектах разных классов.
* cost-sensitive accuracy
* precision/recall
* area under the ROC curve.



**Precision, Recall, Sensitivity, Specificity (Точность, Полнота, Чувствительность, Специфичность)**
$$
accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\\
precision = \frac{TP}{TP + FP}
\\
recall = \frac{TP}{TP + FN}
\\
sensitivity = \frac{TP}{TP+FN}
\\
specificity = \frac{TN}{TN + FP}
\\
F1 = 2 \frac{precision*recall}{precision+recall}
$$

Precision - количество сбитых самолетов / общее количество выстрелов

Recall - количество сбитых самолетов / общее количество самолетов

The precision is the proportion of relevant documents in the list of all returned documents. The recall is the ratio of the relevant documents returned by the search engine to the total number of the relevant documents that could have been returned.

However, scikit-learn also implements macro and micro averaging methods to extend those scoring metrics to multiclass problems via one-vs.-all (OvA) classification. The micro-average is calculated from the individual TPs, TNs, FPs, and FNs of the system. For example, the micro-average of the precision score in a k-class system can be calculated as follows:
$$
precision_{micro} = \frac{TP_1 + ... + TP_k}{TP_1 + ... + TP_k + FP_1 + ... FP_k}
\\
precision_{macro} = \frac{precision_1 + ... precision_k}{k}
$$



**Cost-sensitive accuracy**

For dealing with the situation in which different classes have different importance, a useful metric is cost-sensitive accuracy. To compute a cost-sensitive accuracy, you first assign a cost (a positive number) to both types of mistakes: FP and FN. You then compute the counts TP, TN, FP, FN as usual and multiply the counts for FP and FN by the corresponding cost before calculating the accuracy.

**Area under the ROC Curve (AUC)**

The ROC curve (stands for “receiver operating characteristic,” the term comes from radar engineering) is a commonly used method to assess the performance of classification models. ROC curves use a combination of the true positive rate (defined exactly as recall) and false positive rate (the proportion of negative examples predicted incorrectly) to build up a summary picture of the classification performance. 
$$
TPR = \frac{TP}{(TP+FN)}
\\
FPR = \frac{FP}{(FP+TN)}
$$
TPR - чувствительность, FPR - специфичность.

ROC curves can only be used to assess classifiers that return some confidence score (or a probability) of prediction. For example, logistic regression, neural networks, and decision trees (and ensemble models based on decision trees) can be assessed using ROC curves.

To draw a ROC curve, you first discretize the range of the confidence score. If this range for a model is [0, 1], then you can discretize it like this: [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]. Then, you use each discrete value as the prediction threshold and predict the labels of examples in your dataset using the model and this threshold. For example, if you want to compute TPR and FPR for the threshold equal to 0.7, you apply the model to each example, get the score, and, if the score if higher than or equal to 0.7, you predict the positive class; otherwise, you predict the negative class.





### Quantifying the quality of clustering via silhouette plots

Another intrinsic metric to evaluate the quality of a clustering is silhouette analysis, which can also be applied to clustering algorithms other than k-means, which we will discuss later in this chapter. Silhouette analysis can be used as a graphical tool to plot a measure of how tightly grouped the examples in the clusters are. To calculate the silhouette coefficient of a single example in our dataset, we can apply the following three steps:

1. Calculate the cluster cohesion, 𝑎^(𝑖) , as the average distance between an example, 𝒙^(𝑖) , and all other points in the same cluster. 

2. Calculate the cluster separation, 𝑏^(𝑖) , from the next closest cluster as the average distance between the example, 𝒙^(i) , and all examples in the nearest cluster. 

3. Calculate the silhouette, 𝑠^(𝑖) , as the difference between cluster cohesion and separation divided by the greater of the two, as shown here:
   $$
   s^{(i)} = \frac{b^{(i)} - a^{(i)}}{max(b^{(i)}, a^{(i)})}
   $$

The silhouette coefficient is bounded in the range –1 to 1. Based on the preceding equation, we can see that the silhouette coefficient is 0 if the cluster separation and cohesion are equal (𝑏𝑏(𝑖𝑖) = 𝑎𝑎(𝑖𝑖) ). Furthermore, we get close to an ideal silhouette coefficient of 1 if 𝑏𝑏(𝑖𝑖) ≫ 𝑎𝑎(𝑖𝑖) , since 𝑏𝑏(𝑖𝑖) quantifies how dissimilar an example is from other clusters, and 𝑎𝑎(𝑖𝑖) tells us how similar it is to the other examples in its own cluster.





# Models improvement





### Regularization

Regularization, significantly reduces the variance of the model, without substantial increase in its bias (this means we avoid overfitting). 

**L1, L2 regularization:**


$$
L1 = ||w|| = \sum_{j=1}^m |w_j|
\\
L2 = \frac{\lambda}{2}||\bold w||^2 = \frac{\lambda}{2}\sum_{j=1}^{m}w_j^2
$$
So we add regularization term to loss function with some parameter lambda. This means we penalized model for large values of weights. Lambda is the tuning parameter that decides how much we want to penalize the flexibility of our model.  Regression example:
$$
J(\bold w)_{Ridge} = \sum_{i=1}^n (y^{(i)} - \hat y^{(i)})^2 +\lambda ||\bold w||_2^2
\\
J(\bold w)_{Lasso} = \sum_{i=1}^n (y^{(i)} - \hat y^{(i)})^2 +\lambda ||\bold w||_1
\\
J(\bold w)_{ElasticNet} = \sum_{i=1}^n (y^{(i)} - \hat y^{(i)})^2 + \lambda_1\sum_{j=1}^{m} w_j^2 + \lambda_2 \sum_{j=1}^{m} |w_j|
$$
In contrast to L2 regularization, L1 regularization usually yields sparse feature vectors and most feature weights will be zero.

С вероятностной точки зрения L2 регуляризация это предположение о том, что вектор параметров w имеет гауссовское распределение с центром в нуле и по всем координатам одинаковые дисперсии:
$$
p(w;C) = \frac{1}{(2\pi C)^{n/2}}exp(-\frac{||w||^2}{2C})
\\
-ln(p(w; C)) = \frac{1}{2C}||w||^2
$$
С вероятностной точки зрения L1 регуляризация это предположение о том, что вектор параметров w имеет распределение Лапласа с центром в нуле и по всем координатам одинаковые дисперсии:
$$
p(w;C) = \frac{1}{(2C)^{n}}exp(-\frac{||w||}{C})
\\
-ln(p(w; C)) = \frac{1}{C}||w||
$$
Где С - гиперпараметр, 1/С - коэффициент регуляризации.

**Another regularization techniques: ** 

1. Injecting noise at the input

2. Injecting noise at the output. For example, label smoothing regularizes a model based on a softmax with k output values by replacing the hard 0 and 1 classification targets with targets of:
   $$
   \frac{\epsilon}{k}
   \\
   1 - \frac{k - 1}k \epsilon
   $$
   respectively.

3. Semi-supervised learning. In the paradigm of semi-supervised learning, both unlabeled examples from P(x) and labeled examples from P(x, y) are used to estimate P(y | x) or predict y from x.  In the context of deep learning, semi-supervised learning usually refers to learning a representation h = f(x). The goal is to learn a representation so that examples from the same class have similar representations.

**For neural network:**



The concept of **dropout** is very simple. Each time you run a training example through the network, you temporarily exclude at random some units from the computation. The higher the percentage of units excluded the higher the regularization effect. Neural network libraries allow you to add a dropout layer between two successive layers, or you can specify the dropout parameter for the layer. The dropout parameter is in the range [0, 1] and it has to be found experimentally by tuning it on the validation data. 

**Early stopping** is the way to train a neural network by saving the preliminary model after every epoch and assessing the performance of the preliminary model on the validation set. As the number of epochs increases, the cost decreases. The decreased cost means that the model fits the training data well. However, at some point, after some epoch e, the model can start overfitting: the cost keeps decreasing, but the performance of the model on the validation data deteriorates. If you keep, in a file, the version of the model after each epoch, you can stop the training once you start observing a decreased performance on the validation set. Alternatively, you can keep running the training process for a fixed number of epochs and then, in the end, you pick the best model. Models saved after each epoch are called checkpoints. Some machine learning practitioners rely on this technique very often; others try to properly regularize the model to avoid such undesirable behavior.

**Batch normalization** (which rather has to be called batch standardization) is a technique that consists of standardizing the outputs of each layer before the units of the subsequent layer receive them as input. In practice, batch normalization results in a faster and more stable training, as well as in some regularization effect. So it’s always a good idea to try to use batch normalization. In neural network libraries, you can often insert a batch normalization layer between two layers. 

Another regularization technique that can be applied not just to neural networks, but to virtually any learning algorithm, is called **data augmentation**. This technique is often used to regularize models that work with images. Once you have your original labeled training set, you can create a synthetic example from an original example by applying various transformations to the original image: zooming it slightly, rotating, flipping, darkening, and so on. You keep the original label in these synthetic examples. In practice, this often results in increased performance of the model.





### Hyperparameter Tuning



Select parameter with validation curve:

```python
from sklearn.model_selection import validation_curve
param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
train_scores, test_scores = validation_curve(
												estimator=pipe_lr,
												X=X_train,
												y=y_train,
												param_name='logisticregression__C',
												param_range=param_range,
												cv=10)
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
plt.plot(param_range, train_mean,
		color='blue', marker='o',
		markersize=5, label='Training accuracy')
plt.fill_between(param_range, train_mean + train_std,
					train_mean - train_std, alpha=0.15,
					color='blue')
plt.plot(param_range, test_mean,
			color='green', linestyle='--',
			marker='s', markersize=5,
			label='Validation accuracy')
plt.fill_between(param_range,
					test_mean + test_std,
					test_mean - test_std,
					alpha=0.15, color='green')
plt.grid()
plt.xscale('log')
plt.legend(loc='lower right')
plt.xlabel('Parameter C')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1.0])
plt.show()
```

Similar to the learning_curve function, the validation_curve function uses stratified k-fold cross-validation by default to estimate the performance of the classifier. Inside the validation_curve function, we specified the parameter that we wanted to evaluate. In this case, it is C, the inverse regularization parameter of the LogisticRegression classifier, which we wrote as 'logisticregression__C' to access the LogisticRegression object inside the scikit-learn pipeline for a specified value range that we set via the param_range parameter.

**Tuning hyperparameters via grid search**

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
pipe_svc = make_pipeline(StandardScaler(),
						SVC(random_state=1))
						
param_range = [0.0001, 0.001, 0.01, 0.1,
						1.0, 10.0, 100.0, 1000.0]
						
param_grid = [{'svc__C': param_range,
						'svc__kernel': ['linear']},
						{'svc__C': param_range,
						'svc__gamma': param_range,
						'svc__kernel': ['rbf']}]
						
gs = GridSearchCV(estimator=pipe_svc,
						param_grid=param_grid,
						scoring='accuracy',
						cv=10,
						refit=True,
						n_jobs=-1)
```



### Bagging

The simple version of ensembles is as follows: 

1. Develop a predictive model and record the predictions for a given data set. 
2. Repeat for multiple models, on the same data. 
3. For each record to be predicted, take an average (or a weighted average, or a majority vote) of the predictions.

Bagging, which stands for “bootstrap aggregating,” was introduced by Leo Breiman in 1994. Suppose we have a response Y and P predictor variables X = X1, X2, ..., Xp with n records. Bagging is like the basic algorithm for ensembles, except that, instead of fitting the various models to the same data, each new model is fit to a bootstrap resample. Here is the algorithm presented more formally: 

1. Initialize M, the number of models to be fit, and n, the number of records to choose (n < N). Set the iteration m = 1. 
2. Take a bootstrap resample (i.e., with replacement) of n records from the training data to form a subsample Ym and Xm (the bag). 
3. Train a model using Ym and Xm to create a set of decision rules f. 
4. Increment the model counter m = m + 1. If m <= M, go to step 1. 

In the case where f predicts the probability Y = 1, the bagged estimate is given by:
$$
\hat f = \frac{1}{M}(\hat f_1(X) + \hat f_2(X) + ... + \hat f_M(X))
$$

### Boosting

The basic idea behind the various boosting algorithms is essentially the same. The easiest to understand is Adaboost, which proceeds as follows: 

1. Initialize M, the maximum number of models to be fit, and set the iteration counter m = 1. Initialize the observation weights w_i = 1/N for i = 1, 2, 3 ... N. Initialize the ensemble model \hat F_0 = 0. 

2. Train a model f_m using the observation weights w1, w2, w3, ..., wn that minimizes the weighted error e defined by summing the weights for the misclassified observations. 

3. Add the model to the ensemble:
   $$
   \hat F_m = \hat F_{m-1} + \alpha_m f_m \\ where\\
   \alpha_m = \frac{log1-e_m}{e_m}
   $$

4. Update the weights w1, w2, ..., wn, so that the weights are increased for the observations that were misclassified. The size of the increase depends on alpha_m with larger values of alpha_m leading to bigger weights. 

5. Increment the model counter m = m + 1. If m <= M, go to step 1.

The boosted estimate is given by:
$$
\hat F =  \alpha_1 \hat f_1 + \alpha_2 \hat f_2 + ... + \alpha_M \hat f_M
$$



In contrast to bagging, the initial formulation of the boosting algorithm uses random subsets of training examples drawn from the training dataset without replacement; the original boosting procedure can be summarized in the following fou2r key steps: 

1. Draw a random subset (sample) of training examples, 𝑑𝑑1, without replacement from the training dataset, D, to train a weak learner, 𝐶𝐶1. 
2. Draw a second random training subset, 𝑑𝑑2, without replacement from the training dataset and add 50 percent of the examples that were previously misclassified to train a weak learner, 𝐶𝐶2. 
3. Find the training examples, 𝑑𝑑3, in the training dataset, D, which 𝐶𝐶1 and 𝐶𝐶2 disagree upon, to train a third weak learner, 𝐶𝐶3. 
4. Combine the weak learners 𝐶𝐶1, 𝐶𝐶2, and 𝐶𝐶3 via majority voting.

Now that we have a better understanding of the basic concept of AdaBoost, let's take a more detailed look at the algorithm using pseudo code. For clarity, we will denote element-wise multiplication by the cross symbol (×) and the dot-product between two vectors by a dot symbol (∙):

1. Set the weight vector, **w**, to uniform weights, where sum of weights = 1

2. For j in m boosting rounds do following:

   * Train a weighted weak learner:  C_j = train(X, y, w)

   * Predict class labels: y^hat = predict(C_j, X)

   * Compute weighted error rate: 
     $$
     \epsilon = w * (\hat y != y)
     $$

   * Compute coefficient:
     $$
     \alpha_j = 0.5 log(\frac{1 - \epsilon}{\epsilon})
     $$

   * Update weights:
     $$
     w := w×exp(-\alpha_j × \hat y × y)
     $$

   * Normalize weights to sum to 1:
     $$
     w := \frac{w}{\sum_i w_i}
     $$

3. Compute the final prediction:
   $$
   \hat y = (\sum_{j=1}^{m}(\alpha_j × predict(C_j, X)) > 0)
   $$

Note that the expression (𝒚̂ ≠ 𝒚) in step 2c refers to a binary vector consisting of 1s and 0s, where a 1 is assigned if the prediction is incorrect and 0 is assigned otherwise.

### Combining models

There are three typical ways to combine models:

1. **Averaging** works for regression as well as those classification models that return classification scores. You simply apply all your models, let’s call them base models, to the input x and then average the predictions. To see if the averaged model works better than each individual algorithm, you test it on the validation set using a metric of your choice. 

2. **Blending** - all models predictions have weights, then averaging predictions * weights

3. **Majority vote** works for classification models. You apply all your base models to the input x and then return the majority class among all predictions. In the case of a tie, you either randomly pick one of the classes, or, you return an error message (if the fact of misclassifying would incur a significant cost). 

4. **Stacking** consists of building a meta-model that takes the output of base models as input. Let’s say you want to combine classifiers f1 and f2, both predicting the same set of classes. To create a training example (xˆi, yˆi) for the stacked model, set xˆi = [f1(x), f2(x)] and yˆi = yi.








