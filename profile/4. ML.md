

[**Machine learning models**](#Machine-learning-models)



1. Regression models:

* [Linear Regression](#linear-regression)
* [Polynomial Regression](#Polynomial Regression)
* [RANSAC](#RANSAC)
* [SVM](#support-vector-machine)
* [Generalized Additive Models](#Generalized Additive Models)
* [K-Nearest Neighboors](#k-nearest-neighboors)
* [Decision Tree](#Decision-tree)
* [Random Forest](#Random-Forest)
* 



2. Classification models:

* [Logistic regression](#logistic-regression)
* [Generalized linear model](#Generalized linear model)
* [SVM](#support-vector-machine)
* [Naive Bayes](#naive-bayes)
* [Not so Naive Bayes](#Not so Naive Bayes)
* [K-Nearest Neighboors](#k-nearest-neighboors)
* [Decision Tree](#Decision-tree)
* [Random Forest](#Random-Forest)



3. Clustering

* [K-means](#k-means)
* [K-means++](#k-means++)
* [Hierarchical Clustering](#hierarchical-clustering)
* [Locating regions of high density via DBSCAN](#Locating regions of high density via DBSCAN)
* [Gaussian Mixture Models](#Gaussian-Mixture-Models)
* [Fuzzy K-means](#fuzzy-k-means)
* [Кластеризация с помощью минимального остовного дерева](#Кластеризация-с-помощью-минимального-остовного-дерева)



4. Dimensionality reduction

* [Principal Components Analysis](#principal-components-analysis)
* [Linear Discriminant Analysis](#Linear Discriminant Analysis)
* [Kernel PCA](#Kernel PCA)
* [UMAP](#UMAP)



5. NLP

   * [Latent Dirichlet Allocation](#Latent Dirichlet Allocation)
   * [VADER - A rule-based sentiment analyzer](#VADER - A rule-based sentiment analyzer)



6. Computer Vision
   * [Histogram of Oriented Gradients](#Histogram of Oriented Gradients)



[**Models fitting and estimating**](#Models-fitting-and-estimating)

* [Train/validation split](#Train/validation-split)
* [Maximum Likelihood](#Maximum Likelihood)
* [Maximum a Posteriori Estimation](#Maximum-a-Posteriori-Estimation)
* [Gradient Descent](#Gradient-Descent)
* [Newton's Method](#Newton's-Method)
* [Iteratively Reweighted Least Squares](#Iteratively-Reweighted-Least-Squares)
* [Regression metrics](#Regression metrics)
* [Regression residuals plot](#Regression-residuals-plot)
* [Classification metrics](#Classification metrics)
* [Quantifying the quality of clustering via silhouette plots](#Quantifying-the-quality-of-clustering-via-silhouette-plots)



[**Models improvement**](#Models-improvement)

* [Regularization](#regularization)
* [Hyperparameter Tuning](#Hyperparameter-Tuning)
* [Bagging](#bagging)
* [Boosting](#boosting)
* [Combining models](#Combining-models)











# Machine learning models



### Linear Regression

* Response - The variable we are trying to predict.
* Independent variable - The variable used to predict to response.
* Regression coefficient - The slope of the regression line.
* Fitted values - The estimates Yi obtained from the regression line.
* Residuals - The difference between the observed values and the fitted values.
* Least squares - The method of fitting a regression by minimizing the sum of squared residuals.

The regression equation models the relationship between a response variable Y and a predictor variable X as a line. A regression model yields fitted values and residuals—predictions of the response and the errors of the predictions. Regression models are typically fit by the method of least squares. Regression is used both for prediction and explanation.

A linear model makes predictions by computing a weighted sum of the input feature, plus constant called the *bias term* (or *intercept term*):

$$
y = w_0 + w_1x_1 + w_2x_2 + ... +w_nx_n + \epsilon
$$


where a - parameter vector (containing bias term), x - feature vector

So the task is to compute vector **a**, we can do this in different ways:

1. The normal equation. It is the *closet form solution* that gives result directly: 
   $$
   w = (x^Tx)^{-1}x^Ty
   $$

   The computational complexity of this approach is between O(n<sup>2.4</sup>) and O(n<sup>3</sup>).

2. Using optimization methods such as Gradient Descent, SGD, Mini-batch GD.

**Implementation of linear regression can be found in python-playground/mini_projects/ml_models_implementations/linear_models.py**



### Polynomial Regression

It is linear regression with powers of each features as a new feature. Polynomial Regression can fit non-linear relationship in data.

```python
from sklearn.preprocessing import PolynomialFeatures 
poly_features = PolynominalFeatures(degree = 2, include_bias = False) 
X_poly = poly_features.fit_transform(X) #add square of each feature

lin_reg = LinearRegression()
lin_reg.fit(X_poly)
```

Also PolynomialFeatures() is capable of finding relationships between features, because it adds all combination of features up to the given degree. PolynomialFeatures() transform array that containing **n** features to array containing **(n+degree)! / (d! n!)** features. 



### RANSAC

Linear regression models can be heavily impacted by the presence of outliers. In certain situations, a very small subset of our data can have a big effect on the estimated model coefficients. There are many statistical tests that can be used to detect outliers. However, removing outliers always requires our own judgment as data scientists as well as our domain knowledge. As an alternative to throwing out outliers, we will look at a robust method of regression using the RANdom SAmple Consensus (RANSAC) algorithm, which fits a regression model to a subset of the data, the so-called inliers. We can summarize the iterative RANSAC algorithm as follows:

1. Select a random number of examples to be inliers and fit the model. 
2. Test all other data points against the fitted model and add those points that fall within a user-given tolerance to the inliers. 
3. Refit the model using all inliers. 
4. Estimate the error of the fitted model versus the inliers. 
5. Terminate the algorithm if the performance meets a certain user-defined threshold or if a fixed number of iterations were reached; go back to step 1 otherwise.



### Generalized Additive Models

Suppose you suspect a nonlinear relationship between the response and a predictor variable, either by a priori knowledge or by examining the regression diagnostics. Polynomial terms may not flexible enough to capture the relationship, and spline terms require specifying the knots. Generalized additive models, or GAM, are a technique to automatically fit a spline regression.





### Logistic regression

**Logistic regression is a special instance of a generalized linear model** (GLM) developed to extend linear regression to other settings.

In linear regression, we minimized the empirical risk which was defined as the average squared error loss, also known as the mean squared error or MSE. 

In logistic regression, on the other hand, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function defines how likely the observation (an example) is according to our model. **Likelihood, L:**
$$
L(\bold w) = P(\bold y | \bold{x;w}) = \prod_{i=1}^{n} P(y^{(i)} | x^{(i)}; \bold w) = \prod_{i=1}^{n} (\phi(z^{i}))^{y^{(i)}} (1 - \phi (z^{(i)}))^{1-y^{(i)}}
$$
We used the product operator in the objective function instead of the sum operator which was used in linear regression. It’s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case).

In practice, it is easier to maximize the (natural) log of this equation, which is called the **log-likelihood function**:
$$
l(\bold w) = logL(\bold w) = \sum_{i=1}^{n} [y^{(i)}log(\phi(z^{(i)})) + (1-y^{(i)})log(1-\phi(z^{(i)}))] \\ 
\\
J(\bold w) = \sum_{i=1}^{n} [-y^{(i)}log(\phi(z^{(i)})) - (1-y^{(i)})log(1-\phi(z^{(i)}))]
$$
Since maximizing the log-likelihood is equal to minimizing the cost function, J, we can write the gradient descent update rule as follows:
$$
\bold w = \bold w + (-\eta \frac{dJ}{dw_j}) = \bold w + \eta \sum_{i=1}^{n} (y^{(i)} - \phi(z^{(i)}))x_j^{(i)}
$$

The logit function takes input values in the range 0 to 1 and transforms them to values over the entire real-number range, which we can use to express a linear relationship between feature values and the log-odds:
$$
logit(p(y=1|\bold x)) = \sum_{i=0}^{m}w_ix_i = \bold w^T \bold x 
$$
Now, we are actually interested in predicting the probability that a certain example belongs to a particular class, which is the inverse form of the logit function.  It is also called the logistic sigmoid function, which is sometimes simply abbreviated to sigmoid function due to its characteristic S-shape:
$$
\phi(z) = \frac{1}{1+e^{-z}}
$$
**Implementation of logistic regression can be found in python-playground/mini_projects/ml_models_implementations/linear_models.py**



### Generalized linear model

Generalized linear models (GLMs) are the second most important class of models besides regression. GLMs are characterized by two main components: 

* A probability distribution or family (binomial in the case of logistic regression) 
* A link function mapping the response to the predictors (logit in the case of logistic regression)

Logistic regression is by far the most common form of GLM. A data scientist will encounter other types of GLMs. Sometimes a log link function is used instead of the logit; in practice, use of a log link is unlikely to lead to very different results for most applications. The poisson distribution is commonly used to model count data (e.g., the number of times a user visits a web page in a certain amount of time). Other families include negative binomial and gamma, often used to model elapsed time (e.g., time to failure). In contrast to logistic regression, application of GLMs with these models is more nuanced and involves greater care. These are best avoided unless you are familiar with and understand the utility and pitfalls of these methods.



### Support Vector Machine

Another powerful and widely used learning algorithm is the support vector machine (SVM), which can be considered an extension of the perceptron. Using the perceptron algorithm, we minimized misclassification errors. However, in SVMs our optimization objective is to maximize the margin. The margin is defined as the distance between the separating hyperplane (decision boundary) and the training examples that are closest to this hyperplane, which are the so-called support vectors. 

The basic idea behind **kernel methods** to deal with such linearly inseparable data is to create nonlinear combinations of the original features to project them onto a higher-dimensional space via a mapping function, 𝜙, where the data becomes linearly separable. 

To solve a nonlinear problem using an SVM, we would transform the training data into a higher-dimensional feature space via a mapping function, 𝜙, and train a linear SVM model to classify the data in this new feature space. Then, we could use the same mapping function, 𝜙, to transform new, unseen data to classify it using the linear SVM model. 

However, one problem with this mapping approach is that the construction of the new features is computationally very expensive, especially if we are dealing with high-dimensional data. This is where the so-called kernel trick comes into play. 

In order to save the expensive step of calculating this dot product between two points explicitly, we define a so-called kernel function:
$$
k(\bold x^{(i)}, \bold x^{(j)}) = \phi(x^{(i)})^T\phi(x^{(j)})
$$
One of the most widely used kernels is the radial basis function (RBF) kernel, which can simply be called the Gaussian kernel:
$$
k(\bold x^{(i)}, \bold x^{(j)}) = exp(-\frac {||x^{(i)}- x^{(j)}||^2}{2\sigma^2} )
\\
or
\\
k(\bold x^{(i)}, \bold x^{(j)}) = exp(-\gamma||x^{(i)}- x^{(j)}||^2)
$$

For example, it can be shown that linear function used by the SVM can be re-written as:
$$
\bold w^T \bold x + b = b + \sum_{i=1}^m \alpha_i \bold x^T \bold x
$$
where \alpha is a vector coefficients. Rewriting the learning algorithm this way allows us to replace x by the output of a given feature function φ(x) and the dot product with a function k(x, x^i) = φ(x)·φ(x^i) called kernel. The · operator represents inner product analogous to φ(x)^T φ(x^i).
$$
f(x) = b + \sum_{i} \alpha_i k(x, x^{(i)})
$$


### Naive Bayes

* Conditional probability - The probability of observing some event (say X = i) given some other event (say Y = i) written as P(X_i | Y_i)

* Posterior probability - The probability of an outcome after the predictor information has been incorporated (in contrast to the prior probability of outcomes, not taking predictor information into account)

  

The Naive Bayes algorithm uses the probability of observing predictor values, given an outcome, to estimate the probability of observing outcome Y = i, given a set of predictor values.

To understand Bayesian classification, we can start out by imagining “non-naive” Bayesian classification. For each record to be classified: 

1. Find all the other records with the same predictor profile (i.e., where the predictor values are the same). 
2. Determine what classes those records belong to and which class is most prevalent (i.e., probable). 
3. Assign that class to the new record.

The preceding approach amounts to finding all the records in the sample that are exactly like the new record to be classified in the sense that all the predictor values are identical.

In the naive Bayes solution, we no longer restrict the probability calculation to those records that match the record to be classified. Instead, we use the entire data set. The naive Bayes modification is as follows: 

1. For a binary response Y = i (i = 0 or 1), estimate the individual conditional probabilities for each predictor  P(X_j | Y = i); these are the probabilities that the predictor value is in the record when we observe Y = i. This probability is estimated by the proportion of X_j values among the Y = i records in the training set. 
2. Multiply these probabilities by each other, and then by the proportion of records belonging to Y = i. 
3. Repeat steps 1 and 2 for all the classes. 
4. Estimate a probability for outcome i by taking the value calculated in step 2 for class i and dividing it by the sum of such values for all classes. 
5. Assign the record to the class with the highest probability for this set of predictor values.

From the definition, we see that the Bayesian classifier works only with categorical predictors (e.g., with spam classification, where presence or absence of words, phrases, characters, and so on, lies at the heart of the predictive task). To apply naive Bayes to numerical predictors, one of two approaches must be taken: 

* Bin and convert the numerical predictors to categorical predictors and apply the algorithm of the previous section. 
* Use a probability model—for example, the normal distribution (see “Normal Distribution”)—to estimate the conditional probability P(X_j | Y = i)

Naive Bayes works with categorical (factor) predictors and outcomes. It asks, “Within each outcome category, which predictor categories are most probable?” That information is then inverted to estimate probabilities of outcome categories, given predictor values.

**When to Use Naive Bayes** 

Because naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model. That said, they have several advantages:

* They are extremely fast for both training and prediction 
* They provide straightforward probabilistic prediction 
* They are often very easily interpretable 
* They have very few (if any) tunable parameters

Naive Bayes classifiers tend to perform especially well in one of the following situations:

* When the naive assumptions actually match the data (very rare in practice) 
* For very well-separated categories, when model complexity is less important 
* For very high-dimensional data, when model complexity is less important

**Пример**

Появление слова viagra увеличит вероятность того, что письмо — спам. Но это еще не окончательно. Нам нужно узнать остальное содержание данного письма. Для начала будем сосредотачиваться только на одном слове за раз, его мы обозначим как word. Затем, применив закон Байеса, мы получим:
$$
p(spam|word) = \frac{p(word|spam)p(spam)}{p(word)}
\\
p(word) = p(word|spam)p(spam) + p(word|ham)p(ham)
$$
Мы моделируем слова независимо от других слов (это также называется «независимые испытания»), так что берем результат в правой части предыдущей формулы и не подсчитываем, сколько раз встречаются эти слова. Вот почему данный метод называется наивным: мы знаем, что на самом деле есть определенные слова, которые, как правило, появляются вместе, но мы игнорируем данный факт.



### Not so Naive Bayes

For naive Bayes, the generative model is a simple axis-aligned Gaussian. With a density estimation algorithm like KDE, we can remove the “naive” element and perform the same classification with a more sophisticated generative model for each class. It’s still Bayesian classification, but it’s no longer naive. 

The general approach for generative classification is this: 

1. Split the training data by label. 
2. For each set, fit a KDE to obtain a generative model of the data. This allows you for any observation x and label y to compute a likelihood P(x|y) . 
3. From the number of examples of each class in the training set, compute the class prior, P(y) . 
4. For an unknown point x, the posterior probability for each class is P(y|x) ֣~P(x|y)P(y). The class that maximizes this posterior is the label assigned to the point.

One benefit of such a generative classifier is interpretability of results: for each unknown sample, we not only get a probabilistic classification, but a full model of the distribution of points we are comparing it to! If desired, this offers an intuitive window into the reasons for a particular classification that algorithms like SVMs and random forests tend to obscure.

**Implementation can be found in python-playground/mini_projects/ml_models_implementations/kde_classifier.py**



### K-Nearest Neighboors

The idea behind K-Nearest Neighbors (KNN) is very simple. For each record to be classified or predicted: 

1. Find K records that have similar features (i.e., similar predictor values). 
2. For classification: Find out what the majority class is among those similar records, and assign that class to the new record. 
3. For prediction (also called KNN regression): Find the average among those similar records, and predict that average for the new record.

KNN as feature engine:

1. KNN is run on the data, and for each record, a classification (or quasiprobability of a class) is derived. 
2. That result is added as a new feature to the record, and another classification method is then run on the data. The original predictor variables are thus used twice.





### Decision Tree



* Recursive partitioning - Repeatedly dividing and subdividing the data with the goal of making the outcome in each final subdivision as homogenous as possible.
* Split value - A predictor value that divides the records into those where that predictor is less than the split value, and those there it is more.
* Node - In the decision tree, a graphical or rule representation of a split value.
* Impurity - The extent to which a mix of classes is found in a subpartition of the data.
* Pruning - The process of taking a fully grown tree and progressively cutting its brunches back, to reduce overfitting.

A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a specific feature j of the feature vector is examined. If the value of the feature is below a specific threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs.

Suppose we have a response variable Y and a set of P predictor variables X for j = 1, 2 ... P. For a partition A of records, recursive partitioning will find the best way to partition A into two subpartitions: 

1. For each predictor variable X , 

   * For each value s of X :

     * Split the records in A with X values < s as one partition, and the remaining records where X ≥ s as another partition. 

     * Measure the homogeneity of classes within each subpartition of A. 

   * Select the value of s that produces maximum within-partition homogeneity of class.

2. Select the variable X and the split value s that produces maximum within-partition homogeneity of class. 

Now comes the recursive part: 

1. Initialize A with the entire data set. 
2. Apply the partitioning algorithm to split A into two subpartitions, A1 and A2 . 
3. Repeat step 2 on subpartitions A1 and A2. 
4. The algorithm terminates when no further partition can be made that sufficiently improves the homogeneity of the partitions.

In order to split the nodes at the most informative features, we need to define an objective function that we want to optimize via the tree learning algorithm. Here, our objective function is to maximize the IG at each split, which we define as follows:
$$
IG(D_p, f) = I(D_p) - \sum_{i=1}^m \frac{N_j}{N_p}I(D_j)
$$
Here, f is the feature to perform the split; 𝐷𝑝 and 𝐷𝑗 are the dataset of the parent and jth child node; I is our impurity measure; 𝑁𝑝 is the total number of training examples at the parent node; and 𝑁𝑗 is the number of examples in the jth child node. As we can see, the information gain is simply the difference between the impurity of the parent node and the sum of the child node impurities—the lower the impurities of the child nodes, the larger the information gain. However, for simplicity and to reduce the combinatorial search space, most libraries (including scikit-learn) implement binary decision trees:
$$
IG(D_p, f) = I(D_p) - \frac{N_{left}}{N_p}I(D_{left}) - \frac{N_{right}}{N_p}I(D_{right})
$$
The three impurity measures or splitting criteria that are commonly used in binary decision trees are Gini impurity (𝐼𝐺), entropy (𝐼𝐻 ), and the classification error (𝐼𝐸). Let's start with the definition of entropy for all non-empty classes (𝑝(𝑖|𝑡) ≠ 0):
$$
I_H(t) = - \sum_{i=1}^c p(i|t)log_2p(i|t)
$$
Here, 𝑝𝑝(𝑖𝑖|𝑡𝑡) is the proportion of the examples that belong to class i for a particular node, t.

When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S-, S+), is simply a weighted sum of two entropies:
$$
H(S_{-}, S_{+}) = \frac{|S_{-}|}{|S|}H(S_{-}) + \frac{|S_{+}|}{|S|}H(S_{+})
$$
So, at each step, at each leaf node, we find a split that minimizes the entropy or we stop at this leaf node.

The Gini impurity can be understood as a criterion to minimize the probability of misclassification:
$$
I_G(t) = \sum_{i=1}^c p(i|t)(1 - p(i|t)) = 1 - \sum_{i=1}^c p(i|t)^2
$$



**CART** - tree for regression.

![](img/decision-tree-cost.png)

When we used decision trees for classification, we defined entropy as a measure of impurity to determine which feature split maximizes the information gain (IG). To use a decision tree for regression, however, we need an impurity metric that is suitable for continuous variables, so we define the impurity measure of a node, t, as the MSE instead:
$$
I(t) = MSE(t) = \frac{1}{N_t} \sum_{i э D_t} (y^{(i)} - \hat y ^{(i)})^2
\\
IG(D_p, x_i) = I(D_p) - \frac{N_{left}}{N_p}I(D_{left}) - \frac{N_{right}}{N_p}I(D_{right})\\
\hat y_t = \frac{1}{N_t}\sum y^{(i)}
$$
Here, 𝑁𝑡 is the number of training examples at node t, 𝐷𝑡 is the training subset at node t, 𝑦(𝑖) is the true target value, and 𝑦̂𝑡 is the predicted target value.

In the context of decision tree regression, the MSE is often referred to as within node variance, which is why the splitting criterion is also better known as variance reduction.



**Pruning**

Pruning consists of going back through the tree once it’s been created and removing branches that don’t contribute significantly enough to the error reduction by replacing them with leaf nodes.




### Random Forest

The random forest is based on applying bagging to decision trees with one important extension: in addition to sampling the records, the algorithm also samples the variables.  The random forest algorithm adds two more steps: the bagging and the bootstrap sampling of variables at each split: 

1. Take a bootstrap (with replacement) subsample from the records. 

2. For the first split, sample p < P variables at random without replacement. 

3. For each of the sampled variables Xj1, Xj2, ..., Xjp, apply the splitting algorithm:

   * For each value s_jk of X_jk:
     * Split the records in partition A with X < s as one partition, and the remaining records where X >= s as another partition. 
     * Measure the homogeneity of classes within each subpartition of A.

   * Select the value of s_jk that produces maximum within-partition homogeneity of class

4. Select the variable X_jk and the split value s_jk that produces maximum within-partition homogeneity of class. 
5. Proceed to the next split and repeat the previous steps, starting with step 2. 
6. Continue with additional splits following the same procedure until the tree is grown. 
7. Go back to step 1, take another bootstrap subsample, and start the process over again.

The random forest algorithm can be summarized in four simple steps: 

1. Draw a random bootstrap sample of size n (randomly choose n examples from the training dataset with replacement). 
2. Grow a decision tree from the bootstrap sample. At each node: 
   * Randomly select d features without replacement. 
   * Split the node using the feature that provides the best split according to the objective function, for instance, maximizing the information gain. 
3. Repeat the steps 1-2 k times. 
4. Aggregate the prediction by each tree to assign the class label by majority vote.











### K-means

The k-means algorithm belongs to the category of prototype-based clustering. 

Prototype-based clustering means that each cluster is represented by a prototype, which is usually either the centroid (average) of similar points with continuous features, or the medoid (the most representative or the point that minimizes the distance to all other points that belong to a particular cluster) in the case of categorical features. 

Thus, our goal is to group the examples based on their feature similarities, which can be achieved using the k-means algorithm, as summarized by the following four steps: 

1. Randomly pick k centroids from the examples as initial cluster centers. 
2. Assign each example to the nearest centroid, 𝜇(𝑗) ,𝑗 ∈ {1, … , 𝑘}. 
3. Move the centroids to the center of the examples that were assigned to it. 
4. Repeat steps 2 and 3 until the cluster assignments do not change or a user defined tolerance or maximum number of iterations is reached.

K-Means also can be described as a E-M algorithm.

Expectation–maximization (E–M) is a powerful algorithm that comes up in a variety of contexts within data science. k-means is a particularly simple and easy-to understand application of the algorithm, and we will walk through it briefly here. In short, the expectation–maximization approach consists of the following procedure: 

1. Guess some cluster centers 

2. Repeat until converged 

   * E-Step: assign points to the nearest cluster center 

   * M-Step: set the cluster centers to the mean

     

### K-means++

So far, we have discussed the classic k-means algorithm, which uses a random seed to place the initial centroids, which can sometimes result in bad clustering or slow convergence if the initial centroids are chosen poorly. One way to address this issue is to run the k-means algorithm multiple times on a dataset and choose the best performing model in terms of the SSE. 

Another strategy is to place the initial centroids far away from each other via the k-means++ algorithm, which leads to better and more consistent results than the classic k-means.

The initialization in k-means++ can be summarized as follows:

1. Initialize an empty set, M, to store the k centroids being selected. 

2. Randomly choose the first centroid, 𝝁(𝑗) , from the input examples and assign it to M. 

3. For each example, 𝒙(𝑖) , that is not in M, find the minimum squared distance, 𝑑(𝒙(𝑖) , 𝚳)^2 , to any of the centroids in M. 

4. To randomly select the next centroid, 𝝁^(𝑝) , use a weighted probability distribution equal to
   $$
   \frac{d(\bold \mu^{(p)}, \bold M)^2}{\sum_i d(\bold x^{(i)}, \bold M)^2}
   $$

5. Repeat steps 2 and 3 until k centroids are chosen. 

6. Proceed with the classic k-means algorithm.





### Hierarchical Clustering

* Dendrogram - A visual representation of the records and the hierarchy of clusters to which they belong.

* Dissimilarity - A measure of how close one cluster is to another.

Hierarchical clustering starts by setting each record as its own cluster and iterates to combine the least dissimilar clusters. The main algorithm for hierarchical clustering is the agglomerative algorithm, which iteratively merges similar clusters. 

The main steps of the agglomerative algorithm are: 

1. Create an initial set of clusters with each cluster consisting of a single record for all records in the data. 
2. Compute the dissimilarity D(C_k, C_l) between all pairs of clusters k, l. 
3. Merge the two clusters C_k and C_l that are least dissimilar as measured by D(C_k, C_l). 
4. If we have more than one cluster remaining, return to step 2. Otherwise, we are done.

There are four common measures of dissimilarity: complete linkage(1), single linkage(2), average linkage, and minimum variance.
$$
D(A, B) = max(d(a_i, b_i)) for:i,j
\\
D(A, B) = min(d(a_i, b_i)) for:i,j
\\
$$
The average linkage method is the average of all distance pairs and represents a compromise between the single and complete linkage methods. Finally, the minimum variance method, also referred to as Ward’s method, is similar to K-means since it minimizes the within-cluster sum of squares.

Start with every record in its own cluster. Progressively, clusters are joined to nearby clusters until all records belong to a single cluster (the agglomerative algorithm). The agglomeration history is retained and plotted, and the user (without specifying the number of clusters beforehand) can visualize the number and structure of clusters at different stages. Inter-cluster distances are computed in different ways, all relying on the set of all inter-record distances.

The two main approaches to hierarchical clustering are agglomerative and divisive hierarchical clustering. In divisive hierarchical clustering, we start with one cluster that encompasses the complete dataset, and we iteratively split the cluster into smaller clusters until each cluster only contains one example. In this section, we will focus on agglomerative clustering, which takes the opposite approach. We start with each example as an individual cluster and merge the closest pairs of clusters until only one cluster remains.

The two standard algorithms for agglomerative hierarchical clustering are single linkage and complete linkage. Using single linkage, we compute the distances between the most similar members for each pair of clusters and merge the two clusters for which the distance between the most similar members is the smallest. The complete linkage approach is similar to single linkage but, instead of comparing the most similar members in each pair of clusters, we compare the most dissimilar members to perform the merge. 



### Locating regions of high density via DBSCAN

Density-based spatial clustering of applications with noise (DBSCAN), which does not make assumptions about spherical clusters like k-means, nor does it partition the dataset into hierarchies that require a manual cut-off point. As its name implies, density-based clustering assigns cluster labels based on dense regions of points. In DBSCAN, the notion of density is defined as the number of points within a specified radius, 𝜀𝜀.

According to the DBSCAN algorithm, a special label is assigned to each example (data point) using the following criteria: 

* A point is considered a core point if at least a specified number (MinPts) of neighboring points fall within the specified radius, 𝜀𝜀. 
* A border point is a point that has fewer neighbors than MinPts within ε, but lies within the 𝜀𝜀 radius of a core point. 
* All other points that are neither core nor border points are considered noise points.

After labeling the points as core, border, or noise, the DBSCAN algorithm can be summarized in two simple steps: 

1. Form a separate cluster for each core point or connected group of core points. (Core points are connected if they are no farther away than 𝜀𝜀.) 
2. Assign each border point to the cluster of its corresponding core point.

With an increasing number of features in our dataset—assuming a fixed number of training examples—the negative effect of the curse of dimensionality increases. This is especially a problem if we are using the Euclidean distance metric. However, the problem of the curse of dimensionality is not unique to DBSCAN: it also affects other clustering algorithms that use the Euclidean distance metric, for example, k-means and hierarchical clustering algorithms. In addition, we have two hyperparameters in DBSCAN (MinPts and 𝜀𝜀) that need to be optimized to yield good clustering results. Finding a good combination of MinPts and 𝜀𝜀 can be problematic if the density differences in the dataset are relatively large.

DBSCAN is a density-based clustering algorithm. Instead of guessing how many clusters you need, by using DBSCAN, you define two hyperparameters: ‘ and n. You start by picking an example x from your dataset at random and assign it to cluster 1. Then you count how many examples have the distance from x less than or equal to ‘. If this quantity is greater than or equal to n, then you put all these ‘-neighbors to the same cluster 1. You then examine each member of cluster 1 and find their respective ‘-neighbors. If some member of cluster 1 has n or more ‘-neighbors, you expand cluster 1 by putting those ‘-neighbors to the cluster. You continue expanding cluster 1 until there are no more examples to put in it. In the latter case, you pick from the dataset another example not belonging to any cluster and put it to cluster 2. You continue like this until all examples either belong to some cluster or are marked as outliers. An outlier is an example whose ‘-neighborhood contains less than n examples.



### Gaussian Mixture Models

A Gaussian mixture model (GMM) attempts to find a mixture of multidimensional Gaussian probability distributions that best model any input dataset.  In the simplest case, GMMs can be used for finding clusters in the same manner as k-means.

Under the hood, a Gaussian mixture model is very similar to k-means: it uses an expectation–maximization approach that qualitatively does the following: 

1. Choose starting guesses for the location and shape 
2. Repeat until converged: 
   * E-step: for each point, find weights encoding the probability of membership in each cluster
   * M-step: for each cluster, update its location, normalization, and shape based on all data points, making use of the weights

The result of this is that each cluster is associated not with a hard-edged sphere, but with a smooth Gaussian model. Just as in the k-means expectation–maximization approach, this algorithm can sometimes miss the globally optimal solution, and thus in practice multiple random initializations are used.

**GMM as Density Estimation Though** 

GMM is often categorized as a clustering algorithm, fundamentally it is an algorithm for density estimation. That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic model describing the distribution of the data.

**How many components?** 

The fact that GMM is a generative model gives us a natural means of determining the optimal number of components for a given dataset. A generative model is inherently a probability distribution for the dataset, and so we can simply evaluate the likelihood of the data under the model, using cross-validation to avoid overfitting. Another means of correcting for overfitting is to adjust the model likelihoods using some analytic criterion such as the Akaike information criterion (AIC) or the Bayesian information criterion (BIC). Scikit-Learn’s GMM estimator actually includes built-in methods that compute both of these, and so it is very easy to operate on this approach.

Notice the important point: this choice of number of components measures how well GMM works as a density estimator, not how well it works as a clustering algorithm. 



### Fuzzy K-means



Hard clustering describes a family of algorithms where each example in a dataset is assigned to exactly one cluster, as in the k-means and k-means++ algorithms that we discussed earlier in this chapter. In contrast, algorithms for soft clustering (sometimes also called fuzzy clustering) assign an example to one or more clusters. A popular example of soft clustering is the fuzzy C-means (FCM) algorithm (also called soft k-means or fuzzy k-means).  Almost a decade later, James C. Bedzek published his work on the improvement of the fuzzy clustering algorithm, which is now known as the FCM algorithm.

The FCM procedure is very similar to k-means. However, we replace the hard cluster assignment with probabilities for each point belonging to each cluster. Here, each value falls in the range [0, 1] and represents a probability of membership of the respective cluster centroid. The sum of the memberships for a given example is equal to 1. As with the k-means algorithm, we can summarize the FCM algorithm in four key steps: 

1. Specify the number of k centroids and randomly assign the cluster memberships for each point. 
2. Compute the cluster centroids, 𝝁(𝑗) ,𝑗 ∈ {1, … , 𝑘}. 
3. Update the cluster memberships for each point. 
4. Repeat steps 2 and 3 until the membership coefficients do not change or a user-defined tolerance or maximum number of iterations is reached.

The objective function of FCM—we abbreviate it as 𝐽𝑚—looks very similar to the within-cluster SSE that we minimize in k-means:
$$
J_m = \sum_{i=1}^n \sum_{j=1}^k w^{(i, j) ^m}||\bold x^{(i)} - \mu^{(j)}||^2_2
$$
However, note that the membership indicator, 𝑤𝑤(𝑖𝑖,𝑗𝑗) , is not a binary value as in k-means (𝑤𝑤(𝑖𝑖,𝑗𝑗) ∈ {0, 1}), but a real value that denotes the cluster membership probability (𝑤𝑤(𝑖𝑖,𝑗𝑗) ∈ [0, 1]). You also may have noticed that we added an additional exponent to 𝑤𝑤(𝑖𝑖,𝑗𝑗) ; the exponent m, any number greater than or equal to one (typically m = 2), is the so-called fuzziness coefficient (or simply fuzzifier), which controls the degree of fuzziness.

The larger the value of m, the smaller the cluster membership, 𝑤𝑤(𝑖𝑖,𝑗𝑗) , becomes, which leads to fuzzier clusters. The cluster membership probability itself is calculated as follows:
$$
w^{(i, j)} = [\sum_{c=1}^{k} (\frac{||\bold x ^ {(i)} - \bold \mu ^ {(j)} ||_2}{||\bold x ^ {(i)} - \bold \mu ^ {(c)} ||_2})^{\frac{2}{m-1}}] ^ {-1}
$$
The center, 𝝁^(𝑗𝑗) , of a cluster itself is calculated as the mean of all examples weighted by the degree to which each example belongs to that cluster (𝑤𝑤(𝑖𝑖,𝑗𝑗)𝑚𝑚 ):
$$
\mu ^ {(j)} = \frac{\sum_{i=1}^n w ^{(i, j)^m}x^{(i)}}{\sum_{i=1}^n w ^{(i, j)^m}}
$$

### 



### Кластеризация с помощью минимального остовного дерева

1. Строим взвешенный граф, где веса ребер - расстояния между объектами.
2. Строим минимальное отсовное дерево (алгоритм Крускала) для этого графа
3. Удаляем К-1 ребро с максимальным весом
4. Получаем К компонент связности, которые интерпретируем как кластеры





### Principal Components Analysis



1. Standardize the d-dimensional dataset. 

2. Construct the covariance matrix. For example, the covariance between two features, 𝑥𝑥𝑗𝑗 and 𝑥𝑥𝑘𝑘, on the population level can be calculated via the following equation:
   $$
   \sigma_{jk} = \frac{1}{n-1}\sum_{i=1}^{n}(x_j^{(i)}-\mu_j)(x_k^{(i)} - \mu_k)
   $$
    Note that the sample means are zero if we standardized the dataset. That's mean:

$$
   \sigma_{jk} = \frac{1}{n-1} \sum_{i=1}^n x_j^{(i)}x_k^{(i)}
$$


3. Decompose the covariance matrix into its eigenvectors and eigenvalues. 

4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors. 

5. Select k eigenvectors, which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace (𝑘𝑘 ≤ 𝑑𝑑). 

6. Construct a projection matrix, W, from the "top" k eigenvectors.

7. Transform the d-dimensional input dataset, X, using the projection matrix, W, to obtain the new k-dimensional feature subspace.

```python
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
cov_mat = np.cov(X_train.T)
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]
eigen_pairs.sort(key=lambda k: k[0], reverse=True)
w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))
X_train_pca = X_train.dot(w)
```

How PCA find the c1, c2 (principal component)? There is standard matrix factorization technique called *Singular Value Decomposition*, that decompose training set matrix X into the matrix multiplication of three matrices  U Σ V<sup>T</sup> where V contains all principal components that we are looking for.

```python
X_centered = X - X.mean(axis=0)
U, s, Vt = np.linalg.svd(X_centered)
c1 = Vt.T[:, 0]
c2 = Vt.T[:, 1]
```



Рассмотрим другой подход к прогнозированию предпочтений. В этом случае вы по-прежнему ищете U и V, но вам больше не нужно S, поэтому следует просто искать U и V, такие, что:
$$
X = UV
$$
Ваша проблема оптимизации заключается в том, что вы хотите свести к минимуму расхождение между фактическим X и приближением к X через оценку значений U и V с помощью квадратичных ошибок:
$$
argmin\sum_{i,j}(x_{i,j} - u_i v_j)^2
$$
Здесь через ui обозначена строка матрицы U, соответствующая i-му пользователю, и аналогично через vj обозначена строка матрицы V, соответствующая j-му элементу. Как обычно, элементы могут включать информацию о метаданных (поэтому вектором возраста всех пользователей будет строка в V). Тогда значение скалярного произведения ui · vj является предсказанным предпочтением i-го пользователя для j-го элемента, и вы хотите, чтобы оно было как можно ближе к действительному предпочтению xi,j

Пока ваша задача выпуклая, решение будет хорошо сходиться (то есть вы не обнаружите себя в локальном, но не глобальном максимуме) и вы можете заставить вашу задачу быть таковой, используя регуляризацию.

Ниже приведен алгоритм. 

* Выберите случайную матрицу V.
* Оптимизируйте U при фиксированной V. 
* Оптимизируйте V при фиксированной U. 
* Продолжайте выполнять два предыдущих шага, пока все полностью не изменится. Точнее, вы выбираете значение ϵ, и если ваши коэффициенты изменяются меньше чем на ϵ, то объявляете свой алгоритм конвергентным.



```python
import math,numpy

pu = [[(0,0,1),(0,1,22),(0,2,1),(0,3,1),(0,5,0)],
      [(1,0,1),(1,1,32),(1,2,0),(1,3,0),(1,4,1),(1,5,0)],
      [(2,0,0),(2,1,18),(2,2,1),(2,3,1),(2,4,0),(2,5,1)],
      [(3,0,1),(3,1,40),(3,2,1),(3,4,0),(3,5,1)],
      [(4,0,0),(4,1,40),(4,2,0),(4,4,1)],
      [(5,0,0),(5,1,25),(5,2,1),(5,3,1),(5,4,1)]]

pv = [[(0,0,1),(0,1,1),(0,2,0),(0,3,1),(0,4,0),(0,5,0)],
      [(1,0,22),(1,1,32),(1,2,18),(1,3,40),(1,4,40),(1,5,25)],
      [(2,0,1),(2,1,0),(2,2,1),(2,3,1),(2,4,0),(2,5,1)],
      [(3,0,1),(3,2,1),(3,3,0),(3,5,1)],
      [(4,1,1),(4,2,0),(4,3,0),(4,5,1)],
      [(5,0,0),(5,1,0),(5,2,1),(5,3,1),(5,4,0)]]

V = numpy.mat([[ 0.15968384, 0.9441198 , 0.83651085],
               [ 0.73573009, 0.24906915, 0.85338239],
               [ 0.25605814, 0.6990532 , 0.50900407],
               [ 0.2405843 , 0.31848888, 0.60233653],
               [ 0.24237479, 0.15293281, 0.22240255],
               [ 0.03943766, 0.19287528, 0.95094265]])

U = numpy.mat(numpy.zeros([6,3]))
L = 0.03

for iter in xrange(5):
	urs = []
	for uset in pu:
		vo = []
		pvo = []
		for i,j,p in uset:
			vor = []
			for k in range(3):
				vor.append(V[j,k])
                
			vo.append(vor)
			pvo.append(p) 
			vo = numpy.mat(vo)
			ur = numpy.linalg.inv(vo.T*vo + L*numpy.mat(numpy.eye(3))) * vo.T * numpy.mat(pvo).T
            
			urs.append(ur.T)
	U = numpy.vstack(urs)

	vrs = []
	for vset in pv:
		uo = []
		puo = []
		for j,i,p in vset:
			uor = []
			for k in xrange(3):
				uor.append(U[i,k])
			uo.append(uor)
			puo.append(p)
		uo = numpy.mat(uo)
		vr = numpy.linalg.inv(uo.T*uo + L*numpy.mat(num py.eye(3))) * uo.T * numpy.mat(puo).T
		vrs.append(vr.T)
	V = numpy.vstack(vrs)

	err = 0.
	n = 0.
	for uset in pu:
		for i,j,p in uset:
			err += (p — (U[i]*V[j].T)[0,0])**2
			n += 1

```



### 





### Linear Discriminant Analysis

It is a method that tries to maximize the distance of points belonging to different classes while minimizing the distance of points of the same class.

The general concept behind LDA is very similar to PCA, but whereas PCA attempts to find the orthogonal component axes of maximum variance in a dataset, the goal in LDA is to find the feature subspace that optimizes class separability.

One assumption in LDA is that the data is normally distributed. Also, we assume that the classes have identical covariance matrices and that the training examples are statistically independent of each other. However, even if one, or more, of those assumptions is (slightly) violated, LDA for dimensionality reduction can still work reasonably well.

Before we dive into the code implementation, let's briefly summarize the main steps that are required to perform LDA: 

1. Standardize the d-dimensional dataset (d is the number of features). 
2. For each class, compute the d-dimensional mean vector. 
3. Construct the between-class scatter matrix, 𝑺𝐵, and the within-class scatter matrix, 𝑺𝑤.
4. Compute the eigenvectors and corresponding eigenvalues of the matrix, 𝑺𝑊^(-1) * 𝑺𝐵. 
5. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors. 
6. Choose the k eigenvectors that correspond to the k largest eigenvalues to construct a 𝑑 × 𝑘-dimensional transformation matrix, W; the eigenvectors are the columns of this matrix. 
7. Project the examples onto the new feature subspace using the transformation matrix, W

 Each mean vector, 𝒎𝑖 , stores the mean feature value, 𝜇𝜇𝑚𝑚, with respect to the examples of class i.

Using the mean vectors, we can now compute the within-class scatter matrix, 𝑺𝑊:
$$
S_W = \sum_{i=1}^c S_i
\\
\\
S_i = \sum_{x э D_i} (x - m_i)(x - m_i)^T
$$
The assumption that we are making when we are computing the scatter matrices is that the class labels in the training dataset are uniformly distributed.  When we divide the scatter matrices by the number of class examples, 𝑛𝑖, we can see that computing the scatter matrix is in fact the same as computing the covariance matrix, Σ𝑖𝑖 —the covariance matrix is a normalized version of the scatter matrix.

After we compute the scaled within-class scatter matrix (or covariance matrix), we can move on to the next step and compute the between-class scatter matrix 𝑺𝑩:
$$
S_B = \sum_{i=1}^c n_i (m_i - m) (m_i - m)^T
$$
Here, m is the overall mean that is computed, including examples from all c classes.

The remaining steps of the LDA are similar to the steps of the PCA. However, instead of performing the eigendecomposition on the covariance matrix, we solve the generalized eigenvalue problem of the matrix, 𝑺𝑊^(-1) * 𝑺𝐵. 

```python
d = 13 # number of features
S_W = np.zeros((d, d))
for label, mv in zip(range(3), means):
    class_scatter = np.zeros((d, d))
    cnt = 0
    for row in features[target == label]:
        row, mv = row.reshape(d, 1), mv.reshape(d, 1)
        class_scatter += (row - mv).dot((row - mv).T)
        cnt += 1
    
    S_W += class_scatter / cnt

m = features.mean(axis=0)
S_B = np.zeros((d, d))
for i, mean in enumerate(means):
    n = features[target == i].shape[0]
    mean = mean.reshape(d, 1)
    m = m.reshape(d, 1)
    S_B += n*(mean - m).dot((mean-m).T)

matrix = np.linalg.inv(S_W).dot(S_B)
eigen_vals, eigen_vecs = np.linalg.eig(matrix)
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) 
               for i in range(len(eigen_vals))] 
eigen_pairs = sorted(eigen_pairs,key=lambda k: k[0], reverse=True)
w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real, eigen_pairs[1][1][:, np.newaxis].real))
X_train_lda = features.dot(w)
```

### Kernel PCA



If we are dealing with nonlinear problems, which we may encounter rather frequently in real-world applications, linear transformation techniques for dimensionality reduction, such as PCA and LDA, may not be the best choice.

We perform a nonlinear mapping via KPCA that transforms the data onto a higher-dimensional space. We then use standard PCA in this higher dimensional space to project the data back onto a lower-dimensional space where the examples can be separated by a linear classifier (under the condition that the examples can be separated by density in the input space). However, one downside of this approach is that it is computationally very expensive, and this is where we use the kernel trick. Using the kernel trick, we can compute the similarity between two high-dimension feature vectors in the original feature space.

Covariance matrix:
$$
\sum = \frac{1}{n} \sum_{i=1}^n \bold x^{(i)} \bold x^{(i)^T}
$$
Bernhard Scholkopf generalized this approach (Kernel principal component analysis, B. Scholkopf, A. Smola, and K.R. Muller, pages 583-588, 1997) so that we can replace the dot products between examples in the original feature space with the nonlinear feature combinations via 𝜙:
$$
\sum = \frac{1}{n}\sum_{i=1}^{n} \phi(\bold x^{(i)}) \phi(\bold x^{(i)^T})
$$
To obtain the eigenvectors—the principal components—from this covariance matrix, we have to solve the following equation:
$$
\sum \bold v = \lambda \bold v
\\
=> \frac{1}{n}\sum_{i=1}^n \phi(\bold x^{(i)}) \phi(\bold x^{(i)^T})\bold v = \lambda \bold v
\\
\bold v = \frac{1}{n\lambda}\sum_{i=1}^n \phi(\bold x^{(i)}) \phi(\bold x^{(i)^T}) \bold v = \frac{1}{n}\sum_{i=1}^{n} \bold a^{(i)} \phi(\bold x ^ {(i)})
\\
\bold K = \phi(\bold X) \phi (\bold X)^ T
$$
Here, 𝜆𝜆 and v are the eigenvalues and eigenvectors of the covariance matrix, Σ, and **a** can be obtained by extracting the eigenvectors of the kernel (similarity) matrix, K, as you will see in the following paragraphs.





To summarize what we have learned so far, we can define the following three steps to implement an RBF  (function of similarity) KPCA: 

 1. We compute the kernel (similarity) matrix, K, where we need to calculate the following:
    $$
    k(x^{(i)}, x^{(j)}) = exp(-\gamma(||x^{(i)} - x^{(j)}||^2))
    $$
    For example, if our dataset contains 100 training examples, the symmetric kernel matrix of the pairwise similarities would be 100 × 100-dimensional.

2. We center the kernel matrix, K, using the following equation:
   $$
   \bold K' = \bold K - 1_n\bold K - \bold K1_n +1_n\bold K1_n
   $$
   Here, 𝟏𝒏 is an 𝑛𝑛 × 𝑛𝑛-dimensional matrix (the same dimensions as the kernel matrix) where all values are equal to 1/𝑛 .

3. We collect the top k eigenvectors of the centered kernel matrix based on their corresponding eigenvalues, which are ranked by decreasing magnitude. In contrast to standard PCA, the eigenvectors are not the principal component axes, but the examples already projected onto these axes.

```python
gamma = 15
n_components = 2
# Calculate pairwise squared Euclidean distances
# in the MxN dimensional dataset
sqdists = pdist(x, 'sqeuclidean')
# Convert pairwise distances into a square matrix
mat_sq_dist = squareform(sqdists)
# Compute the symmetric kernel matrix.
K = exp(-gamma * mat_sq_dist)
# Center the kernel matrix.
N = K.shape[0]
one_n = np.ones((N,N)) / N
K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)
# Obtaining eigenpairs from the centered kernel matrix
# scipy.linalg.eigh returns them in ascending order
eigvals, eigvecs = eigh(K)
eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]
# Collect the top k eigenvectors (projected examples)
alphas = np.column_stack([eigvecs[:, i] for i in range(n_components)])
# Collect the corresponding eigenvalues
lambdas = [eigvals[i] for i in range(n_components)]

```

To project new data we have to calculate the pairwise RBF kernel (similarity) between each ith example in the training dataset and the new example, 𝒙𝒙′:
$$
\phi (x')^T v = \sum_i a^{(i)}\phi(x')^T \phi(x^{(i)}) = \sum_i a^{(i)} k(x', x^{(i)})
$$
After calculating the similarity between the new examples and the examples in the training dataset, we have to normalize the eigenvector, a, by its eigenvalue. 

```python
def project_x(x_new, X, gamma, alphas, lambdas):
	pair_dist = np.array([np.sum((x_new-row)**2) for row in X])
	k = np.exp(-gamma * pair_dist)
	return k.dot(alphas / lambdas)
```

### UMAP

The idea behind many of the modern dimensionality reduction algorithms, especially those designed specifically for visualization purposes, such as t-SNE and UMAP, is basically the same. We first design a similarity metric for two examples. For visualization purposes, besides the Euclidean distance between the two examples, this similarity metric often reflects some local properties of the two examples, such as the density of other examples around them.

In UMAP, this similarity metric w is defined as follows,
$$
w(x_i, x_j) = w_i(x_i, x_j) + w_j(x_j, x_i) - w_i(x_i, x_j)w_j(x_j, x_i)
$$
The function wi(xi, xj ) is defined as,
$$
w_i(x_i, x_j) = exp(-\frac{d(x_i, x_j) - p_i}{\sigma_i})
$$
Let w denote the similarity of two examples in the original high-dimensional space and let w' be the similarity given by the same eq. 5 in the new low-dimensional space. Because the values of w and w' lie in the range between 0 and 1, we can see them as probabilities of two binary random variables. A widely used metric of similarity between two probability distributions is cross-entropy:
$$
C_{w, w'} = \sum_{i=1}^N \sum_{j=1}^N [w(x_i, x_j) ln(\frac{w(x_i,x_j)}{w'(x'_i, x'_j)}) + (1 - w(x_i, x_j))ln(\frac{1 - w(x_i,x_j)}{1 - w'(x'_i, x'_j)}))]
$$









## Natural language processing



### Latent Dirichlet Allocation

LDA is a generative probabilistic model that tries to find groups of words that appear frequently together across different documents. These frequently appearing words represent our topics, assuming that each document is a mixture of different words. The input to an LDA is the bag-of-words model that we discussed earlier in this chapter. Given a bag-of-words matrix as input, LDA decomposes it into two new matrices: 

* A document-to-topic matrix 
* A word-to-topic matrix

LDA decomposes the bag-of-words matrix in such a way that if we multiply those two matrices together, we will be able to reproduce the input, the bag-of-words matrix, with the lowest possible error. 



In particular, we have a collection of documents, each of which is a list of words. And we have a corresponding collection of document_topics that assigns a topic (here a number between 0 and K – 1) to each word in each document. We can estimate the likelihood that topic 1 produces a certain word by comparing how many times topic 1 produces that word with how many times topic 1 produces any word. 

We start by assigning every word in every document a topic completely at random. Now we go through each document one word at a time. For that word and document, we construct weights for each topic that depend on the (current) distribution of topics in that document and the (current) distribution of words for that topic. We then use those weights to sample a new topic for that word. If we iterate this process many times, we will end up with a joint sample from the topic–word distribution and the document– topic distribution.



#### VADER - A rule-based sentiment analyzer



Hutto and Gilbert at GA Tech came up with one of the first successful rule-based sentiment analysis algorithms. They called their algorithm VADER, for Valence Aware Dictionary for sEntiment Reasoning. Many NLP packages implement some form of this algorithm. The NLTK package has an implementation of the VADER algorithm in nltk.sentiment.vader.

```python
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
sa = SentimentIntensityAnalyzer()

corpus = ["Absolutely perfect! Love it! :-) :-) :-)",
          "Horrible! Completely useless. :(",
          "It was OK. Some good and some bad things."]
for doc in corpus:
    scores = sa.polarity_scores(doc)
    print('{:+}: {}'.format(scores['compound'], doc))
```

The only drawback is that VADER doesn’t look at all the words in a document, only about 7,500.







## Computer Vision



### Histogram of Oriented Gradients

The Histogram of Gradients is a straightforward feature extraction procedure that was developed in the context of identifying pedestrians within images. HOG involves the following steps: 

1. Optionally prenormalize images. This leads to features that resist dependence on variations in illumination.
2. Convolve the image with two filters that are sensitive to horizontal and vertical brightness gradients. These capture edge, contour, and texture information. 
3. Subdivide the image into cells of a predetermined size, and compute a histogram of the gradient orientations within each cell.
4. Normalize the histograms in each cell by comparing to the block of neighboring cells. This further suppresses the effect of illumination across the image. 
5. Construct a one-dimensional feature vector from the information in each cell.

```python
from skimage import data, color, feature
import skimage.data
 
image = color.rgb2gray(data.chelsea())
hog_vec, hog_vis = feature.hog(image, visualise=True)

fig, ax = plt.subplots(1, 2, figsize=(12, 6),
                       subplot_kw=dict(xticks=[], yticks=[]))

ax[0].imshow(image, cmap='gray')
ax[0].set_title('input image')

ax[1].imshow(hog_vis)
ax[1].set_title('visualization of HOG features');

```

**HOG in Action: A Simple Face Detector** 

Using these HOG features, we can build up a simple facial detection algorithm with any Scikit-Learn estimator; here we will use a linear support vector machine. 

The steps are as follows: 

1. Obtain a set of image thumbnails of faces to constitute “positive” training samples. 
2. Obtain a set of image thumbnails of nonfaces to constitute “negative” training samples. 
3. Extract HOG features from these training samples.
4. Train a linear SVM classifier on these samples.
5. For an “unknown” image, pass a sliding window across the image, using the model to evaluate whether that window contains a face or not. 
6. If detections overlap, combine them into a single window.



















# Models fitting and estimating



### Train/validation split



**Cross-validation (кросс проверка)**

Cross-validation works like follows. First, you fix the values of the hyperparameters you want to evaluate. Then you split your training set into several subsets of the same size. Each subset is called a fold. Typically, five-fold cross-validation is used in practice. With five-fold cross-validation, you randomly split your training data into five folds: {F1, F2,...,F5}. Each Fk, k = 1,..., 5 contains 20% of your training data. Then you train five models as follows. To train the first model, f1, you use all examples from folds F2, F3, F4, and F5 as the training set and the examples from F1 as the validation set. To train the second model, f2, you use the examples from folds F1, F3, F4, and F5 to train and the examples from F2 as the validation set. You continue building models iteratively like this and compute the value of the metric of interest on each validation set, from F1 to F5. Then you average the five values of the metric to get the final value

**Hold-out validation**

**Leave-one-out**

**q-fold Cross-validation (Поблочная кросс-проверка)**

**t * q-fold Cross-validation (Многократная поблочная кросс-проверка)**





### Maximum likelihood

Consider a set of *m* examples X drawn independently from the true but unknow data generating distribution p_{data}(x). The maximum likelihood estimator for θ is then given as:
$$
\theta_{ML} = argmax_{\theta} p_{model}(X; \theta) = argmax_{\theta} \prod_{i=1}^m p_{model} (x^{(i)};\theta)
$$
Maximizing likelihood corresponds exactly to minimizing the cross-entropy between distributions (for example MSE is the cross-entropy between empirical and Gaussian distributions). We can thus see maximum likelihood as an attempt to make the model distribution match the empirical distribution.

The likelihood function indicates how likely the observed sample is as a function of possible parameter values. Therefore, maximizing the likelihood function determines the parameters that are most likely to produce the observed data. From a statistical point of view, MLE is usually recommended for large samples because it is versatile, applicable to most models and different types of data, and produces the most precise estimates.

Least squares estimates are calculated by fitting a regression line to the points from a data set that has the minimal sum of the deviations squared (least square error). In reliability analysis, the line and the data are plotted on a probability plot.

**Maximum likelihood properties**

1. If T is the maximum likelihood estimator of parameter \theta and g(\theta) is an invertible function of \theta then g(T) is the maximum likelihood estimator of g(\theta).
2. The maximum likelihood estimator T may be biased. But asymptotically (as the size n of the dataset goes to infinity) maximum likelihood estimators are unbiased.
3. Maximum likelihood estimators have asymptotically the smallest variance among unbiased estimators.



**Для регрессии** 

Модель данных с некоррелированным гауссовким шумом:
$$
y(x_i) = f(x_i, \alpha) + \epsilon, \epsilon = N(0, \sigma_i^2)
$$
Метод максимума правдоподобия и метод наименьших квадратов:
$$
L(\epsilon_1, ... , \epsilon_n|\alpha) = \prod_{i=1}^l \frac{1}{\sigma_i \sqrt{2\pi}}exp(-\frac{1}{2\sigma^2_i}\epsilon_i^2) \rightarrow max_{\alpha};
\\
-lnL(\epsilon_1, ... , \epsilon_n|\alpha) = const(\alpha) + \frac{1}{2}\sum_{i=1}^l \frac{1}{\sigma^2_i}(f(x_i, \alpha) - y_i)^2 \rightarrow min_{\alpha}
$$
Постановки ММП и МНК совпадают, причем веса объектов обратно пропорциональны дисперсии шума.

**Для классификации**

Можем линейную модель с заданной функцией потерь переинтерпретировать как параметрическую модель вероятности класса у.

Максимизация правдоподобия:
$$
L(w) = \sum_{i=1}^l logP(y_i|x_i, w) \rightarrow max_w
$$
Минимизация имперического риска:
$$
Q(w) = \sum_{i=1}^l L(y_ig(x_i, w)) \rightarrow min_w
$$
Эти два принципа эквивалентны, если положить
$$
-logP(y_i|x_i, w) = L(y_ig(x_i, w))
$$



###  Maximum a Posteriori Estimation



While the most principled approach is to make predictions using the full Bayesian posterior distribution over the parameter θ, it is still often desirable to have a single point estimate. One common reason for desiring a point estimate is that most operations involving the Bayesian posterior for most interesting models are intractable, and a point estimate offers a tractable approximation. Rather than simply returning to the maximum likelihood estimate, we can still gain some of the benefit of the Bayesian approach by allowing the prior to influence the choice of the point estimate.

The MAP estimate chooses the point of maximal posterior probability (or maximal probability density in the more common case of continuous θ):
$$
\theta_{MAP} = argmax_{\theta} p(\theta | x) = argmax_{\theta}p(x|\theta) + log(p(\theta))
$$
As with full Bayesian inference, MAP Bayesian inference has the advantage of leveraging information that is brought by the prior and cannot be found in the training data. This additional information helps to reduce the variance in the MAP point estimate (in comparison to the ML estimate). However, it does so at the price of increased bias. Many regularized estimation strategies, such as maximum likelihood learning regularized with weight decay, can be interpreted as making the MAP approximation to Bayesian inference.













### Gradient Descent





### Newton's Method



```python
from autograd import grad
from autograd import hessian

# import NumPy library
import numpy as np
# Newton ’s method
def newtons_method (g, max_its, w):
    # compute gradient/ Hessian using autograd
    gradient = grad(g)
    hess = hessian (g)
    
    # set numerical stability parameter
    epsilon = 10 **(-7)
    if ’epsilon ’ in kwargs:
        epsilon = kwargs[’epsilon ’]

    # run the Newton ’s method loop
    weight_history = [w] # container for weight history
    cost_history = [g(w)] # container for cost function history

    for k in range( max_its ):
        # evaluate the gradient and hessian
        grad_eval = gradient (w)
        hess_eval = hess(w)

        # reshape hessian to square matrix
        hess_eval .shape = (int((np. size( hess_eval ))**(0 .5)),int((np.size( hess_eval ))**(0 .5)))

        # solve second -order system for weight update
        A = hess_eval + epsilon* np. eye(w. size)
        b = grad_eval
        w = np. linalg. solve(A, np .dot(A ,w)-b)

        # record weight and cost
        weight_history. append(w)
        cost_history.append(g(w))
        
    return weight_history, cost_history
```



Newton’s method is a powerful algorithm that makes enormous progress towards finding a function’s minimum at each step, compared to zero- and first order methods that can require a large number of steps to make equivalent progress. However, Newton’s method suffers from its own unique weaknesses – primarily in dealing with nonconvexity, as well as scaling with input dimension.





### Iteratively Reweighted Least Squares

(МНК с итерационным перевзвешиванием объектов)

Что делать, если модель регрессии не линейная или функция потерь не квадратичная? Общий рецепт такой: применение метода Ньютона-Рафсона приводит к итерационному процессу, на каждом шаге которого решается задача линейной регрессии. Смысл её сводится к тому, чтобы поточнее настроиться на тех объектах, на которых модель в текущем её приближении работает недостаточно хорошо. В этот общий сценарий неплохо вписывается серия важных частных случаев. Нелинейная регрессия с квадратичной функцией потерь. Логистическая регрессия. Обобщённая линейная модель (GLM), в которой прогнозируемая величина описывается экспоненциальным семейством распределений. Логистическая регрессия является частным случаем GLM, и, благодаря этому факту, мы теперь понимаем, почему вероятность классов выражается через сигмоиду от дискриминантной функции

**Вход**: F, y - матрица объекты признаки и вектор ответов

**Выход**: w - вектор коэффициентов линейной коомбинации.

1. w = (F^T F) ^ {-1} F^T y - начальное приближение, обычный МНК
2. для t = 1, 2, 3...
   * \sigma_i = \sigma(y_i w^T x)  для всех i
   * \gamma_i = \sqrt{(1 - \sigma_i) \sigma_i} для всех i
   * ~F = diag(\gamma_1, ... \gamma_n)F
   * ~y_i = y_i \sqrt{(1 - \sigma_i) \sigma_i} для всех i
   * выбрать градиентный шаг h_t
   * w = w + h_t (~F^T ~F)^{-1} ~F^T ~y
   * если \sigma_i мало изменилось, выйти из цикла







### Regression metrics

* Residual standard error - The same as the RMSE, but adjusted for degrees of freedom.

* R-squared - The proportion of variance explained by the model, from 0 to 1. It is useful mainly in explanatory uses of regression where you want to assess how well the model fits the data.

* t-statistic - The coefficient for a predictor, divided by the standard error of the coefficient, giving a metric to compare the importance of variables in the model. The t-statistic—and its mirror image, the p-value—measures the extent to which a coefficient is “statistically significant”—that is, outside the range of what a random chance arrangement of predictor and target variable might produce. The higher the t-statistic (and the lower the p-value), the more significant the predictor.

* Weighted regression - Regression with the records having different weights.

  

$$
MSE = \frac{\sum_{i=1}^{n}(y_i - \hat y_i)^2}{n}
\\
RMSE = \sqrt{\frac{\sum_{i=1}^{n}(y_i - \hat y_i)^2}{n}}
\\
R^2 = 1 - \frac{\sum_{i = 1}^{n}(y_i - \hat y_i)^2}{\sum_{i = 1}^{n}(y_i - \bar y_i)^2}
\\
t_b = \frac{\hat b}{RMSE(\hat b)} 
\\
AIC = 2P + nlog(RSS/n)
$$

AIC - Akaike's information criteria, penalizes adding terms to a model. P - is the number of variables and n is the number of records.

How do we find the model that minimizes AIC? One approach is to search through all possible models, called all subset regression. This is computationally expensive and is not feasible for problems with large data and many variables. An attractive alternative is to use stepwise regression, which successively adds and drops predictors to find a model that lowers AIC.

Simpler yet are forward selection and backward selection. In forward selection, you start with no predictors and add them one-by-one, at each step adding the predictor that has the largest contribution to R^2, stopping when the contribution is no longer statistically significant. In backward selection, or backward elimination, you start with the full model and take away predictors that are not statistically significant until you are left with a model in which all predictors are statistically significant.

Penalized regression is similar in spirit to AIC. Instead of explicitly searching through a discrete set of models, the model-fitting equation incorporates a constraint that penalizes the model for too many variables (parameters). Rather than eliminating predictor variables entirely—as with stepwise, forward, and backward selection—penalized regression applies the penalty by reducing coefficients, in some cases to near zero. Common penalized regression methods are ridge regression and lasso regression.



### Regression residuals plot

Since our model uses multiple explanatory variables, we can't visualize the linear regression line (or hyperplane, to be precise) in a two-dimensional plot, but we can plot the residuals (the differences or vertical distances between the actual and predicted values) versus the predicted values to diagnose our regression model. Residual plots are a commonly used graphical tool for diagnosing regression models. They can help to detect nonlinearity and outliers, and check whether the errors are randomly distributed.

```python
plt.scatter(y_train_pred, y_train_pred - y_train,
			c='steelblue', marker='o', edgecolor='white',
			label='Training data')
plt.scatter(y_test_pred, y_test_pred - y_test,
			c='limegreen', marker='s', edgecolor='white',
			label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()
```

![](img/residual plot.png)

In the case of a perfect prediction, the residuals would be exactly zero, which we will probably never encounter in realistic and practical applications. However, for a good regression model, we would expect the errors to be randomly distributed and the residuals to be randomly scattered around the centerline. If we see patterns in a residual plot, it means that our model is unable to capture some explanatory information, which has leaked into the residuals, as you can slightly see in our previous residual plot. Furthermore, we can also use residual plots to detect outliers, which are represented by the points with a large deviation from the centerline.







## 





### Classification metrics

For classification, things are a little bit more complicated. The most widely used metrics and tools to assess the classification model are: 

* confusion matrix
* accuracy - не учитывается ни дисбаланс классов, ни цена ошибки на объектах разных классов.
* cost-sensitive accuracy
* precision/recall
* area under the ROC curve.



**Precision, Recall, Sensitivity, Specificity (Точность, Полнота, Чувствительность, Специфичность)**
$$
accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\\
precision = \frac{TP}{TP + FP}
\\
recall = \frac{TP}{TP + FN}
\\
sensitivity = \frac{TP}{TP+FN}
\\
specificity = \frac{TN}{TN + FP}
\\
F1 = 2 \frac{precision*recall}{precision+recall}
\\
TPR = \frac{TP}{TP+FN}
\\
FPR = \frac{FP}{FP+TN}
$$

Precision - количество сбитых самолетов / общее количество выстрелов

Recall - количество сбитых самолетов / общее количество самолетов

The precision is the proportion of relevant documents in the list of all returned documents. The recall is the ratio of the relevant documents returned by the search engine to the total number of the relevant documents that could have been returned.

However, scikit-learn also implements macro and micro averaging methods to extend those scoring metrics to multiclass problems via one-vs.-all (OvA) classification. The micro-average is calculated from the individual TPs, TNs, FPs, and FNs of the system. For example, the micro-average of the precision score in a k-class system can be calculated as follows:
$$
precision_{micro} = \frac{TP_1 + ... + TP_k}{TP_1 + ... + TP_k + FP_1 + ... FP_k}
\\
precision_{macro} = \frac{precision_1 + ... precision_k}{k}
$$



**Cost-sensitive accuracy**

For dealing with the situation in which different classes have different importance, a useful metric is cost-sensitive accuracy. To compute a cost-sensitive accuracy, you first assign a cost (a positive number) to both types of mistakes: FP and FN. You then compute the counts TP, TN, FP, FN as usual and multiply the counts for FP and FN by the corresponding cost before calculating the accuracy.

**Area under the ROC Curve (AUC)**

The ROC curve (stands for “receiver operating characteristic,” the term comes from radar engineering) is a commonly used method to assess the performance of classification models. ROC curves use a combination of the true positive rate (defined exactly as recall) and false positive rate (the proportion of negative examples predicted incorrectly) to build up a summary picture of the classification performance. 
$$
TPR = \frac{TP}{(TP+FN)}
\\
FPR = \frac{FP}{(FP+TN)}
$$
TPR - чувствительность, FPR - специфичность.

ROC curves can only be used to assess classifiers that return some confidence score (or a probability) of prediction. For example, logistic regression, neural networks, and decision trees (and ensemble models based on decision trees) can be assessed using ROC curves.

To draw a ROC curve, you first discretize the range of the confidence score. If this range for a model is [0, 1], then you can discretize it like this: [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]. Then, you use each discrete value as the prediction threshold and predict the labels of examples in your dataset using the model and this threshold. For example, if you want to compute TPR and FPR for the threshold equal to 0.7, you apply the model to each example, get the score, and, if the score if higher than or equal to 0.7, you predict the positive class; otherwise, you predict the negative class.





### Quantifying the quality of clustering via silhouette plots

Another intrinsic metric to evaluate the quality of a clustering is silhouette analysis, which can also be applied to clustering algorithms other than k-means, which we will discuss later in this chapter. Silhouette analysis can be used as a graphical tool to plot a measure of how tightly grouped the examples in the clusters are. To calculate the silhouette coefficient of a single example in our dataset, we can apply the following three steps:

1. Calculate the cluster cohesion, 𝑎^(𝑖) , as the average distance between an example, 𝒙^(𝑖) , and all other points in the same cluster. 

2. Calculate the cluster separation, 𝑏^(𝑖) , from the next closest cluster as the average distance between the example, 𝒙^(i) , and all examples in the nearest cluster. 

3. Calculate the silhouette, 𝑠^(𝑖) , as the difference between cluster cohesion and separation divided by the greater of the two, as shown here:
   $$
   s^{(i)} = \frac{b^{(i)} - a^{(i)}}{max(b^{(i)}, a^{(i)})}
   $$

The silhouette coefficient is bounded in the range –1 to 1. Based on the preceding equation, we can see that the silhouette coefficient is 0 if the cluster separation and cohesion are equal (𝑏𝑏(𝑖𝑖) = 𝑎𝑎(𝑖𝑖) ). Furthermore, we get close to an ideal silhouette coefficient of 1 if 𝑏𝑏(𝑖𝑖) ≫ 𝑎𝑎(𝑖𝑖) , since 𝑏𝑏(𝑖𝑖) quantifies how dissimilar an example is from other clusters, and 𝑎𝑎(𝑖𝑖) tells us how similar it is to the other examples in its own cluster.





# Models improvement





### Regularization

Regularization, significantly reduces the variance of the model, without substantial increase in its bias (this means we avoid overfitting). 

**L1, L2 regularization:**


$$
L1 = ||w|| = \sum_{j=1}^m |w_j|
\\
L2 = \frac{\lambda}{2}||\bold w||^2 = \frac{\lambda}{2}\sum_{j=1}^{m}w_j^2
$$
So we add regularization term to loss function with some parameter lambda. This means we penalized model for large values of weights. Lambda is the tuning parameter that decides how much we want to penalize the flexibility of our model.  Regression example:
$$
J(\bold w)_{Ridge} = \sum_{i=1}^n (y^{(i)} - \hat y^{(i)})^2 +\lambda ||\bold w||_2^2
\\
J(\bold w)_{Lasso} = \sum_{i=1}^n (y^{(i)} - \hat y^{(i)})^2 +\lambda ||\bold w||_1
\\
J(\bold w)_{ElasticNet} = \sum_{i=1}^n (y^{(i)} - \hat y^{(i)})^2 + \lambda_1\sum_{j=1}^{m} w_j^2 + \lambda_2 \sum_{j=1}^{m} |w_j|
$$
In contrast to L2 regularization, L1 regularization usually yields sparse feature vectors and most feature weights will be zero.

С вероятностной точки зрения L2 регуляризация это предположение о том, что вектор параметров w имеет гауссовское распределение с центром в нуле и по всем координатам одинаковые дисперсии:
$$
p(w;C) = \frac{1}{(2\pi C)^{n/2}}exp(-\frac{||w||^2}{2C})
\\
-ln(p(w; C)) = \frac{1}{2C}||w||^2
$$
С вероятностной точки зрения L1 регуляризация это предположение о том, что вектор параметров w имеет распределение Лапласа с центром в нуле и по всем координатам одинаковые дисперсии:
$$
p(w;C) = \frac{1}{(2C)^{n}}exp(-\frac{||w||}{C})
\\
-ln(p(w; C)) = \frac{1}{C}||w||
$$
Где С - гиперпараметр, 1/С - коэффициент регуляризации.

**Another regularization techniques: ** 

1. Injecting noise at the input

2. Injecting noise at the output. For example, label smoothing regularizes a model based on a softmax with k output values by replacing the hard 0 and 1 classification targets with targets of:
   $$
   \frac{\epsilon}{k}
   \\
   1 - \frac{k - 1}k \epsilon
   $$
   respectively.

3. Semi-supervised learning. In the paradigm of semi-supervised learning, both unlabeled examples from P(x) and labeled examples from P(x, y) are used to estimate P(y | x) or predict y from x.  In the context of deep learning, semi-supervised learning usually refers to learning a representation h = f(x). The goal is to learn a representation so that examples from the same class have similar representations.

**For neural network:**



The concept of **dropout** is very simple. Each time you run a training example through the network, you temporarily exclude at random some units from the computation. The higher the percentage of units excluded the higher the regularization effect. Neural network libraries allow you to add a dropout layer between two successive layers, or you can specify the dropout parameter for the layer. The dropout parameter is in the range [0, 1] and it has to be found experimentally by tuning it on the validation data. 

**Early stopping** is the way to train a neural network by saving the preliminary model after every epoch and assessing the performance of the preliminary model on the validation set. As the number of epochs increases, the cost decreases. The decreased cost means that the model fits the training data well. However, at some point, after some epoch e, the model can start overfitting: the cost keeps decreasing, but the performance of the model on the validation data deteriorates. If you keep, in a file, the version of the model after each epoch, you can stop the training once you start observing a decreased performance on the validation set. Alternatively, you can keep running the training process for a fixed number of epochs and then, in the end, you pick the best model. Models saved after each epoch are called checkpoints. Some machine learning practitioners rely on this technique very often; others try to properly regularize the model to avoid such undesirable behavior.

**Batch normalization** (which rather has to be called batch standardization) is a technique that consists of standardizing the outputs of each layer before the units of the subsequent layer receive them as input. In practice, batch normalization results in a faster and more stable training, as well as in some regularization effect. So it’s always a good idea to try to use batch normalization. In neural network libraries, you can often insert a batch normalization layer between two layers. 

Another regularization technique that can be applied not just to neural networks, but to virtually any learning algorithm, is called **data augmentation**. This technique is often used to regularize models that work with images. Once you have your original labeled training set, you can create a synthetic example from an original example by applying various transformations to the original image: zooming it slightly, rotating, flipping, darkening, and so on. You keep the original label in these synthetic examples. In practice, this often results in increased performance of the model.





### Hyperparameter Tuning



Select parameter with validation curve:

```python
from sklearn.model_selection import validation_curve
param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
train_scores, test_scores = validation_curve(
												estimator=pipe_lr,
												X=X_train,
												y=y_train,
												param_name='logisticregression__C',
												param_range=param_range,
												cv=10)
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
plt.plot(param_range, train_mean,
		color='blue', marker='o',
		markersize=5, label='Training accuracy')
plt.fill_between(param_range, train_mean + train_std,
					train_mean - train_std, alpha=0.15,
					color='blue')
plt.plot(param_range, test_mean,
			color='green', linestyle='--',
			marker='s', markersize=5,
			label='Validation accuracy')
plt.fill_between(param_range,
					test_mean + test_std,
					test_mean - test_std,
					alpha=0.15, color='green')
plt.grid()
plt.xscale('log')
plt.legend(loc='lower right')
plt.xlabel('Parameter C')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1.0])
plt.show()
```

Similar to the learning_curve function, the validation_curve function uses stratified k-fold cross-validation by default to estimate the performance of the classifier. Inside the validation_curve function, we specified the parameter that we wanted to evaluate. In this case, it is C, the inverse regularization parameter of the LogisticRegression classifier, which we wrote as 'logisticregression__C' to access the LogisticRegression object inside the scikit-learn pipeline for a specified value range that we set via the param_range parameter.

**Tuning hyperparameters via grid search**

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
pipe_svc = make_pipeline(StandardScaler(),
						SVC(random_state=1))
						
param_range = [0.0001, 0.001, 0.01, 0.1,
						1.0, 10.0, 100.0, 1000.0]
						
param_grid = [{'svc__C': param_range,
						'svc__kernel': ['linear']},
						{'svc__C': param_range,
						'svc__gamma': param_range,
						'svc__kernel': ['rbf']}]
						
gs = GridSearchCV(estimator=pipe_svc,
						param_grid=param_grid,
						scoring='accuracy',
						cv=10,
						refit=True,
						n_jobs=-1)
```



### Bagging

The simple version of ensembles is as follows: 

1. Develop a predictive model and record the predictions for a given data set. 
2. Repeat for multiple models, on the same data. 
3. For each record to be predicted, take an average (or a weighted average, or a majority vote) of the predictions.

Bagging, which stands for “bootstrap aggregating,” was introduced by Leo Breiman in 1994. Suppose we have a response Y and P predictor variables X = X1, X2, ..., Xp with n records. Bagging is like the basic algorithm for ensembles, except that, instead of fitting the various models to the same data, each new model is fit to a bootstrap resample. Here is the algorithm presented more formally: 

1. Initialize M, the number of models to be fit, and n, the number of records to choose (n < N). Set the iteration m = 1. 
2. Take a bootstrap resample (i.e., with replacement) of n records from the training data to form a subsample Ym and Xm (the bag). 
3. Train a model using Ym and Xm to create a set of decision rules f. 
4. Increment the model counter m = m + 1. If m <= M, go to step 1. 

In the case where f predicts the probability Y = 1, the bagged estimate is given by:
$$
\hat f = \frac{1}{M}(\hat f_1(X) + \hat f_2(X) + ... + \hat f_M(X))
$$

### Boosting

The basic idea behind the various boosting algorithms is essentially the same. The easiest to understand is Adaboost, which proceeds as follows: 

1. Initialize M, the maximum number of models to be fit, and set the iteration counter m = 1. Initialize the observation weights w_i = 1/N for i = 1, 2, 3 ... N. Initialize the ensemble model \hat F_0 = 0. 

2. Train a model f_m using the observation weights w1, w2, w3, ..., wn that minimizes the weighted error e defined by summing the weights for the misclassified observations. 

3. Add the model to the ensemble:
   $$
   \hat F_m = \hat F_{m-1} + \alpha_m f_m \\ where\\
   \alpha_m = \frac{log1-e_m}{e_m}
   $$

4. Update the weights w1, w2, ..., wn, so that the weights are increased for the observations that were misclassified. The size of the increase depends on alpha_m with larger values of alpha_m leading to bigger weights. 

5. Increment the model counter m = m + 1. If m <= M, go to step 1.

The boosted estimate is given by:
$$
\hat F =  \alpha_1 \hat f_1 + \alpha_2 \hat f_2 + ... + \alpha_M \hat f_M
$$



In contrast to bagging, the initial formulation of the boosting algorithm uses random subsets of training examples drawn from the training dataset without replacement; the original boosting procedure can be summarized in the following fou2r key steps: 

1. Draw a random subset (sample) of training examples, 𝑑𝑑1, without replacement from the training dataset, D, to train a weak learner, 𝐶𝐶1. 
2. Draw a second random training subset, 𝑑𝑑2, without replacement from the training dataset and add 50 percent of the examples that were previously misclassified to train a weak learner, 𝐶𝐶2. 
3. Find the training examples, 𝑑𝑑3, in the training dataset, D, which 𝐶𝐶1 and 𝐶𝐶2 disagree upon, to train a third weak learner, 𝐶𝐶3. 
4. Combine the weak learners 𝐶𝐶1, 𝐶𝐶2, and 𝐶𝐶3 via majority voting.

Now that we have a better understanding of the basic concept of AdaBoost, let's take a more detailed look at the algorithm using pseudo code. For clarity, we will denote element-wise multiplication by the cross symbol (×) and the dot-product between two vectors by a dot symbol (∙):

1. Set the weight vector, **w**, to uniform weights, where sum of weights = 1

2. For j in m boosting rounds do following:

   * Train a weighted weak learner:  C_j = train(X, y, w)

   * Predict class labels: y^hat = predict(C_j, X)

   * Compute weighted error rate: 
     $$
     \epsilon = w * (\hat y != y)
     $$

   * Compute coefficient:
     $$
     \alpha_j = 0.5 log(\frac{1 - \epsilon}{\epsilon})
     $$

   * Update weights:
     $$
     w := w×exp(-\alpha_j × \hat y × y)
     $$

   * Normalize weights to sum to 1:
     $$
     w := \frac{w}{\sum_i w_i}
     $$

3. Compute the final prediction:
   $$
   \hat y = (\sum_{j=1}^{m}(\alpha_j × predict(C_j, X)) > 0)
   $$

Note that the expression (𝒚̂ ≠ 𝒚) in step 2c refers to a binary vector consisting of 1s and 0s, where a 1 is assigned if the prediction is incorrect and 0 is assigned otherwise.

### Combining models

There are three typical ways to combine models:

1. **Averaging** works for regression as well as those classification models that return classification scores. You simply apply all your models, let’s call them base models, to the input x and then average the predictions. To see if the averaged model works better than each individual algorithm, you test it on the validation set using a metric of your choice. 

2. **Blending** - all models predictions have weights, then averaging predictions * weights

3. **Majority vote** works for classification models. You apply all your base models to the input x and then return the majority class among all predictions. In the case of a tie, you either randomly pick one of the classes, or, you return an error message (if the fact of misclassifying would incur a significant cost). 

4. **Stacking** consists of building a meta-model that takes the output of base models as input. Let’s say you want to combine classifiers f1 and f2, both predicting the same set of classes. To create a training example (xˆi, yˆi) for the stacked model, set xˆi = [f1(x), f2(x)] and yˆi = yi.








