

[**Neural Networks**](#neural-networks)

* [MLP units choice](#MLP units choice)
* [Implementing a Multilayer Artificial Neural Network from Scratch](#Implementing a Multilayer Artificial Neural Network from Scratch)
* [Activation Functions](#Activation Functions)
* [Exploding gradient and vanishing gradient](#Exploding gradient and vanishing gradient)
* [Transformer](#Transformer)
* [Generative Adversarial Networks for Synthesizing New Data](#Generative Adversarial Networks for Synthesizing New Data)
* [Temporal difference learning](#Temporal difference learning)



[**Convolutional Neural Networks**](#**Convolutional Neural Networks**)

* [Discrete convolutions in one dimension](#Discrete convolutions in one dimension)

* [Determining the size of the convolution output](#Determining the size of the convolution output)

* [Performing a discrete convolution in 2D](#Performing a discrete convolution in 2D)

* [Transposed convolution](#Transposed-convolution)

* [Subsampling layers](#Subsampling layers)

* [Implementing CNN](#Implementing CNN)






[**Recurrent Neural Networks**](#Recurrent Neural Networks)

* [Different architectures of RNN](#Different architectures of RNN)
* [Backpropagation through time](#Backpropagation through time)
* [Hidden-recurrence versus output-recurrence](#Hidden-recurrence versus output-recurrence)
* [LSTM](#LSTM)





## Neural Networks



### MLP units choice



#### Output units

The choice of cost function is tightly coupled with the choice of output unit. Most of the time, we simply use the cross-entropy between the data distribution and the model distribution. The choice of how to represent the output then determines the form of the cross-entropy function.

**Linear unit** -  if linear output layers used to produce the mean of conditional Gaussian distribution - minimizing the log-likelihood is then equivalent to maximizing the mean squared error.

**Sigmoid unit** (binary classification) - the loss for maximum likelihood learning of Bernoulli parametrized by a sigmoid is:
$$
J(\theta) = -logP(y|\bold x) = -log(\sigma((2y-1)z)) = \gamma ((1-2y)z)
$$
**Softmax unit** (n-classes classification) 

#### Hidden units

**Rectified Linear Units** (RELU) - best default choice. *g(z) = max{0, z}*. One drawback to relu is that they cannot learn via gradient based methods on examples for which their activation is zero.

Three generalization of relu are based on using non-zero slope \alpha_i when z_i < 0: h_i = g(z, \alpha) = max(0, z_i) + \alpha min(0, z_i)

**Absolute RELU** fixes \alpha = -1 to obtain g(z) = |z| - it is used for ibject recognition from images, where it makes sense to seek features that are invariant under a polarity reversal of the input illumination.

**Leaky RELU** - fixes \alpha to a small value like 0.01 

**Parametric RELU** - treat \alpha as a learnable parameter



### Implementing a Multilayer Artificial Neural Network from Scratch



Let's summarize the MLP learning procedure in three simple steps: 

1. Starting at the input layer, we forward propagate the patterns of the training data through the network to generate an output. 
   $$
   z_1^{(h)} = a_0^{(in)}w_{0, 1}^{(in)} + a_1^{(in)}w_{1, 1}^{(in)} + ... + a_m^{(in)}w_{m, 1}^{(in)}
   \\
   a_1^{(h)} = \phi(z_1^{(h)})
   $$
   Here, 𝑧𝑧1 (ℎ) is the net input and 𝜙𝜙(∙) is the activation function, which has to be differentiable to learn the weights that connect the neurons using a gradient-based approach. For example:
   $$
   \phi(z) = \frac{1}{1+e^{-z}}
   $$
   For purposes of code efficiency and readability, we will now write the activation in a more compact form using the concepts of basic linear algebra:
   $$
   \bold Z ^{(h)} = \bold A^{(in)} \bold W^{(h)}
   \\
   \bold A ^ {(h)} = \phi(\bold Z^{(h)})
   \\
   \bold Z ^{(out)} = \bold A^{(h)} \bold W^{(out)}
   \\
   \bold A ^ {(out)} = \phi(\bold Z^{(out)})
   $$
   Here, 𝒂^(𝑖n) is our 1 × 𝑚 dimensional feature vector of a sample 𝒙^(𝑖n) plus a bias unit. 𝑾^(ℎ) is an 𝑚 × 𝑑 dimensional weight matrix where d is the number of units in the hidden layer.

   

2. Based on the network's output, we calculate the error that we want to minimize using a cost function that we will describe later. 

3. We backpropagate the error, find its derivative with respect to each weight in the network, and update the model. In backpropagation, we propagate the error from right to left. We start by calculating the error vector of the output layer: (* - element-wise multiplication)
   $$
   \bold \delta^{(out)} = \bold a ^{(out)} - \bold y
   \\
   \delta ^{(h)} = \delta ^{(out)} (\bold W ^{(out)})^T * \frac{\delta\phi(z^{(h)})}{\delta z^{(h)}}
   \\
   \frac{\delta\phi(z^{(h)})}{\delta z^{(h)}} = (a^{(h)} * (1-a^{(h)}))
   $$
   Eventually, after obtaining the 𝛿𝛿 terms, we can now write the derivation of the cost function as follows:
   $$
   \frac{\delta}{\delta w_{i,j}^{(out)}} J(\bold W) = a_j^{(h)}\delta_i^{(out)}
   \\
   \frac{\delta}{\delta w_{i,j}^{(h)}} J(\bold W) = a_j^{(in)}\delta_i^{(h)}
   $$
   Next, we need to accumulate the partial derivative of every node in each layer and the error of the node in the next layer. However, remember that we need to compute ∆𝑖𝑖,𝑗𝑗 (𝑙𝑙) for every sample in the training dataset. Thus, it is easier to implement it as a vectorized version like in our NeuralNetMLP code implementation:
   $$
   ∆^{(h)} = (\bold A^{(in)})^T \delta^{(h)}
   \\
   ∆^{(out)} = (\bold A^{(h)})^T \delta^{(out)}
   $$
   And after we have accumulated the partial derivatives, we can add the following regularization term: (Please note that the bias units are usually not regularized.)
   $$
   ∆ ^{(l)} = ∆ ^{(l)} + \lambda^{(l)}\bold W^{(l)}
   \\
   \bold W^{(l)} = \bold W^{(l)} - \eta∆ ^{(l)}
   $$
   ![](C:/Users/sqrte/python-playground/profile/img/backpropagation.png)

   **Forward propagation algorithm:**

   ```pseudocode
   for i = 1, 2, ..., n_i do
   	u[i] = x[i]
   end for
   
   for i = n_i + 1, ..., n do
   	A[i] = {u[j]|j ∈ Pa(u[i])}
   	u[i] = f(A[i])
   end for
   
   return u[n]
   ```

   * u[n] - scalar is the quantity whose gradient we want to obtain, with respect to the n_i input nodes u[1] to u[n_i].
   * A[i] - is the set of all nodes that are parents of u[i]

   **Simplified version of the backprop algorithm:**

   1. Run forward propagation to obtain the activations of the network
   2. Initialize grad_table, a data structure that will store the derivatives that have been computed.

   ```pseudocode
   grad_table[u[n]] = 1
   for j = n-1 down to 1 do
   	The next line computes du[n]/du[j] = \sum_{i:j∈Pa(u[i])} du[n]/du[i] * du[i]/du[j] using stored 		values:
   	grad_table[u[j]] = \sum_{i:j∈Pa(u[i])} grad_table[u[i]] * du[i]/du[j]
   end for
   return {grad_table[u[i]] | i = 1, ..., n}
   ```

   **General backprop algorithm:**

   **Require:** T, the target set of variables whose gradient must be computed.

   **Require:** G, the computational graph

   **Require:** z, the variable to be differentiated

   Let G' be G pruned to contain only nodes that ancestors of z and descendants of nodes in T.

   Initialize grad_table, a data structure associating tensors to their gradients

   ```pseudocode
   grad_table[z] = 1
   for V in T do
   	build_grad(V, G, G`, grad_table)
   end for
   return grad_table
   ```

    **build_grad function:**

   **Require:** V, the variable whose gradient should be added to G and grad_table. 

   **Require:** G, the graph to modify. 

   **Require:** G 0, the restriction of G to nodes that participate in the gradient. 

   **Require:** grad_table, a data structure mapping nodes to their gradients

   ```pseudocode
   if V is in grad_table then
   	return grad_table[V]
   end if
   
   i = 1
   for C in get_consumers(V, G`) do
   	op = get_operation(C)
   	D = build_grad(C, G, G`, grad_table)
   	g[i] = op.bprop(get_inputs(C, G`), V, D)
   	i = i + 1
   end for
   
   g = sum(g[i])
   grad_table[V] = g[i]
   insert g and the operations created it into G
   return G
   ```

   * get_operation - This returns the operation that computes V, represented by the edges coming into V in the computational graph.
   * get_consumers - This returns the list of variables that are children of V in the computational graph G.
   * get_inputs - This returns the list of variables that are parents of V in the computational graph G.

   

   

   **Code for this implementation in python-playground/mini_projects/ml_models_implementations/neural_network.py**

   

   

### Activation functions

![](C:/Users/sqrte/python-playground/profile/img/activation_func.png)



### Exploding gradient and vanishing gradient

Deep learning refers to training neural networks with more than two non-output layers. In the past, it became more difficult to train such networks as the number of layers grew. The two biggest challenges were referred to as the problems of exploding gradient and vanishing gradient as gradient descent was used to train the network parameters. 

While the problem of exploding gradient was easier to deal with by applying simple techniques like gradient clipping and L1 or L2 regularization, the problem of vanishing gradient remained intractable for decades.

During gradient descent, the neural network’s parameters receive an update proportional to the partial derivative of the cost function with respect to the current parameter in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing some parameters from changing their value. In the worst case, this may completely stop the neural network from further training.

However, the modern implementations of neural network learning algorithms allow you to eectively train very deep neural networks (up to hundreds of layers). This is due to several improvements combined together, including ReLU, LSTM (and other gated units; we consider them below), as well as techniques such as skip connections used in residual neural networks, as well as advanced modifications of the gradient descent algorithm



### Transformer



**Self-attention**

More formally, the output of self-attention is the weighted sum of all input sequences. For instance, for the ith input element, the corresponding output value is computed as follows:
$$
\bold o ^{(i)} = \sum_{j=0}^T\bold W_{ij}\bold x^{(j)}
$$
Here, the weights, 𝑊𝑖j, are computed based on the similarity between the current input element, 𝒙𝒙(𝑖𝑖) , and all other elements in the input sequence. More concretely, this similarity is computed as the dot product between the current input element, 𝒙^(𝑖) , and another element in the input sequence, 𝒙^(𝑗) :
$$
\omega_{ij} = \bold x^{(i)^T}\bold x^{(j)}
$$
After computing these similarity-based weights for the ith input and all inputs in the sequence (𝒙^(𝑖) to 𝒙^(𝑇) ), the "raw" weights (𝜔i0 to 𝜔𝑖T) are then normalized using the familiar softmax function, as follows:
$$
W_{ij} = \frac{exp(\omega_{ij})}{\sum_{j=0}^T exp(\omega_{ij})} = softmax([\omega_{ij}]_{j=0...T})
$$
To recap, let's summarize the three main steps behind the self-attention operation: 

1. For a given input element, 𝒙^(𝑖) , and each jth element in the range [0, T], compute the dot product, 𝒙^(𝑖)^⊤ 𝒙(𝑗) 

2. Obtain the weight, 𝑊𝑖j, by normalizing the dot products using the softmax function 

3. Compute the output, 𝒐(𝑖) , as the weighted sum over the entire input sequence:
   $$
   \bold o ^{(i)} = \sum_{j=0}^T W_{ij}\bold x^{j}
   $$

These steps are further illustrated in the following figure:

![](C:/Users/sqrte/python-playground/profile/img/self-attention.png)

 To make the self-attention mechanism more flexible and amenable to model optimization, we will introduce three additional weight matrices that can be fit as model parameters during model training. We denote these three weight matrices as 𝑼𝑼𝑞𝑞, 𝑼𝑼𝑘𝑘, and 𝑼𝑼𝑣𝑣. They are used to project the inputs into query, key, and value sequence elements:

* query sequence: q^(i) = U_q * x^(i) for i in [0, ..., T]
* key sequence: k^(i) = U_k * x^(i) for i in [0, ... ,T]
* value sequence v^(i) = U_v * x^(i) for i in [0, ..., T]



Another trick that greatly improves the discriminatory power of the self-attention mechanism is multi-head attention (MHA), which combines multiple self-attention operations together.

![](C:/Users/sqrte/python-playground/profile/img/transformer_block.png)

First, the input sequence is passed to the MHA layers, which is based on the self-attention mechanism that we discussed earlier. In addition, the input sequences are added to the output of the MHA layers via the residual connections—this ensures that the earlier layers will receive sufficient gradient signals during training, which is a common trick that is used to improve training speed and convergence.

After the input sequences are added to the output of the MHA layers, the outputs are normalized via layer normalization. These normalized signals then go through a series of MLP (that is, fully connected) layers, which also have a residual connection. Finally, the output from the residual block is normalized again and returned as the output sequence, which can be used for sequence classification or sequence generation.





## Generative Adversarial Networks for Synthesizing New Data



### Starting with autoencoders



Autoencoders can be used as a dimensionality reduction technique as well. In fact, when there is no nonlinearity in either of the two subnetworks (encoder and decoder), then the autoencoder approach is almost identical to PCA. 



### Generative models for synthesizing new data



Autoencoders are deterministic models, which means that after an autoencoder is trained, given an input, x, it will be able to reconstruct the input from its compressed version in a lower-dimensional space. Therefore, it cannot generate new data beyond reconstructing its input through the transformation of the compressed representation. A generative model, on the other hand, can generate a new example, 𝒙𝒙̃, from a random vector, z (corresponding to the latent representation).

However, the major difference between the two is that we do not know the distribution of z in the autoencoder, while in a generative model, the distribution of z is fully characterizable. It is possible to generalize an autoencoder into a generative model, though. One approach is VAEs.

In a VAE receiving an input example, x, the encoder network is modified in such a way that it computes two moments of the distribution of the latent vector: the mean, 𝝁𝝁, and variance, 𝝈𝝈2. During the training of a VAE, the network is forced to match these moments with those of a standard normal distribution (that is, zero mean and unit variance). Then, after the VAE model is trained, the encoder is discarded, and we can use the decoder network to generate new examples, 𝒙𝒙̃, by feeding random z vectors from the "learned" Gaussian distribution.



The objective function of GANs:
$$
V(\theta^{(D)}, \theta^{(G)}) = E_{\bold x ..p_{data}(x)}[log(D(x))] + E_{z..p_z(z)}[log(1 - D(G(z)))]
$$


One training step of a GAN model with such a value function requires two optimization steps: (1) maximizing the payoff for the discriminator and (2) minimizing the payoff for the generator. 

![](C:/Users/sqrte/python-playground/profile/img/gan.png)



### Batch Normalization

One of the main ideas behind BatchNorm is normalizing the layer inputs and preventing changes in their distribution during training, which enables faster and better convergence.

Assume that we have the net preactivation feature maps obtained after a convolutional layer in a four-dimensional tensor, Z, with the shape [𝑚𝑚 × ℎ × 𝑤𝑤 × 𝑐𝑐] , where m is the number of examples in the batch (i.e., batch size), ℎ × 𝑤𝑤 is the spatial dimension of the feature maps, and c is the number of channels. BatchNorm can be summarized in three steps, as follows:

1. Compute the mean and standard deviation of the net inputs for each mini-batch:
   $$
   \mu_B = \frac{1}{m * h*w}\sum_{i,j,k}Z^{[i,j,k,]}
   \\
   \sigma_B^2 = \frac{1}{m * h*w}\sum_{i,j,k}(Z^{[i,j,k,]} - \mu_B)^2
   $$

2. Standardized the net inputs for all example in the batch

3. Scale and shifts the normalized net inputs using two learnable parameter vector, \gamma and \beta, of size c (number of channels):
   $$
   \bold A^{[i]}_{pre} = \gamma \bold Z_{std} ^{[i]} + \beta
   $$






### Temporal difference learning



In TD learning, we can leverage some of the learned properties to update the estimated values before reaching the end of the episode. 

Let's first revisit the value prediction by MC. At the end of each episode, we are able to estimate the return 𝐺𝐺𝑡𝑡 for each time step t. Therefore, we can update our estimates for the visited states as follows:
$$
V(S_t) = V(S_t) + \alpha(G_t - V(S_t))
$$









## Convolutional Neural Networks



Certain types of NNs, such as CNNs, are able to automatically learn the features from raw data that are most useful for a particular task. For this reason, it's common to consider CNN layers as feature extractors: the early layers (those right after the input layer) extract low-level features from raw data, and the later layers (often fully connected layers like in a multilayer perceptron (MLP)) use these features to predict a continuous target value or class label.

#### **Discrete convolutions in one dimension**

A discrete convolution for two vectors, x and w, is denoted by 𝒚𝒚 = 𝒙𝒙 ∗ 𝒘𝒘, in which vector x is our input (sometimes called signal) and w is called the filter or kernel. A discrete convolution is mathematically defined as follows:
$$
y = x*w => y[i] = \sum_{k=-\inf}^{+\inf}x[i-k]w[k]
$$
The sum from -inf to +inf solves with padding:

![](C:/Users/sqrte/python-playground/profile/img/padding.png)

Let's assume that the original input, x, and filter, w, have n and m elements, respectively, where 𝑚 ≤ 𝑛. Therefore, the padded vector, 𝒙^𝑝, has size n + 2p. The practical formula for computing a discrete convolution will change to the following:
$$
y = x*w =>y[i] = \sum_{k=0}^{k = m-1} x^p[i+m-k]w[k]
$$
Now that we have solved the infinite index issue, the second issue is indexing x with i + m – k. The important point to notice here is that x and w are indexed in different directions in this summation. Computing the sum with one index going in the reverse direction is equivalent to computing the sum with both indices in the forward direction after flipping one of those vectors, x or w, after they are padded. 

![](C:/Users/sqrte/python-playground/profile/img/1d_conv.png)

There are three modes of padding that are commonly used in practice: full, same, and valid: 

* In full mode, the padding parameter, p, is set to p = m – 1. Full padding increases the dimensions of the output; thus, it is rarely used in CNN architectures. 
* Same padding is usually used to ensure that the output vector has the same size as the input vector, x. In this case, the padding parameter, p, is computed according to the filter size, along with the requirement that the input size and output size are the same. 
* Finally, computing a convolution in the valid mode refers to the case where p = 0 (no padding).

In practice, it is recommended that you preserve the spatial size using same padding for the convolutional layers and decrease the spatial size via pooling layers instead.


#### Performing a discrete convolution in 2D

![](C:/Users/sqrte/python-playground/profile/img/2d_conv.png)



```python
import numpy as np
import scipy.signal

def conv2d(X, W, p=(0,0), s=(1,1)):
  W_rot = np.array(W)[::-1, ::-1]
  X_orig = np.array(X)
  n1 = X_orig.shape[0] + 2*p[0]
  n2 = X_orig.shape[0] + 2*p[1]
  X_padded = np.zeros(shape=(n1, n2))
  X_padded[p[0]:p[0]+X_orig.shape[0],
           p[1]:p[1]+X_orig.shape[1]] = X_orig
  res = []
  for i in range(0, int((X_padded.shape[0] - \
                         W_rot.shape[0])/s[0])+1, s[0]):
    res.append([])
    for j in range(0, int((X_padded.shape[1] - \
                           W_rot.shape[1])/s[1])+1, s[1]):
      X_sub = X_padded[i:i+W_rot.shape[0], j:j+W_rot.shape[1]]
      res[-1].append(np.sum(X_sub*W_rot))
  return np.array(res)
```

#### Determining the size of the convolution output

The output size of a convolution is determined by the total number of times that we shift the filter, w, along the input vector. Let's assume that the input vector is of size n and the filter is of size m. Then, the size of the output resulting from 𝒚𝒚 = 𝒙𝒙 ∗ 𝒘𝒘, with padding, p, and stride, s, would be determined as follows:
$$
o = floor(\frac{n + 2p - m}{s}) + 1
$$



### Transposed convolution

 While a convolution operation is usually used to downsample the feature space, a transposed convolution operation is usually used for upsampling the feature space. Upsampling feature maps using transposed convolution works by inserting 0s between the elements of the input feature maps.

![](C:/Users/sqrte/python-playground/profile/img/transpose_conv.png)



#### Subsampling layers

Subsampling is typically applied in two forms of pooling operations in CNNs: max-pooling and mean-pooling (also known as average-pooling).

![](C:/Users/sqrte/python-playground/profile/img/pooling.png)

Traditionally, pooling is assumed to be nonoverlapping. Pooling is typically performed on nonoverlapping neighborhoods, which can be done by setting the stride parameter equal to the pooling size.



#### Implementing CNN

![](C:/Users/sqrte/python-playground/profile/img/img_conv.png)





## Recurrent Neural Networks

Recurrent neural networks (RNNs) are used to label, classify, or generate sequences.



Each training example is a matrix in which each row is a feature vector. For simplicity, let’s illustrate this matrix as a sequence of vectors X = [x1, x2,..., xt≠1, xt , xt+1,..., xlengthX ], where lengthX is the length of the input sequence. If our input example X is a text sentence, then feature vector xt for each t = 1, . . . , lengthX represents a word in the sentence at position t. As depicted in Figure 8, in an RNN, the feature vectors from an input example are “read” by the neural network sequentially in the order of the timesteps. The index t denotes a timestep. To update the state ht l,u at each timestep t in each unit u of each layer l we first calculate a linear combination of the input feature vector with the state vector ht≠1 l,u of this same layer from the previous timestep, t ≠ 1. The linear combination of two vectors is calculated using two parameter vectors wl,u, ul,u and a parameter bl,u. The value of ht l,u is then obtained by applying activation function g1 to the result of the linear combination. A typical choice for function g1 is tanh. The output yt l is typically a vector calculated for the whole layer l at once. To obtain yt l, we use activation function g2 that takes a vector as input and returns a dierent vector of the same dimensionality. The function g2 is applied to a linear combination of the state vector values ht l,u calculated using a parameter matrix Vl and a parameter vector cl,u. 

#### **Different architectures of RNN**

![](C:/Users/sqrte/python-playground/profile/img/rnn_architectures.png)

* Many-to-one: The input data is a sequence, but the output is a fixed-size vector or scalar, not a sequence. For example, in sentiment analysis, the input is text-based (for example, a movie review) and the output is a class label (for example, a label denoting whether a reviewer liked the movie). 
* One-to-many: The input data is in standard format and not a sequence, but the output is a sequence. An example of this category is image captioning— the input is an image and the output is an English phrase summarizing the content of that image
* Many-to-many: Both the input and output arrays are sequences. This category can be further divided based on whether the input and output are synchronized. An example of a synchronized many-to-many modeling task is video classification, where each frame in a video is labeled. An example of a delayed many-to-many modeling task would be translating one language into another. For instance, an entire English sentence must be read and processed by a machine before its translation into German is produced.

![](C:/Users/sqrte/python-playground/profile/img/rnn_weights.png)

Each directed edge (the connections between boxes) in the representation of an RNN that we just looked at is associated with a weight matrix. Those weights do not depend on time, t; therefore, they are shared across the time axis. The different weight matrices in a single-layer RNN are as follows:

* 𝑾𝑥ℎ: The weight matrix between the input, 𝒙(𝑡) , and the hidden layer, h 
* 𝑾ℎℎ: The weight matrix associated with the recurrent edge 
* 𝑾ℎ𝑜: The weight matrix between the hidden layer and output layer

Computing the activations:
$$
\bold z_h^{(t)} = \bold W _{xh} \bold x^{(t)} + \bold W_{hh} \bold h ^{t-1} + \bold b _h
\\
\bold h^{(t)} = \phi_h(\bold z_h^{(t)})
$$

#### **Backpropagation through time**

The derivation of the gradients might be a bit complicated, but the basic idea is that the overall loss, L, is the sum of all the loss functions at times t = 1 to t = T. Since the loss at time t is dependent on the hidden units at all previous time steps 1 : t, the gradient will be computed as follows:
$$
L = \sum_{t=1}^T L^{(t)}
\\
\frac{dL^{(t)}}{d\bold W_{hh}} = \frac{dL^{(t)}}{d\bold o^{(t)}} \frac{d\bold o^{(t)}}{d\bold h^{(t)}}(\sum_{k=1}^t \frac {d\bold h^{(t)}}{d\bold h^{(k)}} \frac{d\bold h^{(k)}}{d\bold W_{hh}})
\\
\frac {d\bold h^{(t)}}{d\bold h^{(k)}} = \prod_{i=k+1}^{t} \frac {d\bold h^{(i)}}{d\bold h^{(i-1)}}
$$

#### **Hidden-recurrence versus output-recurrence**

![](C:/Users/sqrte/python-playground/profile/img/output_rec_prop.png)

#### LSTM

The most effective recurrent neural network models used in practice are gated RNNs. These include the long short-term memory (LSTM) networks and networks based on the gated recurrent unit (GRU). The beauty of using gated units in RNNs is that such networks can store information in their units for future use, much like bits in a computer’s memory

![](C:/Users/sqrte/python-playground/profile/img/LSTM.png)

![](C:/Users/sqrte/python-playground/profile/img/text.png)

In an LSTM cell, there are three different types of gates, which are known as the forget gate, the input gate, and the output gate: 

* The forget gate ( 𝒇𝑡) allows the memory cell to reset the cell state without growing indefinitely. In fact, the forget gate decides which information is allowed to go through and which information to suppress. Now, 𝒇𝑡 is computed as follows:
  $$
  \bold f_t = \sigma(\bold W_{xf} \bold x^{(t)} + \bold W_{hf} \bold h ^{(t-1)} + \bold b_f)
  $$

* The input gate (𝒊𝑡) and candidate value {\hat C_t} are responsible for updating the cell state. They are computed as follows:
  $$
  \bold i _t = \sigma (\bold W _{xi} \bold x^{(t)} + \bold W _{hi}\bold h ^{(t-1)} + \bold b_i)
  \\
  \bold{\hat C_t} = tanh(\bold W_{xc}\bold x^{(t)} + \bold W_{hc} \bold h^{(t-1)} +\bold b_c)
  $$
  The cell state at time t is computed as follows:
  $$
  C^{(t)} = (C^{(t-1)} ⊙f_t) ⨁ (i_t ⊙{\hat  C_t})
  $$

* The output gate (𝒐𝑡) decides how to update the values of hidden units:
  $$
  \bold o_t = \sigma(\bold W_{xo} \bold x^{(t)} + \bold W_{ho} \bold h ^{(t-1)} + \bold b_o)
  $$

Given this, the hidden units at the current time step are computed as follows: 
$$
\bold h_t = \bold o_t ⊙ tanh(\bold C ^{(t)})
$$


```python
# import necessary libraries
import numpy as np
import tensorflow as tf
from google.colab import drive
import os
drive.mount('drive')
os.chdir('drive/My Drive/vern/')


#read file
with open('1268-0.txt', 'r') as tf:
  text = tf.read()

#crop useful text
start_index = text.find('THE MYSTERIOUS ISLAND')
end_index = text.find('End of the Project Gutenberg')
text = text[start_index:end_index]
#find unique characters in text
char_set = set(text)
#create dict to convert characters to integers
char_sorted = sorted(char_set)
chr2int = {ch:i for i, ch in enumerate(char_sorted)}
char_array = np.array(char_sorted)
int2chr = {i:ch for i, ch in enumerate(char_sorted)}
#encode data, create tensor
text_encoded = np.array([chr2int[ch] for ch in text], dtype=np.int32)
ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)
#create x (input) and y(target) sequences
seq_length = 40
chunk_size = seq_length + 1
ds_chunks = ds_text_encoded.batch(chunk_size, drop_remainder=True)
def split_input_target(chunk):
  input_seq = chunk[:-1]
  target_seq = chunk[1:]
  return input_seq, target_seq
ds_sequences = ds_chunks.map(split_input_target)

#create batches
BATCH_SIZE = 64
BUFFER_SIZE = 10000
ds = ds_sequences.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

#build model
def build_model(vocab_size, embedding_dim,rnn_units):
  model = tf.keras.models.Sequential([
                                      tf.keras.layers.Embedding(vocab_size, embedding_dim),
                                      tf.keras.layers.LSTM(rnn_units,
                                                           return_sequences=True),
                                      tf.keras.layers.Dense(vocab_size)
  ])
  return model

charset_size = len(char_array)
embedding_dim = 256
rnn_units = 512

tf.random.set_seed(1)
#compile and train
model = build_model(charset_size, embedding_dim, rnn_units)
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))
model.fit(ds, epochs=20)
```

Other important extensions to RNNs include bi-directional RNNs, RNNs with attention and sequence-to-sequence RNN models. The latter, in particular, are frequently used to build neural machine translation models and other models for text to text transformations. A generalization of an RNN is a recursive neural network.



