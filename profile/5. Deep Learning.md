

[**Neural Networks**](#neural-networks)

* [MLP units choice](#MLP units choice)
* [Implementing a Multilayer Artificial Neural Network from Scratch](#Implementing a Multilayer Artificial Neural Network from Scratch)
* [Activation Functions](#Activation Functions)
* [Exploding gradient and vanishing gradient](#Exploding gradient and vanishing gradient)
* [Transformer](#Transformer)
* [Generative Adversarial Networks for Synthesizing New Data](#Generative Adversarial Networks for Synthesizing New Data)
* [Temporal difference learning](#Temporal difference learning)



[**Convolutional Neural Networks**](#**Convolutional Neural Networks**)

* [Discrete convolutions in one dimension](#Discrete convolutions in one dimension)

* [Determining the size of the convolution output](#Determining the size of the convolution output)

* [Performing a discrete convolution in 2D](#Performing a discrete convolution in 2D)

* [Transposed convolution](#Transposed-convolution)

* [Subsampling layers](#Subsampling layers)

* [Implementing CNN](#Implementing CNN)






[**Recurrent Neural Networks**](#Recurrent Neural Networks)

* [Different architectures of RNN](#Different architectures of RNN)
* [Backpropagation through time](#Backpropagation through time)
* [Hidden-recurrence versus output-recurrence](#Hidden-recurrence versus output-recurrence)
* [LSTM](#LSTM)
  * [LSTM data flow](#LSTM data flow)

* [Sequence-to-sequence models and attention](#Sequence-to-sequence models and attention)
  * [Encoder-decoder architecture](#Encoder-decoder architecture)
  * [Attention](#Attention)



## Neural Networks



### MLP units choice



#### Output units

The choice of cost function is tightly coupled with the choice of output unit. Most of the time, we simply use the cross-entropy between the data distribution and the model distribution. The choice of how to represent the output then determines the form of the cross-entropy function.

**Linear unit** -  if linear output layers used to produce the mean of conditional Gaussian distribution - minimizing the log-likelihood is then equivalent to maximizing the mean squared error.

**Sigmoid unit** (binary classification) - the loss for maximum likelihood learning of Bernoulli parametrized by a sigmoid is:
$$
J(\theta) = -logP(y|\bold x) = -log(\sigma((2y-1)z)) = \gamma ((1-2y)z)
$$
**Softmax unit** (n-classes classification) 

#### Hidden units

**Rectified Linear Units** (RELU) - best default choice. *g(z) = max{0, z}*. One drawback to relu is that they cannot learn via gradient based methods on examples for which their activation is zero.

Three generalization of relu are based on using non-zero slope \alpha_i when z_i < 0: h_i = g(z, \alpha) = max(0, z_i) + \alpha min(0, z_i)

**Absolute RELU** fixes \alpha = -1 to obtain g(z) = |z| - it is used for ibject recognition from images, where it makes sense to seek features that are invariant under a polarity reversal of the input illumination.

**Leaky RELU** - fixes \alpha to a small value like 0.01 

**Parametric RELU** - treat \alpha as a learnable parameter



### Implementing a Multilayer Artificial Neural Network from Scratch



Let's summarize the MLP learning procedure in three simple steps: 

1. Starting at the input layer, we forward propagate the patterns of the training data through the network to generate an output. 
   $$
   z_1^{(h)} = a_0^{(in)}w_{0, 1}^{(in)} + a_1^{(in)}w_{1, 1}^{(in)} + ... + a_m^{(in)}w_{m, 1}^{(in)}
   \\
   a_1^{(h)} = \phi(z_1^{(h)})
   $$
   Here, ùëßùëß1 (‚Ñé) is the net input and ùúôùúô(‚àô) is the activation function, which has to be differentiable to learn the weights that connect the neurons using a gradient-based approach. For example:
   $$
   \phi(z) = \frac{1}{1+e^{-z}}
   $$
   For purposes of code efficiency and readability, we will now write the activation in a more compact form using the concepts of basic linear algebra:
   $$
   \bold Z ^{(h)} = \bold A^{(in)} \bold W^{(h)}
   \\
   \bold A ^ {(h)} = \phi(\bold Z^{(h)})
   \\
   \bold Z ^{(out)} = \bold A^{(h)} \bold W^{(out)}
   \\
   \bold A ^ {(out)} = \phi(\bold Z^{(out)})
   $$
   Here, ùíÇ^(ùëñn) is our 1 √ó ùëö dimensional feature vector of a sample ùíô^(ùëñn) plus a bias unit. ùëæ^(‚Ñé) is an ùëö √ó ùëë dimensional weight matrix where d is the number of units in the hidden layer.

   

2. Based on the network's output, we calculate the error that we want to minimize using a cost function that we will describe later. 

3. We backpropagate the error, find its derivative with respect to each weight in the network, and update the model. In backpropagation, we propagate the error from right to left. We start by calculating the error vector of the output layer: (* - element-wise multiplication)
   $$
   \bold \delta^{(out)} = \bold a ^{(out)} - \bold y
   \\
   \delta ^{(h)} = \delta ^{(out)} (\bold W ^{(out)})^T * \frac{\delta\phi(z^{(h)})}{\delta z^{(h)}}
   \\
   \frac{\delta\phi(z^{(h)})}{\delta z^{(h)}} = (a^{(h)} * (1-a^{(h)}))
   $$
   Eventually, after obtaining the ùõøùõø terms, we can now write the derivation of the cost function as follows:
   $$
   \frac{\delta}{\delta w_{i,j}^{(out)}} J(\bold W) = a_j^{(h)}\delta_i^{(out)}
   \\
   \frac{\delta}{\delta w_{i,j}^{(h)}} J(\bold W) = a_j^{(in)}\delta_i^{(h)}
   $$
   Next, we need to accumulate the partial derivative of every node in each layer and the error of the node in the next layer. However, remember that we need to compute ‚àÜùëñùëñ,ùëóùëó (ùëôùëô) for every sample in the training dataset. Thus, it is easier to implement it as a vectorized version like in our NeuralNetMLP code implementation:
   $$
   ‚àÜ^{(h)} = (\bold A^{(in)})^T \delta^{(h)}
   \\
   ‚àÜ^{(out)} = (\bold A^{(h)})^T \delta^{(out)}
   $$
   And after we have accumulated the partial derivatives, we can add the following regularization term: (Please note that the bias units are usually not regularized.)
   $$
   ‚àÜ ^{(l)} = ‚àÜ ^{(l)} + \lambda^{(l)}\bold W^{(l)}
   \\
   \bold W^{(l)} = \bold W^{(l)} - \eta‚àÜ ^{(l)}
   $$
   ![](C:/Users/sqrte/python-playground/profile/img/backpropagation.png)

   **Forward propagation algorithm:**

   ```pseudocode
   for i = 1, 2, ..., n_i do
   	u[i] = x[i]
   end for
   
   for i = n_i + 1, ..., n do
   	A[i] = {u[j]|j ‚àà Pa(u[i])}
   	u[i] = f(A[i])
   end for
   
   return u[n]
   ```

   * u[n] - scalar is the quantity whose gradient we want to obtain, with respect to the n_i input nodes u[1] to u[n_i].
   * A[i] - is the set of all nodes that are parents of u[i]

   **Simplified version of the backprop algorithm:**

   1. Run forward propagation to obtain the activations of the network
   2. Initialize grad_table, a data structure that will store the derivatives that have been computed.

   ```pseudocode
   grad_table[u[n]] = 1
   for j = n-1 down to 1 do
   	The next line computes du[n]/du[j] = \sum_{i:j‚ààPa(u[i])} du[n]/du[i] * du[i]/du[j] using stored 		values:
   	grad_table[u[j]] = \sum_{i:j‚ààPa(u[i])} grad_table[u[i]] * du[i]/du[j]
   end for
   return {grad_table[u[i]] | i = 1, ..., n}
   ```

   **General backprop algorithm:**

   **Require:** T, the target set of variables whose gradient must be computed.

   **Require:** G, the computational graph

   **Require:** z, the variable to be differentiated

   Let G' be G pruned to contain only nodes that ancestors of z and descendants of nodes in T.

   Initialize grad_table, a data structure associating tensors to their gradients

   ```pseudocode
   grad_table[z] = 1
   for V in T do
   	build_grad(V, G, G`, grad_table)
   end for
   return grad_table
   ```

    **build_grad function:**

   **Require:** V, the variable whose gradient should be added to G and grad_table. 

   **Require:** G, the graph to modify. 

   **Require:** G 0, the restriction of G to nodes that participate in the gradient. 

   **Require:** grad_table, a data structure mapping nodes to their gradients

   ```pseudocode
   if V is in grad_table then
   	return grad_table[V]
   end if
   
   i = 1
   for C in get_consumers(V, G`) do
   	op = get_operation(C)
   	D = build_grad(C, G, G`, grad_table)
   	g[i] = op.bprop(get_inputs(C, G`), V, D)
   	i = i + 1
   end for
   
   g = sum(g[i])
   grad_table[V] = g[i]
   insert g and the operations created it into G
   return G
   ```

   * get_operation - This returns the operation that computes V, represented by the edges coming into V in the computational graph.
   * get_consumers - This returns the list of variables that are children of V in the computational graph G.
   * get_inputs - This returns the list of variables that are parents of V in the computational graph G.

   

   

   **Code for this implementation in python-playground/mini_projects/ml_models_implementations/neural_network.py**

   

   

### Activation functions

![](C:/Users/sqrte/python-playground/profile/img/activation_func.png)



### Exploding gradient and vanishing gradient

Deep learning refers to training neural networks with more than two non-output layers. In the past, it became more difficult to train such networks as the number of layers grew. The two biggest challenges were referred to as the problems of exploding gradient and vanishing gradient as gradient descent was used to train the network parameters. 

While the problem of exploding gradient was easier to deal with by applying simple techniques like gradient clipping and L1 or L2 regularization, the problem of vanishing gradient remained intractable for decades.

During gradient descent, the neural network‚Äôs parameters receive an update proportional to the partial derivative of the cost function with respect to the current parameter in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing some parameters from changing their value. In the worst case, this may completely stop the neural network from further training.

However, the modern implementations of neural network learning algorithms allow you to eectively train very deep neural networks (up to hundreds of layers). This is due to several improvements combined together, including ReLU, LSTM (and other gated units; we consider them below), as well as techniques such as skip connections used in residual neural networks, as well as advanced modifications of the gradient descent algorithm



### Transformer



**Self-attention**

More formally, the output of self-attention is the weighted sum of all input sequences. For instance, for the ith input element, the corresponding output value is computed as follows:
$$
\bold o ^{(i)} = \sum_{j=0}^T\bold W_{ij}\bold x^{(j)}
$$
Here, the weights, ùëäùëñj, are computed based on the similarity between the current input element, ùíôùíô(ùëñùëñ) , and all other elements in the input sequence. More concretely, this similarity is computed as the dot product between the current input element, ùíô^(ùëñ) , and another element in the input sequence, ùíô^(ùëó) :
$$
\omega_{ij} = \bold x^{(i)^T}\bold x^{(j)}
$$
After computing these similarity-based weights for the ith input and all inputs in the sequence (ùíô^(ùëñ) to ùíô^(ùëá) ), the "raw" weights (ùúîi0 to ùúîùëñT) are then normalized using the familiar softmax function, as follows:
$$
W_{ij} = \frac{exp(\omega_{ij})}{\sum_{j=0}^T exp(\omega_{ij})} = softmax([\omega_{ij}]_{j=0...T})
$$
To recap, let's summarize the three main steps behind the self-attention operation: 

1. For a given input element, ùíô^(ùëñ) , and each jth element in the range [0, T], compute the dot product, ùíô^(ùëñ)^‚ä§ ùíô(ùëó) 

2. Obtain the weight, ùëäùëñj, by normalizing the dot products using the softmax function 

3. Compute the output, ùíê(ùëñ) , as the weighted sum over the entire input sequence:
   $$
   \bold o ^{(i)} = \sum_{j=0}^T W_{ij}\bold x^{j}
   $$

These steps are further illustrated in the following figure:

![](C:/Users/sqrte/python-playground/profile/img/self-attention.png)

 To make the self-attention mechanism more flexible and amenable to model optimization, we will introduce three additional weight matrices that can be fit as model parameters during model training. We denote these three weight matrices as ùëºùëºùëûùëû, ùëºùëºùëòùëò, and ùëºùëºùë£ùë£. They are used to project the inputs into query, key, and value sequence elements:

* query sequence: q^(i) = U_q * x^(i) for i in [0, ..., T]
* key sequence: k^(i) = U_k * x^(i) for i in [0, ... ,T]
* value sequence v^(i) = U_v * x^(i) for i in [0, ..., T]



Another trick that greatly improves the discriminatory power of the self-attention mechanism is multi-head attention (MHA), which combines multiple self-attention operations together.

![](C:/Users/sqrte/python-playground/profile/img/transformer_block.png)

First, the input sequence is passed to the MHA layers, which is based on the self-attention mechanism that we discussed earlier. In addition, the input sequences are added to the output of the MHA layers via the residual connections‚Äîthis ensures that the earlier layers will receive sufficient gradient signals during training, which is a common trick that is used to improve training speed and convergence.

After the input sequences are added to the output of the MHA layers, the outputs are normalized via layer normalization. These normalized signals then go through a series of MLP (that is, fully connected) layers, which also have a residual connection. Finally, the output from the residual block is normalized again and returned as the output sequence, which can be used for sequence classification or sequence generation.





## Generative Adversarial Networks for Synthesizing New Data



### Starting with autoencoders



Autoencoders can be used as a dimensionality reduction technique as well. In fact, when there is no nonlinearity in either of the two subnetworks (encoder and decoder), then the autoencoder approach is almost identical to PCA. 



### Generative models for synthesizing new data



Autoencoders are deterministic models, which means that after an autoencoder is trained, given an input, x, it will be able to reconstruct the input from its compressed version in a lower-dimensional space. Therefore, it cannot generate new data beyond reconstructing its input through the transformation of the compressed representation. A generative model, on the other hand, can generate a new example, ùíôùíôÃÉ, from a random vector, z (corresponding to the latent representation).

However, the major difference between the two is that we do not know the distribution of z in the autoencoder, while in a generative model, the distribution of z is fully characterizable. It is possible to generalize an autoencoder into a generative model, though. One approach is VAEs.

In a VAE receiving an input example, x, the encoder network is modified in such a way that it computes two moments of the distribution of the latent vector: the mean, ùùÅùùÅ, and variance, ùùàùùà2. During the training of a VAE, the network is forced to match these moments with those of a standard normal distribution (that is, zero mean and unit variance). Then, after the VAE model is trained, the encoder is discarded, and we can use the decoder network to generate new examples, ùíôùíôÃÉ, by feeding random z vectors from the "learned" Gaussian distribution.



The objective function of GANs:
$$
V(\theta^{(D)}, \theta^{(G)}) = E_{\bold x ..p_{data}(x)}[log(D(x))] + E_{z..p_z(z)}[log(1 - D(G(z)))]
$$


One training step of a GAN model with such a value function requires two optimization steps: (1) maximizing the payoff for the discriminator and (2) minimizing the payoff for the generator. 

![](C:/Users/sqrte/python-playground/profile/img/gan.png)



### Batch Normalization

One of the main ideas behind BatchNorm is normalizing the layer inputs and preventing changes in their distribution during training, which enables faster and better convergence.

Assume that we have the net preactivation feature maps obtained after a convolutional layer in a four-dimensional tensor, Z, with the shape [ùëöùëö √ó ‚Ñé √ó ùë§ùë§ √ó ùëêùëê] , where m is the number of examples in the batch (i.e., batch size), ‚Ñé √ó ùë§ùë§ is the spatial dimension of the feature maps, and c is the number of channels. BatchNorm can be summarized in three steps, as follows:

1. Compute the mean and standard deviation of the net inputs for each mini-batch:
   $$
   \mu_B = \frac{1}{m * h*w}\sum_{i,j,k}Z^{[i,j,k,]}
   \\
   \sigma_B^2 = \frac{1}{m * h*w}\sum_{i,j,k}(Z^{[i,j,k,]} - \mu_B)^2
   $$

2. Standardized the net inputs for all example in the batch

3. Scale and shifts the normalized net inputs using two learnable parameter vector, \gamma and \beta, of size c (number of channels):
   $$
   \bold A^{[i]}_{pre} = \gamma \bold Z_{std} ^{[i]} + \beta
   $$






### Temporal difference learning



In TD learning, we can leverage some of the learned properties to update the estimated values before reaching the end of the episode. 

Let's first revisit the value prediction by MC. At the end of each episode, we are able to estimate the return ùê∫ùê∫ùë°ùë° for each time step t. Therefore, we can update our estimates for the visited states as follows:
$$
V(S_t) = V(S_t) + \alpha(G_t - V(S_t))
$$









## Convolutional Neural Networks



Certain types of NNs, such as CNNs, are able to automatically learn the features from raw data that are most useful for a particular task. For this reason, it's common to consider CNN layers as feature extractors: the early layers (those right after the input layer) extract low-level features from raw data, and the later layers (often fully connected layers like in a multilayer perceptron (MLP)) use these features to predict a continuous target value or class label.

#### **Discrete convolutions in one dimension**

A discrete convolution for two vectors, x and w, is denoted by ùíöùíö = ùíôùíô ‚àó ùíòùíò, in which vector x is our input (sometimes called signal) and w is called the filter or kernel. A discrete convolution is mathematically defined as follows:
$$
y = x*w => y[i] = \sum_{k=-\inf}^{+\inf}x[i-k]w[k]
$$
The sum from -inf to +inf solves with padding:

![](C:/Users/sqrte/python-playground/profile/img/padding.png)

Let's assume that the original input, x, and filter, w, have n and m elements, respectively, where ùëö ‚â§ ùëõ. Therefore, the padded vector, ùíô^ùëù, has size n + 2p. The practical formula for computing a discrete convolution will change to the following:
$$
y = x*w =>y[i] = \sum_{k=0}^{k = m-1} x^p[i+m-k]w[k]
$$
Now that we have solved the infinite index issue, the second issue is indexing x with i + m ‚Äì k. The important point to notice here is that x and w are indexed in different directions in this summation. Computing the sum with one index going in the reverse direction is equivalent to computing the sum with both indices in the forward direction after flipping one of those vectors, x or w, after they are padded. 

![](C:/Users/sqrte/python-playground/profile/img/1d_conv.png)

There are three modes of padding that are commonly used in practice: full, same, and valid: 

* In full mode, the padding parameter, p, is set to p = m ‚Äì 1. Full padding increases the dimensions of the output; thus, it is rarely used in CNN architectures. 
* Same padding is usually used to ensure that the output vector has the same size as the input vector, x. In this case, the padding parameter, p, is computed according to the filter size, along with the requirement that the input size and output size are the same. 
* Finally, computing a convolution in the valid mode refers to the case where p = 0 (no padding).

In practice, it is recommended that you preserve the spatial size using same padding for the convolutional layers and decrease the spatial size via pooling layers instead.


#### Performing a discrete convolution in 2D

![](C:/Users/sqrte/python-playground/profile/img/2d_conv.png)



```python
import numpy as np
import scipy.signal

def conv2d(X, W, p=(0,0), s=(1,1)):
  W_rot = np.array(W)[::-1, ::-1]
  X_orig = np.array(X)
  n1 = X_orig.shape[0] + 2*p[0]
  n2 = X_orig.shape[0] + 2*p[1]
  X_padded = np.zeros(shape=(n1, n2))
  X_padded[p[0]:p[0]+X_orig.shape[0],
           p[1]:p[1]+X_orig.shape[1]] = X_orig
  res = []
  for i in range(0, int((X_padded.shape[0] - \
                         W_rot.shape[0])/s[0])+1, s[0]):
    res.append([])
    for j in range(0, int((X_padded.shape[1] - \
                           W_rot.shape[1])/s[1])+1, s[1]):
      X_sub = X_padded[i:i+W_rot.shape[0], j:j+W_rot.shape[1]]
      res[-1].append(np.sum(X_sub*W_rot))
  return np.array(res)
```

#### Determining the size of the convolution output

The output size of a convolution is determined by the total number of times that we shift the filter, w, along the input vector. Let's assume that the input vector is of size n and the filter is of size m. Then, the size of the output resulting from ùíöùíö = ùíôùíô ‚àó ùíòùíò, with padding, p, and stride, s, would be determined as follows:
$$
o = floor(\frac{n + 2p - m}{s}) + 1
$$



### Transposed convolution

 While a convolution operation is usually used to downsample the feature space, a transposed convolution operation is usually used for upsampling the feature space. Upsampling feature maps using transposed convolution works by inserting 0s between the elements of the input feature maps.

![](C:/Users/sqrte/python-playground/profile/img/transpose_conv.png)



#### Subsampling layers

Subsampling is typically applied in two forms of pooling operations in CNNs: max-pooling and mean-pooling (also known as average-pooling).

![](C:/Users/sqrte/python-playground/profile/img/pooling.png)

Traditionally, pooling is assumed to be nonoverlapping. Pooling is typically performed on nonoverlapping neighborhoods, which can be done by setting the stride parameter equal to the pooling size.



#### Implementing CNN

![](C:/Users/sqrte/python-playground/profile/img/img_conv.png)





## Recurrent Neural Networks

Recurrent neural networks (RNNs) are used to label, classify, or generate sequences.



Each training example is a matrix in which each row is a feature vector. For simplicity, let‚Äôs illustrate this matrix as a sequence of vectors X = [x1, x2,..., xt‚â†1, xt , xt+1,..., xlengthX ], where lengthX is the length of the input sequence. If our input example X is a text sentence, then feature vector xt for each t = 1, . . . , lengthX represents a word in the sentence at position t. As depicted in Figure 8, in an RNN, the feature vectors from an input example are ‚Äúread‚Äù by the neural network sequentially in the order of the timesteps. The index t denotes a timestep. To update the state ht l,u at each timestep t in each unit u of each layer l we first calculate a linear combination of the input feature vector with the state vector ht‚â†1 l,u of this same layer from the previous timestep, t ‚â† 1. The linear combination of two vectors is calculated using two parameter vectors wl,u, ul,u and a parameter bl,u. The value of ht l,u is then obtained by applying activation function g1 to the result of the linear combination. A typical choice for function g1 is tanh. The output yt l is typically a vector calculated for the whole layer l at once. To obtain yt l, we use activation function g2 that takes a vector as input and returns a dierent vector of the same dimensionality. The function g2 is applied to a linear combination of the state vector values ht l,u calculated using a parameter matrix Vl and a parameter vector cl,u. 

#### **Different architectures of RNN**

![](C:/Users/sqrte/python-playground/profile/img/rnn_architectures.png)

* Many-to-one: The input data is a sequence, but the output is a fixed-size vector or scalar, not a sequence. For example, in sentiment analysis, the input is text-based (for example, a movie review) and the output is a class label (for example, a label denoting whether a reviewer liked the movie). 
* One-to-many: The input data is in standard format and not a sequence, but the output is a sequence. An example of this category is image captioning‚Äî the input is an image and the output is an English phrase summarizing the content of that image
* Many-to-many: Both the input and output arrays are sequences. This category can be further divided based on whether the input and output are synchronized. An example of a synchronized many-to-many modeling task is video classification, where each frame in a video is labeled. An example of a delayed many-to-many modeling task would be translating one language into another. For instance, an entire English sentence must be read and processed by a machine before its translation into German is produced.

![](C:/Users/sqrte/python-playground/profile/img/rnn_weights.png)

Each directed edge (the connections between boxes) in the representation of an RNN that we just looked at is associated with a weight matrix. Those weights do not depend on time, t; therefore, they are shared across the time axis. The different weight matrices in a single-layer RNN are as follows:

* ùëæùë•‚Ñé: The weight matrix between the input, ùíô(ùë°) , and the hidden layer, h 
* ùëæ‚Ñé‚Ñé: The weight matrix associated with the recurrent edge 
* ùëæ‚Ñéùëú: The weight matrix between the hidden layer and output layer

Computing the activations:
$$
\bold z_h^{(t)} = \bold W _{xh} \bold x^{(t)} + \bold W_{hh} \bold h ^{t-1} + \bold b _h
\\
\bold h^{(t)} = \phi_h(\bold z_h^{(t)})
$$

#### **Backpropagation through time**

The derivation of the gradients might be a bit complicated, but the basic idea is that the overall loss, L, is the sum of all the loss functions at times t = 1 to t = T. Since the loss at time t is dependent on the hidden units at all previous time steps 1 : t, the gradient will be computed as follows:
$$
L = \sum_{t=1}^T L^{(t)}
\\
\frac{dL^{(t)}}{d\bold W_{hh}} = \frac{dL^{(t)}}{d\bold o^{(t)}} \frac{d\bold o^{(t)}}{d\bold h^{(t)}}(\sum_{k=1}^t \frac {d\bold h^{(t)}}{d\bold h^{(k)}} \frac{d\bold h^{(k)}}{d\bold W_{hh}})
\\
\frac {d\bold h^{(t)}}{d\bold h^{(k)}} = \prod_{i=k+1}^{t} \frac {d\bold h^{(i)}}{d\bold h^{(i-1)}}
$$

#### **Hidden-recurrence versus output-recurrence**

![](C:/Users/sqrte/python-playground/profile/img/output_rec_prop.png)

#### LSTM

The most effective recurrent neural network models used in practice are gated RNNs. These include the long short-term memory (LSTM) networks and networks based on the gated recurrent unit (GRU). The beauty of using gated units in RNNs is that such networks can store information in their units for future use, much like bits in a computer‚Äôs memory

![](C:/Users/sqrte/python-playground/profile/img/LSTM.png)

![](C:/Users/sqrte/python-playground/profile/img/text.png)

In an LSTM cell, there are three different types of gates, which are known as the forget gate, the input gate, and the output gate: 

* The forget gate ( ùíáùë°) allows the memory cell to reset the cell state without growing indefinitely. In fact, the forget gate decides which information is allowed to go through and which information to suppress. Now, ùíáùë° is computed as follows:
  $$
  \bold f_t = \sigma(\bold W_{xf} \bold x^{(t)} + \bold W_{hf} \bold h ^{(t-1)} + \bold b_f)
  $$

* The input gate (ùíäùë°) and candidate value {\hat C_t} are responsible for updating the cell state. They are computed as follows:
  $$
  \bold i _t = \sigma (\bold W _{xi} \bold x^{(t)} + \bold W _{hi}\bold h ^{(t-1)} + \bold b_i)
  \\
  \bold{\hat C_t} = tanh(\bold W_{xc}\bold x^{(t)} + \bold W_{hc} \bold h^{(t-1)} +\bold b_c)
  $$
  The cell state at time t is computed as follows:
  $$
  C^{(t)} = (C^{(t-1)} ‚äôf_t) ‚®Å (i_t ‚äô{\hat  C_t})
  $$

* The output gate (ùíêùë°) decides how to update the values of hidden units:
  $$
  \bold o_t = \sigma(\bold W_{xo} \bold x^{(t)} + \bold W_{ho} \bold h ^{(t-1)} + \bold b_o)
  $$

Given this, the hidden units at the current time step are computed as follows: 
$$
\bold h_t = \bold o_t ‚äô tanh(\bold C ^{(t)})
$$


```python
def build_model(vocab_size, embedding_dim,rnn_units):
  model = tf.keras.models.Sequential([
                                      tf.keras.layers.Embedding(vocab_size, embedding_dim),
                                      tf.keras.layers.LSTM(rnn_units,
                                                           return_sequences=True),
                                      tf.keras.layers.Dense(vocab_size)
  ])
  return model
```





#### LSTM data flow



You take the first token from the first sample and pass its 300-element vector representation into the first LSTM cell. On the way into the cell, the vector representation of the data is concatenated with the vector output from the previous time step (which is a 0 vector in the first time step). In this example, you‚Äôll have a vector that is 300 + 50 elements long.

At the first fork in the road, you hand off a copy of the combined input vector to the ominous sounding **forget gate**. The forget gate‚Äôs goal is to learn, based on a given input, how much of the cell‚Äôs memory you want to erase. The idea behind wanting to forget is as important as wanting to remember. As a human reader, when you pick up certain bits of information from text‚Äîsay whether the noun is singular or plural‚Äîyou want to retain that information so that later in the sentence you can recognize the right verb conjugation or adjective form to match with it.  The forget gate itself is just a feed forward network. It consists of n neurons each with m + n + 1 weights. So your example forget gate has 50 neurons each with 351 (300 + 50 + 1) weights. The activation function for a forget gate is the sigmoid function, because you want the output for each neuron in the gate to be between 0 and 1. The output vector of the forget gate is then a mask of sorts, albeit a porous one, that erases elements of the memory vector. As the forget gate outputs values closer to 1, more of the memory‚Äôs knowledge in the associated element is retained for that time step; the closer it is to 0 the more of that memory value is erased.

Just like in the forget gate, you use a small network to learn how much to augment the memory based on two things: the input so far and the output from the last time step. This is what happens in the next gate you branch into: the **candidate (input) gate**. The candidate gate has two separate neurons inside it that do two things: 

1. Decide which input vector elements are worth remembering (similar to the mask in the forget gate) - neuron with a sigmoid activation function .
2. Route the remembered input elements to the right memory ‚Äúslot‚Äù - The second part of this gate determines what values you‚Äôre going to update the memory with. tanh activation function that forces the output value to range between -1 and 1.

The output of these two vectors are multiplied together elementwise. The resulting vector from this multiplication is then added, again elementwise, to the memory register, thus remembering the new details .

![image](https://drive.google.com/uc?export=view&id=1F98X04-B51OOqG0ZOrrqH8E5CejFswMm)

This gate is learning simultaneously which values to extract and the magnitude of those particular values. The mask and magnitude become what‚Äôs added to the memory state. As in the forget gate, the candidate gate is learning to mask off the inappropriate information before adding it to the cell‚Äôs memory.

So old, hopefully irrelevant things are forgotten, and new things are remembered. Then you arrive at the last gate of the cell: the **output gate**. Up until this point in the journey through the cell, you‚Äôve only written to the cell‚Äôs memory. Now it‚Äôs finally time to get some use out of this structure. The output gate takes the input (remember this is still the concatenation of the input at time step t and the output of the cell at time step t-1) and passes it into the output gate.

**How does this thing learn then?** Backpropagation‚Äîas with any other neural net. For a moment, let‚Äôs step back and look at the problem you‚Äôre trying to solve with this new complexity. A vanilla RNN is susceptible to a vanishing gradient because the derivative at any given time step is a factor of the weights themselves, so as you step back in time coalescing the various deltas, after a few iterations, the weights may shrink the gradient away to 0. The update to the weights at the end of the backpropagation (which would equate to the beginning of the sequence) are either minuscule or effectively 0. A similar problem occurs when the weights are somewhat large: the gradient explodes and grows disproportionately to the network.

An LSTM avoids this problem via the memory state itself. The neurons in each of the gates are updated via derivatives of the functions they fed, namely those that update the memory state on the forward pass. So at any given time step, as the normal chain rule is applied backwards to the forward propagation, the updates to the neurons are dependent on only the memory state at that time step and the previous one. This way, the error of the entire function is kept ‚Äúnearer‚Äù to the neurons for each time step. This is known as the error carousel.





## Sequence-to-sequence models and attention

Key sequence-to-sequence applications are: 

* Chatbot conversations 
* Question answering 
* Machine translation 
* Image captioning 
* Visual question answering 
* Document summarization



### Encoder-decoder architecture



The first half of an encoder-decoder model is the sequence encoder, a network which turns a sequence, such as natural language text, into a lower-dimensional representation.  The other half of an encoder-decoder architecture is the sequence decoder. A sequence decoder can be designed to turn a vector back into human readable text again.

A sequence-to-sequence network consists of two modular recurrent networks with a thought vector between them. The encoder outputs a thought vector at the end of its input sequence. The decoder picks up that thought and outputs a sequence of tokens.

The training and inference stages are treated differently in this particular setup. During **training**, you pass the starting text to the encoder and the expected text as the input to the decoder. You‚Äôre getting the decoder network to learn that, given a primed state and a key to ‚Äúget started,‚Äù it should produce a series of tokens. The first direct input to the decoder will be the start token; the second input should be the first expected or predicted token, which should in turn prompt the network to produce the second expected token.

At **inference** time, however, you don‚Äôt have the expected text, so what do you use to pass into the decoder other than the state? You use the generic start token and then take the first generated element, which will then become the input to the decoder at the next time step, to generate the next element, and so on. This process repeats until the maximum number of sequence elements is reached or a stop token is generated.

Trained end-to-end this way, the decoder will turn a thought vector into a fully decoded response to the initial input sequence (such as the user question). Splitting the solution into two networks with the thought vector as the binding piece in between allows you to map input sequences to output sequences of different lengths. 

Usually, you‚Äôd extend the input sequences to match the longest input sequence with pad tokens. In the case of the sequence-to-sequence network, you also need to prepare your target data and **pad** it to match the longest target sequence. Remember, the sequence lengths of the input and target data don‚Äôt need to be the same.

In addition to the required padding, the output sequence should be annotated with the **start and stop tokens**, to tell the decoder when the job starts and when it‚Äôs done. Just keep in mind that you‚Äôll need two versions of the target sequence for training: one that starts with the start token (which you‚Äôll use for the decoder input), and one that starts without the start token (the target sequence the loss function will score for accuracy).

```python
encoder_inputs = Input(shape=(None, input_vocab_size))
encoder = LSTM(num_neurons, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)
encoder_states = (state_h, state_c)

decoder_inputs = Input(shape=(None, output_vocab_size))

decoder_lstm = LSTM(num_neurons,return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)

decoder_dense = Dense(output_vocab_size, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

model = Model(inputs=[encoder_inputs, decoder_inputs],
              outputs=decoder_outputs)

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')

model.fit([encoder_input_data, decoder_input_data],
          decoder_target_data,
          batch_size=batch_size, epochs=epochs)

# Generate output sequences
encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)

thought_input = [Input(shape=(num_neurons,)),
                 Input(shape=(num_neurons,))]

decoder_outputs, state_h, state_c = decoder_lstm(
    decoder_inputs, initial_state=thought_input)

decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)

decoder_model = Model(
    inputs=[decoder_inputs] + thought_input,
    output=[decoder_outputs] + decoder_states)

thought = encoder_model.predict(input_seq)

while not stop_condition:
    output_tokens, h, c = decoder_model.predict([target_seq] + thought)
```



#### Attention



As the name suggests, the idea is to tell the decoder what to pay attention to in the input sequence. This ‚Äúsneak preview‚Äù is achieved by allowing the decoder to also look all the way back into the states of the encoder network in addition to the thought vector. With the attention mechanism, the decoder receives an additional input with every time step representing the one (or many) tokens in the input sequence to pay ‚Äúattention‚Äù to, at this given decoder time step. All sequence positions from the encoder will be represented as a weighted average for each decoder time step.

