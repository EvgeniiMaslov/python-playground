[**Reinforcement Learning**](#Reinforcement-Learning)

* [Markov decision processes](#Markov-decision-processes)
* [The Thompson Sampling model - multi arm bandit example](#The-Thompson-Sampling-model)
* [Q-Learning](#Q-Learning)
* [Deep Q-learning](#Deep-Q-learning)
* [Reinforcement Learning with Monte Carlo](#Reinforcement Learning with Monte Carlo)





## Reinforcement Learning



### Markov decision processes



The Markov decision process, or MDP, is simply a process that models how the AI interacts with the environment over time.

The standard approach for solving MDP problems is by using dynamic programming, but RL offers some key advantages over dynamic programming.

The types of problems that require learning an interactive and sequential decision making process, where the decision at time step t affects the subsequent situations, are mathematically formalized as Markov decision processes (MDPs).
$$
p(s',r|s,a) =^{def} P(S_{t+1}=s', R_{t+1} = r | S_t = s, A_t = a) 
$$
Where p() - conditional probability over the preceding state S_t and taken action A_t.

This probability distribution completely defines the dynamics of the environment (or model of the environment) because, based on this distribution, all transition probabilities of the environment can be computed. Therefore, the environment dynamics are a central criterion for categorizing different RL methods. The types of RL methods that require a model of the environment or try to learn a model of the environment (that is, the environment dynamics) are called model-based methods, as opposed to model-free methods.

The environment dynamics can be considered deterministic if particular actions for given states are always or never taken, that is,p () ∈ {0,1}. Otherwise, in the more general case, the environment would have stochastic behavior.

A Markov process can be represented as a directed cyclic graph in which the nodes in the graph represent the different states of the environment. The edges of the graph (that is, the connections between the nodes) represent the transition probabilities between the states.

![](C:/Users/sqrte/python-playground/profile/img/markov_process.png)

As the agent interacts with the environment, the sequence of observations or states forms a trajectory. There are two types of trajectories. If an agent's trajectory can be divided into subparts such that each starts at time t = 0 and ends in a terminal state 𝑆_𝑇 (at t = T), the task is called an episodic task. On the other hand, if the trajectory is infinitely continuous without a terminal state, the task is called a continuing task. The task related to a learning agent for the game of chess is an episodic task, whereas a cleaning robot that is keeping a house tidy is typically performing a continuing task. 



The so-called **return** at time t is the cumulated reward obtained from the entire duration of an episode. The return at time t can then be calculated from the immediate reward as well as the subsequent ones, as follows:
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}\gamma^k R_{t+k+1}
$$
Here, 𝛾𝛾 is the discount factor in range [0, 1]. The parameter 𝛾𝛾 indicates how much the future rewards are "worth" at the current moment (time t). Note that by setting 𝛾𝛾 = 0, we would imply that we do not care about future rewards

A **policy** typically denoted by 𝜋𝜋(𝑎𝑎|𝑠𝑠) is a function that determines the next action to take, which can be either deterministic, or stochastic (that is, the probability for taking the next action). A stochastic policy then has a probability distribution over actions that an agent can take at a given state:
$$
\pi(a|s) = P[A=a|S=s]
$$
The **value function**, also referred to as the state-value function, measures the goodness of each state—in other words, how good or bad it is to be in a particular state. Now, based on the return 𝐺𝐺𝑡𝑡, we define the value function of state s as the expected return (the average return over all possible episodes) after following policy 𝜋𝜋:
$$
v_{\pi}(s) = E_{\pi}[G_t | S_t = s] = E_{\pi}[\sum_{k=0} \gamma^{k+1} R_{t+k+1} | S_t = s]
$$
Moreover, we can also define a value for each state-action pair, which is called the action-value function and is denoted by 𝑞𝑞𝜋𝜋(𝑠𝑠, 𝑎𝑎). The action-value function refers to the expected return 𝐺𝐺𝑡𝑡 when the agent is at state 𝑆𝑆𝑡𝑡 = 𝑠𝑠 and takes action 𝐴𝐴𝑡𝑡 = 𝑎𝑎. Extending the definition of state-value function to state-action pairs, we get the following:
$$
q_{\pi}(s, a) = E_{\pi}[G_t | S_t = s, A_t = a] = E_{\pi}[\sum_{k=0} \gamma^{k+1} R_{t+k+1} | S_t = s, A_t = a]
$$
In short, the return is the weighted sum of rewards for an entire episode, which would be equal to the discounted final reward in our chess example (since there is only one reward). The value function is the expectation over all possible episodes, which basically computes how "valuable" it is on average to make a certain move.



### The Thompson Sampling model



Each slot machine has its own Beta distribution. Over the rounds, the Beta distribution of the slot machine with the highest conversion rate will be progressively shifted to the right, and the Beta distributions of the strategies with lower conversion rates will be progressively shifted to the left. Therefore,  the slot machine with the highest conversion rate will be selected more and more.

```python
# Importing the libraries
import numpy as np

# Setting conversion rates and the number of samples
conversionRates = [0.15, 0.04, 0.13, 0.11, 0.05]
N = 10000
d = len(conversionRates)

# Creating the dataset
X = np.zeros((N, d))
for i in range(N):
 for j in range(d):
 if np.random.rand() < conversionRates[j]:
 X[i][j] = 1

# Making arrays to count our losses and wins
nPosReward = np.zeros(d)
nNegReward = np.zeros(d)

# Taking our best slot machine through beta distribution and updating
its losses and wins
for i in range(N):
 selected = 0
 maxRandom = 0
    
 for j in range(d):
 	randomBeta = np.random.beta(nPosReward[j] + 1, nNegReward[j] + 1)
 	if randomBeta > maxRandom:
 		maxRandom = randomBeta
 		selected = j
        
 if X[i][selected] == 1:
 	nPosReward[selected] += 1
 else:
 	nNegReward[selected] += 1
    
# Showing which slot machine is considered the best
nSelected = nPosReward + nNegReward
for i in range(d):
 print('Machine number ' + str(i + 1) + ' was selected ' + str(nSelected[i]) + ' times')

print('Conclusion: Best machine is machine number ' + str(np.argmax(nSelected) + 1))
```



### Q-Learning



Q-learning fundamentals:

1. Q-learning is a Reinforcement Learning model. 
2. Q-learning works on the inputs (states) and outputs (actions) principle. 
3. Q-learning works on a predefined environment, including the states (the inputs), the actions (the outputs), and the rewards. 
4. Q-learning is modeled by a Markov decision process. 
5. Q-learning uses a training mode, during which the parameters that are learned are called the Q-values, and an inference mode.
6. There are a finite number of states (there is not an infinity of possible inputs). 
7. There are a finite number of actions (only a certain number of actions can be performed).



**Example: The Maze**



![](C:/Users/sqrte/python-playground/profile/img/maze.png)



**The state**, at a specific time or specific iteration, is simply going to be the position of the AI at that time (letter A-L where agent in specific time).

**The actions** are simply going to be the next moves the AI can make to go from one location to the next.

**The reward:**

1. Initialize the matrix (where the rows correspond to the states, columns correspond to the actions) with 1 to the actions robot can perform and 0 to the actions robot cannot perform. 
2. Put very high reward to goal location (G state).  

![](C:/Users/sqrte/python-playground/profile/img/reward.png)



**Building AI:**



1. **The Q-value** - to each couple of state and action (s, a) we are going to associated a numeric value Q(s, a).

   The Q-values measure the accumulation of "good surprise" or "frustration" associated with the couple of action and state ( ) ,t t s a . In the "good surprise" case of a high temporal difference, the AI is reinforced, and in the "frustration" case of a low temporal difference, the AI is weakened. We want to learn the Q-values that will give the AI the maximum "good surprise," and that's exactly what the Bellman equation does by updating the Q-values at each iteration.

2. **The temporal difference** - lets say we are in specific state s_t, at a specific time t. Let's just perform random action.  The temporal difference (TD) represents how well the AI is learning.

   The TD:
   $$
   TD_t(s_t, a_t) = [R(s_t, a_t) + \gamma max_a(Q(s_{t+1}, a))] - Q(s_t, a_t)
   $$

   * [...] - the reward obtained by performing action a in the state s, plus the Q-value of the best action performed in future state. discounted by a factor gamma {0, 1}.
   * Q(s_t, a_t) - the Q-value of the action a_t, performing in the state s_t.

   

   Here's how it works exactly, with respect to the training process (during which the Q-values are learned): 

   1. At the beginning of the training, the Q-values are set to zero. Since the AI is looking to get the good rewards (here 1 or 1000), it is looking for the high temporal differences. Accordingly, if in the first iterations,  TD_t (s_t, a_t) is high, the AI gets a "pleasant surprise" because that means the AI was able to find a good reward. On the other hand, if TD_t (s_t, a_t)  is small, the AI gets a "frustration." 
   2. When the AI gets a great reward, the specific Q-value of the (state, action) that led to that great reward increases, so the AI can remember how it got to that high reward (you'll see exactly how it increases in the next section). For example, let's say that it was the action t a performed in the state t s that led to that high reward R(s_t, a_t) . That would mean the Q-value Q(s_t, a_t) increases automatically (remember, you'll see how in the next section). Those increased Q-values are important information, because they indicate to the AI which transitions lead to the good rewards.
   3. The next step of the AI is not only to look for the great rewards, but also to look at the same time for the high Q-values. Why? Because the high Q-values are the ones that lead to the great reward. In fact, the high Q-values are the ones that lead to higher Q-values, themselves leading to even higher Q-values, themselves leading eventually to the highest reward (1000). That's the role of gamma Q()  in the temporal difference formula. The AI looks for the high Q-values, and as soon as it finds them, the Q-values of the (state, action) that led to these high Q-values will increase again, since they indicate the right path towards the goal. 
   4. At some point, the AI will know all the transitions that lead to the good rewards and high Q-values. Since the Q-values of these transitions have already been increased over time, the temporal differences decrease in the end. In fact, the closer we get to the final goal, the smaller the temporal differences become.

3. **The Bellman equation**.

   In order to perform better and better actions that will lead the AI to reach its goal, you have to increase the Q-values of actions when you find high temporal differences. At each iteration, you update the Q-values from time t-1 (previous iteration) to t (current iteration) through the following equation, called the Bellman equation:
   $$
   Q_t(s_t, a_t) = Q_{t-1}(s_t, a_t) + \alpha TD_t(s_t, a_t)
   $$

4. 

**Training mode:**

1. Initialization: for all couples of states and actions, the Q-values are initialize to 0.

2. At each iteration t >= 1 repeat:

   2.1. Select random state s_t from possible states

   2.2. Perform random action a_t, that can lead to next possible state

   2.3. Reach state s_t+1, get reward R(s_t, a_t)

   2.4. Compute TD

   2.5. Update Q-learning by applying Bellman equation

**Inference mode:**

1. In state s_t perform action a_t, that has highest Q-value for state s_t.





### Deep Q-learning

Initialization: 

1. Initialize the memory of the experience replay to an empty list M. 
2. Choose a maximum size for the memory. 

At each time t, we repeat the following process, until the end of the epoch: 

1. Predict the Q-values of the current state s_t. 

2. Perform the action selected by the Softmax method:
   $$
   a_t = Softmax(Q(s_t, a))
   $$

3. Get the reward R(s_t, a_t).

4. Reach the next state s_t+1.

5. Append the transition (s_t, a_t, r_t, s_t+1) to the memory M. 

6. Take a random batch B ⊂ M of transitions. For all the transitions of batch B:

   * Get the predictions: Q(s_tB, a_tB)

   * Get the targets:
     $$
     R(s_t, a_t) + \gamma max_a(Q(s_{t_B}, a_{t_B}))
     $$

   * Compute the loss between the predictions and target, over whole batch B:
     $$
     Loss = \frac{1}{2} \sum_B (R(s_{t_B}, a_{t_B}) + \gamma max_a (Q(s_{t_B+1}, a)) - Q(s_{t_B}, a_{t_B}))^2 = \frac{1}{2} \sum_B TD_{t_B}(s_{t_B}, a_{t_B})^2
     $$

   * Back-propagate this loss error back into neural network, and through SGD update the weights according to how much contributed to the loss error.





### Reinforcement Learning with Monte Carlo

For MC-based RL, we define an agent class that follows a probabilistic policy, 𝜋𝜋, and based on this policy, our agent takes an action at each step. This results in a simulated episode.

After generating a set of episodes, for each state, s, the set of episodes that all pass through state s is considered for calculating the value of state s. Let's assume that a lookup table is used for obtaining the value corresponding to the value function, 𝑉𝑉(𝑆𝑆𝑡𝑡 = 𝑠𝑠). MC updates for estimating the value function are based on the total return obtained in that episode starting from the first time that state s is visited. This algorithm is called first-visit Monte Carlo value prediction.

MC control refers to the optimization procedure for improving a policy. Similar to the policy iteration approach in previous section (Dynamic programming), we can repeatedly alternate between policy evaluation and policy improvement until we reach the optimal policy. So, starting from a random policy, 𝜋𝜋0, the process of alternating between policy evaluation and policy improvement can be illustrated as follows:

![](C:/Users/sqrte/python-playground/profile/img/mc_control.png)

Given an action-value function, q(s, a), we can generate a greedy (deterministic) policy as follows:
$$
\pi (s) = argmax(q (s,a))
$$
In order to avoid the lack-of-exploration problem, and to consider the non-visited state-action pairs as discussed earlier, we can let the non-optimal actions have a small chance (𝜖𝜖) to be chosen. This is called the 𝜖𝜖-greedy policy, according to which, all non-optimal actions at state s have a minimal (1)  probability of being selected (instead of 0), and the optimal action has a probability of (2) (instead of 1).
$$
\frac{\epsilon}{|A(s)|} (1)
\\
1 - \frac{(|A(s)| - 1)*\epsilon}{|A(s)|}(2)
$$

