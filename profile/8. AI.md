![](https://miro.medium.com/max/650/1*-XKVI5SAEpffNR7BusdvNQ.png)

## Content

1. [What is AI](#What-is-AI)

2. [Problem Solving Agents](#Problem-solving-Agents)

   2.1. [Well-defined problems and solutions](#Well-defined-problems-and-solutions)

   2.2. [Infrastructure for search algorithms](#Infrastructure for search algorithms)

   2.3. [Uniformed Search Strategies](#Uniformed Search Strategies)

   

   * [Breadth-first search](#Breadth-first search)
   * [Uniform-cost search](#Uniform-cost search)
   * [Depth-first search](#Depth-first search)
   * [Depth-limited search](#Depth-limited search)
   * [Iterative deepening depth-first search](#Iterative deepening depth-first search)
   * [Bidirectional search](#Bidirectional search)
   * [Comparing uninformed search strategies](#Comparing uninformed search strategies)

   

   2.4. [Informed (Heuristic) Search Strategies](#Informed Search Strategies)

   

   * [Greedy best-first search](#Greedy best-first search)
   * [A* search: Minimizing the total estimated solution cost](#A* search: Minimizing the total estimated solution cost)
   * [Memory-bounded heuristic search](#Memory-bounded heuristic search)
   * [Heuristic Functions](#Heuristic Functions)

   2.5. [Summary](#Summary-Search)

3. [Beyond classical search](#Beyond classical search)

   * [Hill-climbing search](#Hill-climbing search)
   * [Simulated annealing](#Simulated annealing)
   * [Local beam search](#Local beam search)
   * [Genetic algorithms](#Genetic algorithms)





### What is AI

Some definitions of artificial intelligence, organized into four categories:

| **Thinking Humanly**:                                        | Thinking Rationally:                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| "The exciting new effort to make computers think ... *machines with minds*, in the full and literal sense" (Haugeland, 1985)<br />“[The automation of] activities that we associate with human thinking, activities such as decision-making, problem solving, learning ...” (Bellman, 1978) | “The study of mental faculties through the use of computational models.” (Charniak and McDermott, 1985) <br />“The study of the computations that make it possible to perceive, reason, and act.” (Winston, 1992) |
| **Acting Humanly:**                                          | **Acting Rationally:**                                       |
| “The art of creating machines that perform functions that require intelligence when performed by people.” (Kurzweil, 1990)<br />“The study of how to make computers do things at which, at the moment, people are better.” (Rich and Knight, 1991) | “Computational Intelligence is the study of the design of intelligent agents.” (Poole et al., 1998)<br />“AI . . . is concerned with intelligent behavior in artifacts.” (Nilsson, 1998) |

**Acting Humanly: Turing Test**: 

The Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactory operational definition of intelligence. A computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a person or from a computer. The computer would need to possess the following capabilities:

* **natural language processing** to enable it to communicate successfully in English;
* **knowledge representation** to store what it knows or hears;
* **automated reasoning** to use the stored information to answer questions and to draw new conclusions;
* **machine learning** to adapt to new circumstances and to detect and extrapolate patterns.

The so-called total Turing Test includes a video signal so that the interrogator can test the subject’s perceptual abilities, as well as the opportunity for the interrogator to pass physical objects “through the hatch.” To pass the total Turing Test, the computer will need:

* **computer vision** to perceive objects, and
* **robotics** to manipulate objects and move about.

These six disciplines compose most of AI.

**Rational agent definition: ** For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.



**Whirlwind tour of AI**:

* An agent is something that perceives and acts in an environment. The **agent function** for an agent specifies the action taken by the agent in response to any percept sequence. 
* The **performance measure** evaluates the behavior of the agent in an environment. A **rational agent** acts so as to maximize the expected value of the performance measure, given the percept sequence it has seen so far. 
* A **task environment** specification includes the performance measure, the external environment, the actuators, and the sensors. In designing an agent, the first step must always be to specify the task environment as fully as possible. 
* Task environments vary along several significant dimensions. They can be fully or partially observable, single-agent or multiagent, deterministic or stochastic, episodic or sequential, static or dynamic, discrete or continuous, and known or unknown. 
* The **agent program** implements the agent function. There exists a variety of basic agent-program designs reflecting the kind of information made explicit and used in the decision process. The designs vary in efficiency, compactness, and flexibility. The appropriate design of the agent program depends on the nature of the environment. 
* **Simple reflex agents** respond directly to percepts, whereas **model-based reflex agents** maintain internal state to track aspects of the world that are not evident in the current percept. **Goal-based agents** act to achieve their goals, and **utility-based agents** try to maximize their own expected “happiness.” 
* All agents can improve their performance through **learning**.





### Problem-solving Agents



Problem-solving agents use atomic representations, that is, states of the world are considered as wholes, with no internal structure visible to the problem-solving algorithms.

**Goal formulation**, based on the current situation and the agent’s performance measure, is the first step in problem solving. We will consider a goal to be a set of world states—exactly those states in which the goal is satisfied. The agent’s task is to find out how to act, now and in the future, so that it reaches a goal state.

**Problem formulation** is the process of deciding what actions and states to consider, given a goal.

The process of looking for a sequence of actions that reaches the goal is called **search**. A search algorithm takes a problem as input and returns a **solution** in the form of an action sequence. Once a solution is found, the actions it recommends can be carried out. This is called the **execution** phase. 

Thus, we have a simple “formulate, search, execute” design for the agent.



```pseudocode
function Simple-Problem-Solving-Agent(percept) return an action:
	persistent: seq, an action sequence, initially empty
				state, some description of current world state
				goal, a goal, initially null,
				problem, a problem formulation
	
	state <- UpdateState(state, percept)
	if seq is empty then:
		goal <- FormulateGoal(state)
		problem <- FormulateProblem(state, goal)
		seq <- Search(problem)
		if seq == failure then return a null action
	action <- First(seq)
	seq <- Rest(seq)
	return action
```





#### Well-defined problems and solutions



A problem can be defined formally by five components:

* The **initial state** that the agent starts in.

* A description of the possible **actions** available to the agent. Given a particular state s, ACTIONS(s) returns the set of actions that can be executed in s. We say that each of these actions is applicable in s.

* A description of what each action does; the formal name for this is the **transition model**, specified by a function RESULT(s, a) that returns the state that results from doing action a in state s. We also use the term **successor** to refer to any state reachable from a given state by a single action.

  Together, the initial state, actions, and transition model implicitly define the **state space** of the problem—the set of all states reachable from the initial state by any sequence of actions.

* The **goal test**, which determines whether a given state is a goal state. Sometimes there is an explicit set of possible goal states, and the test simply checks whether the given state is one of them.

* A **path cost** function that assigns a numeric cost to each path. The problem-solving agent chooses a cost function that reflects its own performance measure.



Having formulated some problems, we now need to solve them. A solution is an action sequence, so search algorithms work by considering various possible action sequences. The possible action sequences starting at the initial state form a search tree with the initial state at the root; the branches are actions and the nodes correspond to states in the state space of the problem.

As the saying goes, algorithms that forget their history are doomed to repeat it. The way to avoid exploring redundant paths is to remember where one has been. To do this, we augment the TREE-SEARCH algorithm with a data structure called the explored set (also known as the closed list), which remembers every expanded node. Newly generated nodes that match previously generated nodes—ones in the explored set or the frontier—can be discarded instead of being added to the frontier. This algorithm is GRAPH-SEARCH.  

```pseudocode
function TREE-SEARCH(problem) returns a solution, or failure

	initialize the frontier using the initial state of problem
	loop do
        if the frontier is empty then return failure
        choose a leaf node and remove it from the frontier
        if the node contains a goal state then return the corresponding solution
        expand the chosen node, adding the resulting nodes to the frontier
        
        
function GRAPH-SEARCH(problem) returns a solution, or failure
    initialize the frontier using the initial state of problem
    initialize the explored set to be empty
    loop do
        if the frontier is empty then return failure
        choose a leaf node and remove it from the frontier
        if the node contains a goal state then return the corresponding solution
        add the node to the explored set
        expand the chosen node, adding the resulting nodes to the frontier
        	only if not in the frontier or explored set
```



#### Infrastructure for search algorithms

Search algorithms require a data structure to keep track of the search tree that is being constructed. For each node n of the tree, we have a structure that contains four components:

* n.STATE: the state in the state space to which the node corresponds; 
* n.PARENT: the node in the search tree that generated this node; 
* n.ACTION: the action that was applied to the parent to generate the node; 
* n.PATH-COST: the cost, traditionally denoted by g(n), of the path from the initial state to the node, as indicated by the parent pointers.

Given the components for a parent node, it is easy to see how to compute the necessary components for a child node. The function CHILD-NODE takes a parent node and an action and returns the resulting child node:

```pseudocode
function CHILD-NODE(problem, parent, action) returns a node
    return a node with
        STATE = problem.RESULT(parent.STATE, action),
        PARENT = parent, ACTION = action,
        PATH-COST = parent.PATH-COST + problem.STEP-COST(parent.STATE, action)
```

Now that we have nodes, we need somewhere to put them. The frontier needs to be stored in such a way that the search algorithm can easily choose the next node to expand according to its preferred strategy. The appropriate data structure for this is a queue. The operations on a queue are as follows:

* EMPTY?(queue) returns true only if there are no more elements in the queue. 
* POP(queue) removes the first element of the queue and returns it. 
* INSERT(element, queue) inserts an element and returns the resulting queue.





#### Uniformed Search Strategies



The term **uninformed search** (also called blind search) means that the strategies have no additional information about states beyond that provided in the problem definition. All they can do is generate successors and distinguish a goal state from a non-goal state.



##### Breadth-first search

Breadth-first search is a simple strategy in which the root node is expanded first, then all the successors of the root node are expanded next, then their successors, and so on. In general, all the nodes are expanded at a given depth in the search tree before any nodes at the next level are expanded.

Algorithm is *complete* — if the shallowest goal node is at some finite depth d, breadth-first search will eventually find it after generating all shallower nodes. 

The shallowest goal node is not necessarily the *optimal* one; technically, breadth-first search is optimal if the path cost is a nondecreasing function of the depth of the node.

Imagine searching a uniform tree where every state has b successors. The root of the search tree generates b nodes at the first level, each of which generates b more nodes, for a total of b^2 at the second level. Each of these generates b more nodes, yielding b^3 nodes at the third level, and so on. Now suppose that the solution is at depth d. In the worst case, it is the last node generated at that level. Then the total number of nodes generated is O(b^d). 

As for space complexity: for any kind of graph search, which stores every expanded node in the explored set, the space complexity is always within a factor of b of the time complexity. 



```pseudocode
function BREADTH-FIRST-SEARCH(problem) returns a solution, or failure
    node ←a node with STATE = problem.INITIAL-STATE, PATH-COST = 0
    if problem.GOAL-TEST(node.STATE) then return SOLUTION(node)
    frontier ← a FIFO queue with node as the only element
    explored ← an empty set
    loop do
        if EMPTY?(frontier ) then return failure
        node ← POP(frontier ) /* chooses the shallowest node in frontier */
        add node.STATE to explored
        for each action in problem.ACTIONS(node.STATE) do
            child ← CHILD-NODE(problem, node, action)
            if child.STATE is not in explored or frontier then
                if problem.GOAL-TEST(child.STATE) then return SOLUTION(child)
                frontier ← INSERT(child,frontier )	
```



##### Uniform-cost search



When all step costs are equal, breadth-first search is optimal because it always expands the shallowest unexpanded node. By a simple extension, we can find an algorithm that is optimal with any step-cost function. Instead of expanding the shallowest node, **uniform-cost search** expands the node n with the lowest path cost g(n). This is done by storing the frontier as a priority queue ordered by g. 

In addition to the ordering of the queue by path cost, there are two other significant differences from breadth-first search. The first is that the goal test is applied to a node when it is selected for expansion rather than when it is first generated.  The second difference is that a test is added in case a better path is found to a node currently on the frontier.

It is easy to see that uniform-cost search is optimal in general. First, we observe that whenever uniform-cost search selects a node n for expansion, the optimal path to that node has been found.

Uniform-cost search does not care about the number of steps a path has, but only about their total cost. Therefore, it will get stuck in an infinite loop if there is a path with an infinite sequence of zero-cost actions.

Uniform-cost search is guided by path costs rather than depths, so its complexity is not easily characterized in terms of b and d. Instead, let C be the cost of the optimal solution,7 and assume that every action costs at least . Then the algorithm’s worst-case time and space complexity is
$$
O(b^{1 + floor(C / \epsilon)}) \text{, which can be much greater than b^d.}
$$

```pseudocode
function UNIFORM-COST-SEARCH(problem) returns a solution, or failure
    node ←a node with STATE = problem.INITIAL-STATE, PATH-COST = 0
    frontier ← a priority queue ordered by PATH-COST, with node as the only element
    explored ← an empty set
    loop do
        if EMPTY?(frontier ) then return failure
        node ← POP(frontier ) /* chooses the lowest-cost node in frontier */
        if problem.GOAL-TEST(node.STATE) then return SOLUTION(node)
        add node.STATE to explored
        for each action in problem.ACTIONS(node.STATE) do
            child ← CHILD-NODE(problem, node, action)
            if child.STATE is not in explored or frontier then
            	frontier ← INSERT(child,frontier )
            else if child.STATE is in frontier with higher PATH-COST then
            	replace that frontier node with child
```



##### Depth-first search



**Depth-first** search always expands the deepest node in the current frontier of the search tree. Depth-first search uses a LIFO queue (stack).

The properties of depth-first search depend strongly on whether the graph-search or tree-search version is used. The graph-search version, which avoids repeated states and redundant paths, is complete in finite state spaces because it will eventually expand every node. The tree-search version, on the other hand, is not complete.

The time complexity of depth-first graph search is bounded by the size of the state space (which may be infinite, of course). A depth-first tree search, on the other hand, may generate all of the O(b^m) nodes in the search tree, where m is the maximum depth of any node; this can be much greater than the size of the state space.

So far, depth-first search seems to have no clear advantage over breadth-first search, so why do we include it? The reason is the space complexity. For a graph search, there is no advantage, but a depth-first tree search needs to store only a single path from the root to a leaf node, along with the remaining unexpanded sibling nodes for each node on the path.

A variant of depth-first search called **backtracking search** uses still less memory. In backtracking, only one successor is generated at a time rather than all successors; each partially expanded node remembers which successor to generate next.



##### Depth-limited search



The embarrassing failure of depth-first search in infinite state spaces can be alleviated by supplying depth-first search with a predetermined depth limit. That is, nodes at depth are treated as if they have no successors.



```pseudocode
function DEPTH-LIMITED-SEARCH(problem, limit) returns a solution, or failure/cutoff
	return RECURSIVE-DLS(MAKE-NODE(problem.INITIAL-STATE), problem, limit)
	
	
function RECURSIVE-DLS(node, problem, limit) returns a solution, or failure/cutoff
    if problem.GOAL-TEST(node.STATE) then return SOLUTION(node)
    else if limit = 0 then return cutoff
    else
        cutoff occurred?←false
        for each action in problem.ACTIONS(node.STATE) do
            child ← CHILD-NODE(problem, node, action)
            result ← RECURSIVE-DLS(child, problem, limit − 1)
            if result = cutoff then cutoff occurred?← true
            else if result = failure then return result
        if cutoff occurred? then return cutoff else return failure
```



##### Iterative deepening depth-first search

**Iterative deepening search** (or iterative deepening depth-first search) is a general strategy, often used in combination with depth-first tree search, that finds the best depth limit. It does this by gradually increasing the limit—first 0, then 1, then 2, and so on—until a goal is found. This will occur when the depth limit reaches d, the depth of the shallowest goal node.

Like depth-first search, its memory requirements are modest: O(bd) to be precise. Like breadth-first search, it is complete when the branching factor is finite and optimal when the path cost is a nondecreasing function of the depth of the node.

Iterative deepening search may seem wasteful because states are generated multiple times. It turns out this is not too costly. The reason is that in a search tree with the same (or nearly the same) branching factor at each level, most of the nodes are in the bottom level, so it does not matter much that the upper levels are generated multiple times.

In general, iterative deepening is the preferred uninformed search method when the search space is large and the depth of the solution is not known.

```pseudocode
function ITERATIVE-DEEPENING-SEARCH(problem) returns a solution, or failure
    for depth = 0 to ∞ do
        result ← DEPTH-LIMITED-SEARCH(problem, depth)
        if result != cutoff then return result
```





##### Bidirectional search

The idea behind bidirectional search is to run two simultaneous searches—one forward from the initial state and the other backward from the goal—hoping that the two searches meet in the middle. Bidirectional search is implemented by replacing the goal test with a check to see whether the frontiers of the two searches intersect; if they do, a solution has been found.

The reduction in time complexity makes bidirectional search attractive, but how do we search backward? This is not as easy as it sounds. Let the **predecessors** of a state x be all those states that have x as a successor. Bidirectional search requires a method for computing predecessors. When all the actions in the state space are reversible, the predecessors of x are just its successors. Other cases may require substantial ingenuity.





##### Comparing uninformed search strategies

* b is the branching factor; 
* d is the depth of the shallowest solution; 
* m is the maximum depth of the search tree; 
* l is the depth limit.

| Criterion | Breadth-First                       | Uniform-Cost                                                 | Depth-Limited | Iterative-Deepening                 | Bidirectional <br />(if applicable)                          |
| --------- | ----------------------------------- | ------------------------------------------------------------ | ------------- | ----------------------------------- | ------------------------------------------------------------ |
| Complete? | Yes if branching factor finite      | Yes if branching factor finite and if step costs ≥ epsilon  for positive epsilon | No            | Yes if branching factor finite      | Yes if branching factor finite and if both directions use breadth-first search |
| Time      | O(b^d)                              | O(b^{1 + floor(C/epsilon)})                                  | O(b^m)        | O(b^d)                              | O(b^{d/2})                                                   |
| Space     | O(b^d)                              | O(b^{1 + floor(C/epsilon)})                                  | O(b*m)        | O(b*d)                              | O(b^{d/2})                                                   |
| Optimal?  | Yes if step costs are all identical | Yes                                                          | No            | Yes if step costs are all identical | Yes if step costs are all identical and if both directions use breadth-first search |







#### Informed Search Strategies

**Informed search strategy**—one that uses problem-specific knowledge beyond the definition of the problem itself—can find solutions more efficiently than can an uninformed strategy.

The general approach we consider is called best-first search. Best-first search is an instance of the general TREE-SEARCH or GRAPH-SEARCH algorithm in which a node is selected for expansion based on an evaluation function, f(n). The evaluation function is construed as a cost estimate, so the node with the lowest evaluation is expanded first. The implementation of best-first graph search is identical to that for uniform-cost search, except for the use of f instead of g to order the priority queue. The choice of f determines the search strategy. Most best-first algorithms include as a component of f a heuristic function, denoted  h(n) = estimated cost of the cheapest path from the state at node n to a goal state.



##### Greedy best-first search



Greedy best-first search tries to expand the node that is closest to the goal, on the grounds that this is likely to lead to a solution quickly. “Greedy”—at each step it tries to get as close to the goal as it can. 

Greedy best-first tree search is also incomplete even in a finite state space, much like depth-first search. The worst-case time and space complexity for the tree version is O(b^m), where m is the maximum depth of the search space. With a good heuristic function, however, the complexity can be reduced substantially. The amount of the reduction depends on the particular problem and on the quality of the heuristic.



##### A* search: Minimizing the total estimated solution cost



The most widely known form of best-first search is called A∗ A search (pronounced “A-star search”). It evaluates nodes by combining g(n), the cost to reach the node, and h(n), the cost to get from the node to the goal:
$$
f(n) = g(n) + h(n)
$$
Since g(n) gives the path cost from the start node to node n, and h(n) is the estimated cost of the cheapest path from n to the goal, we have f(n) = estimated cost of the cheapest solution through n .

Thus, if we are trying to find the cheapest solution, a reasonable thing to try first is the node with the lowest value of g(n) + h(n). It turns out that this strategy is more than just reasonable: provided that the heuristic function h(n) satisfies certain conditions, A∗ search is both complete and optimal. The algorithm is identical to UNIFORM-COST-SEARCH except that A∗ uses g + h instead of g

The first condition we require for optimality is that h(n) be an **admissible heuristic**. An admissible heuristic is one that never overestimates the cost to reach the goal. Because g(n) is the actual cost to reach n along the current path, and f(n) = g(n) + h(n), we have as an immediate consequence that f(n) never overestimates the true cost of a solution along the current path through n.

A second, slightly stronger condition called **consistency** (or sometimes monotonicity) MONOTONICITY is required only for applications of A∗ to graph search.9 A heuristic h(n) is consistent if, for every node n and every successor n' of n generated by any action a, the estimated cost of reaching the goal from n is no greater than the step cost of getting to n' plus the estimated cost of reaching the goal from n':
$$
h(n) ≤ c(n, a, n') + h(n')
$$
This is a form of the general **triangle inequality**, which stipulates that each side of a triangle cannot be longer than the sum of the other two sides.

A∗ has the following properties: the tree-search version of A∗ is optimal if h(n) is admissible, while the graph-search version is optimal if h(n) is consistent.

Computation time is not, however, A∗’s main drawback. Because it keeps all generated nodes in memory (as do all GRAPH-SEARCH algorithms), A∗ usually runs out of space long before it runs out of time. For this reason, A∗ is not practical for many large-scale problems. 









##### Memory-bounded heuristic search



The simplest way to reduce memory requirements for A∗ is to adapt the idea of iterative deepening to the heuristic search context, resulting in the **iterative-deepening A∗** (IDA∗) algorithm. The main difference between IDA∗ and standard iterative deepening is that the cutoff used is the f-cost (g +h) rather than the depth; at each iteration, the cutoff value is the smallest f-cost of any node that exceeded the cutoff on the previous iteration. 

**Recursive best-first search** (RBFS) is a simple recursive algorithm that attempts to mimic the operation of standard best-first search, but using only linear space. Its structure is similar to that of a recursive depth-first search, but rather than continuing indefinitely down the current path, it uses the f limit variable to keep track of the f-value of the best alternative path available from any ancestor of the current node. If the current node exceeds this limit, the recursion unwinds back to the alternative path. As the recursion unwinds, RBFS replaces the f-value of each node along the path with a backed-up value—the best f-value of its children. In this way, RBFS remembers the f-value of the best leaf in the forgotten subtree and can therefore decide whether it’s worth reexpanding the subtree at some later time.

RBFS is somewhat more efficient than IDA∗, but still suffers from excessive node regeneration.



```pseudocode
function RECURSIVE-BEST-FIRST-SEARCH(problem) returns a solution, or failure
	return RBFS(problem, MAKE-NODE(problem.INITIAL-STATE),∞)
	
function RBFS(problem, node,f limit) returns a solution, or failure and a new f-cost limit
    if problem.GOAL-TEST(node.STATE) then return SOLUTION(node)
    successors ←[ ]
    for each action in problem.ACTIONS(node.STATE) do
    	add CHILD-NODE(problem, node, action) into successors
    if successors is empty then return failure, ∞
    for each s in successors do /* update f with value from previous search, if any */
    	s.f ← max(s.g + s.h, node.f ))
    loop do
        best ← the lowest f-value node in successors
        if best.f > f limit then return failure, best.f
        alternative ←the second-lowest f-value among successors
        result, best.f ← RBFS(problem, best, min(f limit, alternative))
        if result != failure then return result
```



IDA∗ and RBFS suffer from using too little memory. Between iterations, IDA∗ retains only a single number: the current f-cost limit. RBFS retains more information in memory, but it uses only linear space: even if more memory were available, RBFS has no way to make use of it. Because they forget most of what they have done, both algorithms may end up reexpanding the same states many times over. 

It seems sensible, therefore, to use all available memory. Two algorithms that do this are **MA∗** (memory-bounded A∗) and **SMA∗** (simplified MA∗).

SMA∗ proceeds just like A∗, expanding the best leaf until memory is full. At this point, it cannot add a new node to the search tree without dropping an old one. SMA∗ always drops the worst leaf node—the one with the highest f-value. Like RBFS, SMA∗ then backs up the value of the forgotten node to its parent. In this way, the ancestor of a forgotten subtree knows the quality of the best path in that subtree. With this information, SMA∗ regenerates the subtree only when all other paths have been shown to look worse than the path it has forgotten.

On very hard problems, however, it will often be the case that SMA∗ is forced to switch back and forth continually among many candidate solution paths, only a small subset of which can fit in memory.





#### Summary Search



This chapter has introduced methods that an agent can use to select actions in environments that are deterministic, observable, static, and completely known. In such cases, the agent can construct sequences of actions that achieve its goals; this process is called search. 

* Before an agent can start searching for solutions, a goal must be identified and a well defined problem must be formulated. 
* A problem consists of five parts: the initial state, a set of actions, a transition model describing the results of those actions, a goal test function, and a path cost function. The environment of the problem is represented by a state space. A path through the state space from the initial state to a goal state is a solution.
* Search algorithms treat states and actions as atomic: they do not consider any internal structure they might possess. 
* A general TREE-SEARCH algorithm considers all possible paths to find a solution, whereas a GRAPH-SEARCH algorithm avoids consideration of redundant paths.
* Search algorithms are judged on the basis of completeness, optimality, time complexity, and space complexity. Complexity depends on b, the branching factor in the state space, and d, the depth of the shallowest solution. 
* Uninformed search methods have access only to the problem definition. The basic algorithms are as follows: 
  * Breadth-first search expands the shallowest nodes first; it is complete, optimal for unit step costs, but has exponential space complexity. 
  * Uniform-cost search expands the node with lowest path cost, g(n), and is optimal for general step costs. 
  * Depth-first search expands the deepest unexpanded node first. It is neither complete nor optimal, but has linear space complexity. Depth-limited search adds a depth bound. 
  * Iterative deepening search calls depth-first search with increasing depth limits until a goal is found. It is complete, optimal for unit step costs, has time complexity comparable to breadth-first search, and has linear space complexity.
  * Bidirectional search can enormously reduce time complexity, but it is not always applicable and may require too much space. 
* Informed search methods may have access to a heuristic function h(n) that estimates the cost of a solution from n.
  * The generic best-first search algorithm selects a node for expansion according to an evaluation function.
  * Greedy best-first search expands nodes with minimal h(n). It is not optimal but is often efficient.
  * A∗ search expands nodes with minimal f(n) = g(n) + h(n). A∗ is complete and optimal, provided that h(n) is admissible (for TREE-SEARCH) or consistent (for GRAPH-SEARCH). The space complexity of A∗ is still prohibitive. 
  * RBFS (recursive best-first search) and SMA∗ (simplified memory-bounded A∗) are robust, optimal search algorithms that use limited amounts of memory; given enough time, they can solve problems that A∗ cannot solve because it runs out of memory. 

* The performance of heuristic search algorithms depends on the quality of the heuristic function. One can sometimes construct good heuristics by relaxing the problem definition, by storing precomputed solution costs for subproblems in a pattern database, or by learning from experience with the problem class.





## Beyond classical search



The search algorithms that we have seen so far are designed to explore search spaces systematically. This systematicity is achieved by keeping one or more paths in memory and by recording which alternatives have been explored at each point along the path. When a goal is found, the path to that goal also constitutes a solution to the problem. In many problems, however, the path to the goal is irrelevant.

**Local search** algorithms operate using a single current node (rather than multiple paths) and generally move only to neighbors of that node.  Although local search algorithms are not systematic, they have two key advantages: (1) they use very little memory—usually a constant amount; and (2) they can often find reasonable solutions in large or infinite (continuous) state spaces for which systematic algorithms are unsuitable

To understand local search, we find it useful to consider the state-space landscape. A landscape has both “location” and “elevation” . If elevation corresponds to cost, then the aim is to find the lowest valley—a global minimum; if elevation corresponds to an objective function, then the aim is to find the highest peak—a global maximum.  A complete local search algorithm always finds a goal if one exists; an optimal algorithm always finds a global minimum/maximum.



#### Hill-climbing search

It is simply a loop that continually moves in the direction of increasing value—that is, uphill.

```pseudocode
function HILL-CLIMBING(problem) returns a state that is a local maximum
    current ← MAKE-NODE(problem.INITIAL-STATE)
    loop do
        neighbor ← a highest-valued successor of current
        if neighbor.VALUE ≤ current.VALUE then return current.STATE
        current ← neighbor
```

Unfortunately, hill climbing often gets stuck for the following reasons:

1. Local maxima. 
2. Ridges. Ridges result in a sequence of local maxima that is very difficult for greedy algorithms to navigate.
3. Plateau.

The hill-climbing algorithms described so far are incomplete—they often fail to find a goal when one exists because they can get stuck on local maxima. **Random-restart hill climbing** adopts the well-known adage, “If at first you don’t succeed, try, try again.” It conducts a series of hill-climbing searches from randomly generated initial states,1 until a goal is found.



#### Simulated annealing



A hill-climbing algorithm that never makes “downhill” moves toward states with lower value (or higher cost) is guaranteed to be incomplete, because it can get stuck on a local maximum.

The innermost loop of the simulated-annealing algorithm is quite similar to hill climbing. Instead of picking the best move, however, it picks a random move. If the move improves the situation, it is always accepted. Otherwise, the algorithm accepts the move with some probability less than 1.

```pseudocode
function SIMULATED-ANNEALING(problem, schedule) returns a solution state
    inputs: problem, a problem
    	schedule, a mapping from time to “temperature”
    	
    current ← MAKE-NODE(problem.INITIAL-STATE)
    for t = 1 to ∞ do
        T ← schedule(t)
        if T = 0 then return current
        next ← a randomly selected successor of current
        ΔE ← next.VALUE – current.VALUE
        if ΔE > 0 then current ← next
        else current ← next only with probability e^ΔE/T
```





#### Local beam search



Keeping just one node in memory might seem to be an extreme reaction to the problem of memory limitations. The local beam search algorithm keeps track of k states rather than just one. It begins with k randomly generated states. At each step, all the successors of all k states are generated. If any one is a goal, the algorithm halts. Otherwise, it selects the k best successors from the complete list and repeats.





#### Genetic algorithms



A genetic algorithm (or GA) is a variant of stochastic beam search in which successor states are generated by combining two parent states rather than by modifying a single state.

Like beam searches, GAs begin with a set of k randomly generated states, called the population. Each state, or individual, is represented as a string over a finite alphabet—most commonly, a string of 0s and 1s.  In each state is rated by the objective function, or (in GA terminology) the fitness function. A fitness function should return higher values for better states. Two pairs are selected at random for reproduction, in accordance with the probabilities. The offspring themselves are created by crossing over the parent strings at the crossover point. Finally, each location is subject to random mutation with a small independent probability.



```pseudocode
function GENETIC-ALGORITHM(population, FITNESS-FN) returns an individual
    inputs: population, a set of individuals
    	FITNESS-FN, a function that measures the fitness of an individual
    	
    repeat
        new population ← empty set
        for i = 1 to SIZE(population) do
            x ← RANDOM-SELECTION(population, FITNESS-FN)
            y ← RANDOM-SELECTION(population, FITNESS-FN)
            child ← REPRODUCE(x , y)
            if (small random probability) then child ← MUTATE(child)
            add child to new population
        population ← new population
    until some individual is fit enough, or enough time has elapsed
    return the best individual in population, according to FITNESS-FN
    
    
function REPRODUCE(x , y) returns an individual
    inputs: x , y, parent individuals
    
    n ← LENGTH(x ); c ←random number from 1 to n
    return APPEND(SUBSTRING(x , 1, c), SUBSTRING(y, c + 1, n))
```



In practice, genetic algorithms have had a widespread impact on optimization problems, such as circuit layout and job-shop scheduling. At present, it is not clear whether the appeal of genetic algorithms arises from their performance or from their æsthetically pleasing origins in the theory of evolution.