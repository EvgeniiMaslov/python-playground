{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9cUYSrljPI8",
        "outputId": "f7d09cee-bbc2-467b-9010-7fa469dcf785"
      },
      "source": [
        "!pip install nlpia"
      ],
      "id": "M9cUYSrljPI8",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nlpia\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/f6/ab35e962dd0b19f1008e88e788b202d45a90d9cd70b9bbf0ac26489ee260/nlpia-0.5.2-py2.py3-none-any.whl (32.0MB)\n",
            "\u001b[K     |████████████████████████████████| 32.0MB 148kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nlpia) (2019.12.20)\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/dc/97f2b63ef0fa1fd78dcb7195aca577804f6b2b51e712516cc0e902a9a201/python-Levenshtein-0.12.2.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hCollecting html2text\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/88/14655f727f66b3e3199f4467bafcc88283e6c31b562686bf606264e09181/html2text-2020.1.16-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from nlpia) (0.22.2.post1)\n",
            "Collecting pypandoc\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/b7/5050dc1769c8a93d3ec7c4bd55be161991c94b8b235f88bf7c764449e708/pypandoc-1.5.tar.gz\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from nlpia) (2.10.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from nlpia) (2.2.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nlpia) (4.41.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from nlpia) (3.2.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from nlpia) (0.16.0)\n",
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.7/dist-packages (from nlpia) (0.9.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from nlpia) (2.4.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from nlpia) (4.4.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from nlpia) (3.6.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from nlpia) (4.2.6)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from nlpia) (1.0.0)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.7/dist-packages (from nlpia) (1.0.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from nlpia) (0.11.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from nlpia) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nlpia) (1.1.5)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from nlpia) (2.4.3)\n",
            "Collecting pugnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/20/a9f0b1f45c074c63da716fc1b301916cc3b64c11d5cbf7cb305eafaf158a/pugnlp-0.2.6-py2.py3-none-any.whl (706kB)\n",
            "\u001b[K     |████████████████████████████████| 716kB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein->nlpia) (54.2.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->nlpia) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->nlpia) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->nlpia) (1.19.5)\n",
            "Requirement already satisfied: pip>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from pypandoc->nlpia) (19.3.1)\n",
            "Requirement already satisfied: wheel>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from pypandoc->nlpia) (0.36.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->nlpia) (1.15.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->nlpia) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->nlpia) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->nlpia) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->nlpia) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->nlpia) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->nlpia) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->nlpia) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->nlpia) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->nlpia) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->nlpia) (0.4.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (3.7.4.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (3.3.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (2.4.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (3.12.4)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (1.12)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->nlpia) (0.3.3)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->nlpia) (1.3.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->nlpia) (5.0.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->nlpia) (4.10.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->nlpia) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->nlpia) (5.3.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->nlpia) (7.6.3)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->nlpia) (5.0.3)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->nlpia) (5.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib->nlpia) (0.5.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nlpia) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nlpia) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nlpia) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nlpia) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->nlpia) (2018.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->nlpia) (3.13)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from pugnlp->nlpia) (4.0.1)\n",
            "Requirement already satisfied: coverage in /usr/local/lib/python3.7/dist-packages (from pugnlp->nlpia) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->nlpia) (3.10.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->nlpia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->nlpia) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->nlpia) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->nlpia) (3.0.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->nlpia) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->nlpia) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->nlpia) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->nlpia) (1.28.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->nlpia) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->nlpia) (5.3.5)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->nlpia) (5.0.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->nlpia) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->nlpia) (5.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nlpia) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nlpia) (0.7.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nlpia) (2.6.1)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nlpia) (5.1.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nlpia) (2.11.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nlpia) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nlpia) (4.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nlpia) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nlpia) (3.3.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nlpia) (1.4.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nlpia) (0.2.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nlpia) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nlpia) (0.9.4)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nlpia) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nlpia) (3.5.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->nlpia) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->nlpia) (22.0.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter->nlpia) (1.0.18)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->pugnlp->nlpia) (1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->nlpia) (3.4.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->nlpia) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->nlpia) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->nlpia) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->nlpia) (0.2.8)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nlpia) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nlpia) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nlpia) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nlpia) (4.4.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4->nbconvert->jupyter->nlpia) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.4->nbconvert->jupyter->nlpia) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->nlpia) (20.9)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->nlpia) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->nlpia) (0.2.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->nlpia) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->nlpia) (0.4.8)\n",
            "Building wheels for collected packages: python-Levenshtein, pypandoc\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149825 sha256=211b5008d5d83a8a9c7a0f0e8bfd54510bb598b8decd105dc686cf0f40465b59\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/26/73/4b48503bac73f01cf18e52cd250947049a7f339e940c5df8fc\n",
            "  Building wheel for pypandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypandoc: filename=pypandoc-1.5-cp37-none-any.whl size=17036 sha256=b83182dd13d7b55aa305941e4c7e63b834f39919ab5515c7dfb8eabe14a78d85\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/7d/d6/2f9af55e800d37e42e546106bcbd36a86e24e725e303d17e04\n",
            "Successfully built python-Levenshtein pypandoc\n",
            "Installing collected packages: python-Levenshtein, html2text, pypandoc, fuzzywuzzy, pugnlp, nlpia\n",
            "Successfully installed fuzzywuzzy-0.18.0 html2text-2020.1.16 nlpia-0.5.2 pugnlp-0.2.6 pypandoc-1.5 python-Levenshtein-0.12.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "artificial-worthy",
        "outputId": "bd9831ef-337f-4f51-9864-ddb3448dd523"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "# read data\n",
        "import glob\n",
        "import os\n",
        "from random import shuffle\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from nlpia.loaders import get_data"
      ],
      "id": "artificial-worthy",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pugnlp/constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
            "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
            "/usr/local/lib/python3.7/dist-packages/pugnlp/constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
            "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
            "/usr/local/lib/python3.7/dist-packages/pugnlp/tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  np = pd.np\n",
            "/usr/local/lib/python3.7/dist-packages/pugnlp/util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  np = pd.np\n",
            "/usr/local/lib/python3.7/dist-packages/nlpia/futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  np = pd.np\n",
            "/usr/local/lib/python3.7/dist-packages/nlpia/loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  np = pd.np\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywiV6LDDjcsX",
        "outputId": "868cfbd9-fe75-4654-b6b3-172cf755825e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "id": "ywiV6LDDjcsX",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeTChgPHkb7f"
      },
      "source": [
        "filepath = '/content/drive/MyDrive/sentiments/aclImdb/train'"
      ],
      "id": "xeTChgPHkb7f",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "falling-algeria"
      },
      "source": [
        "# load data\n",
        "def pre_process_data(filepath):\n",
        "\n",
        "    positive_path = os.path.join(filepath, 'pos')\n",
        "    negative_path = os.path.join(filepath, 'neg')\n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    dataset = []\n",
        "\n",
        "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((pos_label, f.read()))\n",
        "            \n",
        "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((neg_label, f.read()))\n",
        "\n",
        "    shuffle(dataset)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "dataset = pre_process_data(filepath)"
      ],
      "id": "falling-algeria",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7TG8SOxlMod",
        "outputId": "d7aa7042-bc55-499b-dfee-fbe6133da7e7"
      },
      "source": [
        "# Vectorizer and tokenizer\n",
        "word_vectors = get_data('w2v', limit=200000)\n",
        "def tokenize_and_vectorize(dataset):\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    vectorized_data = []\n",
        "    expected = []\n",
        "    for sample in dataset:\n",
        "        tokens = tokenizer.tokenize(sample[1])\n",
        "        sample_vecs = []\n",
        "        for token in tokens:\n",
        "          try:\n",
        "            sample_vecs.append(word_vectors[token])\n",
        "          except KeyError:\n",
        "            pass # No matching token in the Google w2v vocab\n",
        "\n",
        "        vectorized_data.append(sample_vecs)\n",
        "\n",
        "    return vectorized_data\n",
        "\n",
        "def collect_expected(dataset):\n",
        "    \"\"\" Peel off the target values from the dataset \"\"\"\n",
        "    expected = []\n",
        "    for sample in dataset:\n",
        "        expected.append(sample[0])\n",
        "    return expected\n",
        "\n",
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)"
      ],
      "id": "f7TG8SOxlMod",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 402111/402111 [00:15<00:00, 25896.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-19 10:27:50,459 WARNING:  nlpia.loaders:528:normalize_ext_rename normalize_ext.filepath=/usr/local/lib/python3.7/dist-packages/nlpia/bigdata/googlenews-vectors-negative300.bin.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyNK0eN1lMs_"
      },
      "source": [
        "# train/test split\n",
        "split_point = int(len(vectorized_data)*.8)\n",
        "x_train = vectorized_data[:split_point_]\n",
        "y_train_ = expected[:split_point]\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]\n",
        "\n",
        "# CNN params\n",
        "maxlen = 400\n",
        "batch_size = 32\n",
        "embedding_dims = 300\n",
        "filters = 250\n",
        "kernel_size = 3\n",
        "hidden_dims = 250\n",
        "epochs = 2"
      ],
      "id": "PyNK0eN1lMs_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSy_1uSWlMwK"
      },
      "source": [
        "def pad_trunc(data, maxlen):\n",
        "    \"\"\"\n",
        "    For a given dataset pad with zero vectors or truncate to maxlen\n",
        "    \"\"\"\n",
        "    new_data = []\n",
        "    \n",
        "    # Create a vector of 0s the length of our word vectors\n",
        "    zero_vector = []\n",
        "    for _ in range(len(data[0][0])):\n",
        "    \tzero_vector.append(0.0)\n",
        "    \n",
        "    for sample in data:\n",
        "        if len(sample) > maxlen:\n",
        "        \ttemp = sample[:maxlen]\n",
        "        elif len(sample) < maxlen:\n",
        "            temp = sample\n",
        "            # Append the appropriate number 0 vectors to the list\n",
        "            additional_elems = maxlen - len(sample)\n",
        "            for _ in range(additional_elems):\n",
        "            \ttemp.append(zero_vector)\n",
        "        else:\n",
        "        \ttemp = sample\n",
        "        new_data.append(temp)\n",
        "    return new_data\n",
        "\n",
        "#  pass train and test data into the padder/truncator\n",
        "x_train = pad_trunc(x_train, maxlen)\n",
        "x_test = pad_trunc(x_test, maxlen)\n",
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ],
      "id": "cSy_1uSWlMwK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU7FXEF5li0b"
      },
      "source": [
        "\n",
        "\t1. For each input example, you applied a filter (weights and activation function).\n",
        "    2. Convolved across the length of the input, which would output a 1D vector\n",
        "    slightly smaller than the original input (1 x 398 which is input with the filter\n",
        "    starting left-aligned and finishing right-aligned) for each filter.\n",
        "    3. For each filter output (there are 250 of them, remember), you took the single\n",
        "    maximum value from each 1D vector.\n",
        "    4. At this point you have a single vector (per input example) that is 1 x 250 (the\n",
        "    number of filters).\n"
      ],
      "id": "DU7FXEF5li0b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkG0hw2slMy-"
      },
      "source": [
        "# CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(\n",
        "                filters,\n",
        "                kernel_size,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                strides=1,\n",
        "                input_shape=(maxlen, embedding_dims)))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "id": "KkG0hw2slMy-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcGN-y1ylch2"
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "id": "HcGN-y1ylch2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtttZjCglck0"
      },
      "source": [
        "# save model\n",
        "model_structure = model.to_json()\n",
        "with open(\"cnn_model.json\", \"w\") as json_file:\n",
        "\tjson_file.write(model_structure)\n",
        "model.save_weights(\"cnn_weights.h5\")"
      ],
      "id": "KtttZjCglck0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKOm1KIZlcoE"
      },
      "source": [
        "# using model\n",
        "from keras.models import model_from_json\n",
        "with open(\"cnn_model.json\", \"r\") as json_file:\n",
        "    json_string = json_file.read()\n",
        "    \n",
        "model = model_from_json(json_string)\n",
        "model.load_weights('cnn_weights.h5')\n",
        "\n",
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "model.predict(test_vec)  # model.predict_classes(test_vec)"
      ],
      "id": "SKOm1KIZlcoE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0WGbKk7lcqz"
      },
      "source": [
        ""
      ],
      "id": "e0WGbKk7lcqz",
      "execution_count": null,
      "outputs": []
    }
  ]
}