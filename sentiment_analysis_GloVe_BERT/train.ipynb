{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecological-italic"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import os"
      ],
      "id": "ecological-italic",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uniform-memorial",
        "outputId": "2148cc41-f52f-4919-ff40-3e2a1be25aca"
      },
      "source": [
        "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\",\n",
        "                                split=\"train\",\n",
        "                                with_info=True, as_supervised=True)\n",
        "\n",
        "imdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", as_supervised=True)"
      ],
      "id": "uniform-memorial",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Load dataset info from /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n",
            "INFO:absl:Reusing dataset imdb_reviews (/root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train, from /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n",
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Load dataset info from /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n",
            "INFO:absl:Reusing dataset imdb_reviews (/root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test, from /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "civic-america"
      },
      "source": [
        "### Keras Tokenizer"
      ],
      "id": "civic-america"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy1G5asKUXkp"
      },
      "source": [
        "X_train = list(map(lambda x: x[0].numpy().decode('utf-8'), imdb_train))\n",
        "y_train = list(map(lambda x: x[1], imdb_train))"
      ],
      "id": "Qy1G5asKUXkp",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reflected-studio",
        "outputId": "7aa21b2a-d2a0-4bce-983b-7f58605aa06a"
      },
      "source": [
        "%%time\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "                                                  lower=True,\n",
        "                                                  split=\" \",)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_encoded = tokenizer.texts_to_sequences(X_train)\n",
        "X_encoded = pad_sequences(X_encoded, padding=\"post\", maxlen=150) "
      ],
      "id": "reflected-studio",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 9.88 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chief-equipment"
      },
      "source": [
        "### TFDS Tokenizer"
      ],
      "id": "chief-equipment"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suburban-characterization",
        "outputId": "c739aac6-e6af-4934-c3a6-d166a64958af"
      },
      "source": [
        "tokenizer_tfds = tfds.deprecated.text.Tokenizer()\n",
        "vocabulary_set = set()\n",
        "MAX_TOKENS = 0\n",
        "for example, label in imdb_train:\n",
        "    some_tokens = tokenizer_tfds.tokenize(example.numpy())\n",
        "    if MAX_TOKENS < len(some_tokens):\n",
        "        MAX_TOKENS = len(some_tokens)\n",
        "    vocabulary_set.update(some_tokens)\n",
        "    \n",
        "imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set, lowercase=True, tokenizer=tokenizer_tfds)\n",
        "\n",
        "vocab_size = imdb_encoder.vocab_size\n",
        "print(vocab_size, MAX_TOKENS)"
      ],
      "id": "suburban-characterization",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93931 2525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chubby-china"
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "def encode_pad_transform(sample):\n",
        "    \n",
        "    encoded = imdb_encoder.encode(sample.numpy())\n",
        "    pad = sequence.pad_sequences([encoded], padding='post',maxlen=150)\n",
        "    return np.array(pad[0], dtype=np.int64)\n",
        "\n",
        "def encode_tf_fn(sample, label):\n",
        "    \n",
        "    encoded = tf.py_function(encode_pad_transform,\n",
        "                             inp=[sample],\n",
        "                             Tout=(tf.int64))\n",
        "    \n",
        "    encoded.set_shape([None])\n",
        "    label.set_shape([])\n",
        "    return encoded, label "
      ],
      "id": "chubby-china",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "intense-python"
      },
      "source": [
        "encoded_train = imdb_train.map(encode_tf_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "encoded_test = imdb_test.map(encode_tf_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "id": "intense-python",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "provincial-powder"
      },
      "source": [
        "### Loading pre-trained GloVe embeddings"
      ],
      "id": "provincial-powder"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "super-forge"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "id": "super-forge",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0RAY0a5Uu-y",
        "outputId": "61aed018-9dd6-47d5-e63f-c8c02582a476"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('drive')\n",
        "glove_path = 'drive/MyDrive/GloVe/glove.6B.50d.txt'"
      ],
      "id": "N0RAY0a5Uu-y",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crude-zoning",
        "outputId": "0cfd2c91-763b-489b-c8b0-702649dbe042"
      },
      "source": [
        "dict_w2v = {}\n",
        "with open(glove_path, \"rb\") as file:\n",
        "    for line in file:\n",
        "        tokens = line.split()\n",
        "        word = tokens[0].decode('utf-8')\n",
        "        vector = np.array(tokens[1:], dtype=np.float32)\n",
        "        \n",
        "        if vector.shape[0] == 50:\n",
        "            dict_w2v[word] = vector\n",
        "        else:\n",
        "            print(\"There was an issue with \" + word)\n",
        "            \n",
        "# let's check the vocabulary size\n",
        "print(\"Dictionary Size: \", len(dict_w2v))"
      ],
      "id": "crude-zoning",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary Size:  400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tutorial-haven"
      },
      "source": [
        "vocab_size = imdb_encoder.vocab_size\n",
        "embedding_dim = 50\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))"
      ],
      "id": "tutorial-haven",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geological-wings"
      },
      "source": [
        "### Tfds"
      ],
      "id": "geological-wings"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "descending-wallace",
        "outputId": "fe378c94-ef72-4908-a106-711305293bec"
      },
      "source": [
        "unk_cnt = 0\n",
        "unk_set = set()\n",
        "for word in imdb_encoder.tokens:\n",
        "    embedding_vector = dict_w2v.get(word)\n",
        "    \n",
        "    if embedding_vector is not None:\n",
        "        tkn_id = imdb_encoder.encode(word)[0]\n",
        "        embedding_matrix[tkn_id] = embedding_vector\n",
        "    else:\n",
        "        unk_cnt += 1\n",
        "        unk_set.add(word)\n",
        "# Print how many weren't found\n",
        "print(\"Total unknown words: \", unk_cnt)"
      ],
      "id": "descending-wallace",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total unknown words:  14553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hairy-balance"
      },
      "source": [
        "### Keras"
      ],
      "id": "hairy-balance"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "choice-dream",
        "outputId": "537c8465-1094-4ead-e54a-f6d2c213ebdb"
      },
      "source": [
        "unk_cnt = 0\n",
        "unk_set = set()\n",
        "for word in list(tokenizer.word_index.keys()):\n",
        "    embedding_vector = dict_w2v.get(word)\n",
        "    \n",
        "    if embedding_vector is not None:\n",
        "        tkn_id = tokenizer.word_index[word]\n",
        "        embedding_matrix[tkn_id] = embedding_vector\n",
        "    else:\n",
        "        unk_cnt += 1\n",
        "        unk_set.add(word)\n",
        "# Print how many weren't found\n",
        "print(\"Total unknown words: \", unk_cnt)"
      ],
      "id": "choice-dream",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total unknown words:  28423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "answering-temple"
      },
      "source": [
        "### Build model"
      ],
      "id": "answering-temple"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liked-engagement"
      },
      "source": [
        "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
        "# Number of RNN units\n",
        "rnn_units = 64\n",
        "#batch size\n",
        "BATCH_SIZE=100"
      ],
      "id": "liked-engagement",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "silent-forum"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
        "\n",
        "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size, train_emb=False):\n",
        "    model = tf.keras.Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
        "                  weights=[embedding_matrix], trainable=train_emb),\n",
        "        Bidirectional(LSTM(rnn_units, return_sequences=True,\n",
        "                           dropout=0.5)),\n",
        "        Bidirectional(LSTM(rnn_units, dropout=0.25)),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model"
      ],
      "id": "silent-forum",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cultural-behalf",
        "outputId": "db1d8ea9-aa2f-4527-e519-e218a66f06a0"
      },
      "source": [
        "model_fe = build_model_bilstm(\n",
        "    vocab_size = vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)\n",
        "model_fe.summary()"
      ],
      "id": "cultural-behalf",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 50)          4696550   \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, None, 128)         58880     \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 4,854,375\n",
            "Trainable params: 157,825\n",
            "Non-trainable params: 4,696,550\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "senior-electronics"
      },
      "source": [
        "model_fe.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'Precision', 'Recall'])"
      ],
      "id": "senior-electronics",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "canadian-glossary",
        "outputId": "33d6336a-19cd-42f8-a562-f92813a996d9"
      },
      "source": [
        "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)\n",
        "model_fe.fit(encoded_train_batched, epochs=10)"
      ],
      "id": "canadian-glossary",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 33s 82ms/step - loss: 0.6347 - accuracy: 0.6282 - precision: 0.6288 - recall: 0.6348\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 22s 90ms/step - loss: 0.5586 - accuracy: 0.7157 - precision: 0.7223 - recall: 0.7025\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 23s 90ms/step - loss: 0.5194 - accuracy: 0.7432 - precision: 0.7470 - recall: 0.7379\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 22s 90ms/step - loss: 0.4896 - accuracy: 0.7647 - precision: 0.7644 - recall: 0.7665\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 23s 90ms/step - loss: 0.5145 - accuracy: 0.7462 - precision: 0.7491 - recall: 0.7424\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 22s 90ms/step - loss: 0.4608 - accuracy: 0.7800 - precision: 0.7829 - recall: 0.7757\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 0.4534 - accuracy: 0.7883 - precision: 0.7968 - recall: 0.7752\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 23s 90ms/step - loss: 0.4411 - accuracy: 0.7967 - precision: 0.7993 - recall: 0.7930\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 23s 90ms/step - loss: 0.4346 - accuracy: 0.7933 - precision: 0.7908 - recall: 0.7991\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 23s 90ms/step - loss: 0.4204 - accuracy: 0.8052 - precision: 0.8070 - recall: 0.8032\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0c8343bdd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9771gUeVVoP"
      },
      "source": [
        "### Fine-tuning"
      ],
      "id": "U9771gUeVVoP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opposite-episode",
        "outputId": "0d1583f7-ff9f-437e-e851-46d9cf5a2378"
      },
      "source": [
        "model_ft = build_model_bilstm(\n",
        " vocab_size=vocab_size,\n",
        " embedding_dim=embedding_dim,\n",
        " rnn_units=rnn_units,\n",
        " batch_size=BATCH_SIZE,\n",
        " train_emb=True)\n",
        "model_ft.summary()"
      ],
      "id": "opposite-episode",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 50)          4696550   \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, None, 128)         58880     \n",
            "_________________________________________________________________\n",
            "bidirectional_7 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 4,854,375\n",
            "Trainable params: 4,854,375\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfiHLm6nVZoO",
        "outputId": "b86bf691-dcb4-4376-85c3-5052dd37e824"
      },
      "source": [
        "model_ft.compile(loss='binary_crossentropy',\n",
        " optimizer='adam',\n",
        " metrics=['accuracy', 'Precision', 'Recall'])\n",
        "model_ft.fit(encoded_train_batched, epochs=10)"
      ],
      "id": "dfiHLm6nVZoO",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 44s 125ms/step - loss: 0.6129 - accuracy: 0.6483 - precision: 0.6497 - recall: 0.6520\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 33s 131ms/step - loss: 0.4159 - accuracy: 0.8108 - precision: 0.8106 - recall: 0.8120\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 33s 131ms/step - loss: 0.3367 - accuracy: 0.8522 - precision: 0.8549 - recall: 0.8491\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 33s 130ms/step - loss: 0.2909 - accuracy: 0.8764 - precision: 0.8749 - recall: 0.8789\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 33s 131ms/step - loss: 0.2436 - accuracy: 0.9008 - precision: 0.9008 - recall: 0.9011\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 33s 131ms/step - loss: 0.2209 - accuracy: 0.9113 - precision: 0.9097 - recall: 0.9135\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 33s 131ms/step - loss: 0.1858 - accuracy: 0.9259 - precision: 0.9270 - recall: 0.9250\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 33s 131ms/step - loss: 0.1679 - accuracy: 0.9377 - precision: 0.9371 - recall: 0.9387\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 33s 131ms/step - loss: 0.1424 - accuracy: 0.9458 - precision: 0.9443 - recall: 0.9477\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 33s 131ms/step - loss: 0.1214 - accuracy: 0.9557 - precision: 0.9526 - recall: 0.9593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0c8343bfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiyMUAkPVfcn"
      },
      "source": [
        "### BERT"
      ],
      "id": "IiyMUAkPVfcn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN8CyksIVe6B"
      },
      "source": [
        "!pip install transformers==3.0.2"
      ],
      "id": "fN8CyksIVe6B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcWGa_OWVhfX"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "bert_name = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_name,\n",
        "                                          add_special_tokens=True,\n",
        "                                          do_lower_case=False,\n",
        "                                          max_length=150,\n",
        "                                          pad_to_max_length=True)"
      ],
      "id": "wcWGa_OWVhfX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVF2L7zFVhcE"
      },
      "source": [
        "def bert_encoder(review):\n",
        " txt = review.numpy().decode('utf-8')\n",
        " encoded = tokenizer.encode_plus(txt, add_special_tokens=True,\n",
        "                                 max_length=150,\n",
        "                                 pad_to_max_length=True,\n",
        "                                 return_attention_mask=True,\n",
        "                                 truncation=True,\n",
        "                                 return_token_type_ids=True)\n",
        " \n",
        " return encoded['input_ids'], encoded['token_type_ids'], encoded['attention_mask']"
      ],
      "id": "fVF2L7zFVhcE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwzIb8kfVhV6"
      },
      "source": [
        "bert_train = [bert_encoder(r) for r, l in imdb_train]\n",
        "bert_lbl = [l for r, l in imdb_train]\n",
        "bert_train = np.array(bert_train)\n",
        "bert_lbl = tf.keras.utils.to_categorical(bert_lbl, num_classes=2)"
      ],
      "id": "XwzIb8kfVhV6",
      "execution_count": null,
      "outputs": []
    }
  ]
}