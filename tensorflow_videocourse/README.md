Course: https://www.youtube.com/watch?v=tPYj3fFJGjk&t=6s



at 1:11:05 he explained that in linear regression in 3d space, assuming a linear relationship, you try to use two variables to predict the value of the third. This is true, but the model is then not a straight line pointing somewhere in 3d space, but a 2d plane. Such false information gives beginners a bad intuition for these things. 

Furthermore, the whole Titanic survival prediction section was a classification task, so it had little to do with linear regression. He used the LinearClassifier object, which (probably) runs a logistic regression in the background. In the output dictionary, there were keys that are typical for the output of a logistic regression. Although linear regression and logistic regression are both algorithms that create models that assume a linear relationship between input variables, they are completely different algorithms. I would have liked to see a clear distinction between regression and classification problems here, because these are fundamental concepts in ML and the video wrongly conveys that you solve a regression problem by predicting whether a passenger survives or not.

edit: His intuitive explanation at 3:09:13 of what activation functions do is just wrong. The sigmoid function does not project the data points into higher dimensional space, quite the opposite, actually. The sigmoid function is defined as f: R^n -> [0,1], so it takes as input an n-dimensional vector and it yields a scalar (fancy word for a single number). This whole "projection into higher dimensional space" thing is known as the Kernel trick and is NOT used in his examples, it's something completely different. Activation functions are used in Neural Networks because they introduce non-linearity which you need because otherwise your Neural Network would just be a linear classifier so it would basically do one big weighted summation of the inputs. This is not enough to capture complicated relationships in data. With non-linearity introduced, it can be shown that a Neural Network can approximate any function with arbitrary accuracy, that is any function that might have generated the data. Read up on the universal approximation theorem, if you want to learn more.

edit 2: 3:36:15 You don't evaluate on your test data multiple times to select the best set of hyperparameters. That way, you overfit your test data. A cleaner approach would be to split the data into three sets: the training set, the validation set and the test set. Prior to training you define a few selections of hyperparameters you want to select the best from before you train all these models on the training data. You then evaluate the performance of each model on the validation data. Finally, you select the best performing model and test it on the test data to get a final estimate of the model's performance on completely new data points. There are more sophisticated approaches to do hyperparameter tuning but this is the basic idea.